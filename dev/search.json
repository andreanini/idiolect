[{"path":"https://andreanini.github.io/idiolect/dev/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 2, June 1991Copyright © 1989, 1991 Free Software Foundation, Inc.,51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"licenses software designed take away freedom share change . contrast, GNU General Public License intended guarantee freedom share change free software–make sure software free users. General Public License applies Free Software Foundation’s software program whose authors commit using . (Free Software Foundation software covered GNU Lesser General Public License instead.) can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge service wish), receive source code can get want , can change software use pieces new free programs; know can things. protect rights, need make restrictions forbid anyone deny rights ask surrender rights. restrictions translate certain responsibilities distribute copies software, modify . example, distribute copies program, whether gratis fee, must give recipients rights . must make sure , , receive can get source code. must show terms know rights. protect rights two steps: (1) copyright software, (2) offer license gives legal permission copy, distribute /modify software. Also, author’s protection , want make certain everyone understands warranty free software. software modified someone else passed , want recipients know original, problems introduced others reflect original authors’ reputations. Finally, free program threatened constantly software patents. wish avoid danger redistributors free program individually obtain patent licenses, effect making program proprietary. prevent , made clear patent must licensed everyone’s free use licensed . precise terms conditions copying, distribution modification follow.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/LICENSE.html","id":"terms-and-conditions-for-copying-distribution-and-modification","dir":"","previous_headings":"","what":"TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION","title":"GNU General Public License","text":"0. License applies program work contains notice placed copyright holder saying may distributed terms General Public License. “Program”, , refers program work, “work based Program” means either Program derivative work copyright law: say, work containing Program portion , either verbatim modifications /translated another language. (Hereinafter, translation included without limitation term “modification”.) licensee addressed “”. Activities copying, distribution modification covered License; outside scope. act running Program restricted, output Program covered contents constitute work based Program (independent made running Program). Whether true depends Program . 1. may copy distribute verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice disclaimer warranty; keep intact notices refer License absence warranty; give recipients Program copy License along Program. may charge fee physical act transferring copy, may option offer warranty protection exchange fee. 2. may modify copy copies Program portion , thus forming work based Program, copy distribute modifications work terms Section 1 , provided also meet conditions: ) must cause modified files carry prominent notices stating changed files date change. b) must cause work distribute publish, whole part contains derived Program part thereof, licensed whole charge third parties terms License. c) modified program normally reads commands interactively run, must cause , started running interactive use ordinary way, print display announcement including appropriate copyright notice notice warranty (else, saying provide warranty) users may redistribute program conditions, telling user view copy License. (Exception: Program interactive normally print announcement, work based Program required print announcement.) requirements apply modified work whole. identifiable sections work derived Program, can reasonably considered independent separate works , License, terms, apply sections distribute separate works. distribute sections part whole work based Program, distribution whole must terms License, whose permissions licensees extend entire whole, thus every part regardless wrote . Thus, intent section claim rights contest rights work written entirely ; rather, intent exercise right control distribution derivative collective works based Program. addition, mere aggregation another work based Program Program (work based Program) volume storage distribution medium bring work scope License. 3. may copy distribute Program (work based , Section 2) object code executable form terms Sections 1 2 provided also one following: ) Accompany complete corresponding machine-readable source code, must distributed terms Sections 1 2 medium customarily used software interchange; , b) Accompany written offer, valid least three years, give third party, charge cost physically performing source distribution, complete machine-readable copy corresponding source code, distributed terms Sections 1 2 medium customarily used software interchange; , c) Accompany information received offer distribute corresponding source code. (alternative allowed noncommercial distribution received program object code executable form offer, accord Subsection b .) source code work means preferred form work making modifications . executable work, complete source code means source code modules contains, plus associated interface definition files, plus scripts used control compilation installation executable. However, special exception, source code distributed need include anything normally distributed (either source binary form) major components (compiler, kernel, ) operating system executable runs, unless component accompanies executable. distribution executable object code made offering access copy designated place, offering equivalent access copy source code place counts distribution source code, even though third parties compelled copy source along object code. 4. may copy, modify, sublicense, distribute Program except expressly provided License. attempt otherwise copy, modify, sublicense distribute Program void, automatically terminate rights License. However, parties received copies, rights, License licenses terminated long parties remain full compliance. 5. required accept License, since signed . However, nothing else grants permission modify distribute Program derivative works. actions prohibited law accept License. Therefore, modifying distributing Program (work based Program), indicate acceptance License , terms conditions copying, distributing modifying Program works based . 6. time redistribute Program (work based Program), recipient automatically receives license original licensor copy, distribute modify Program subject terms conditions. may impose restrictions recipients’ exercise rights granted herein. responsible enforcing compliance third parties License. 7. , consequence court judgment allegation patent infringement reason (limited patent issues), conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. distribute satisfy simultaneously obligations License pertinent obligations, consequence may distribute Program . example, patent license permit royalty-free redistribution Program receive copies directly indirectly , way satisfy License refrain entirely distribution Program. portion section held invalid unenforceable particular circumstance, balance section intended apply section whole intended apply circumstances. purpose section induce infringe patents property right claims contest validity claims; section sole purpose protecting integrity free software distribution system, implemented public license practices. Many people made generous contributions wide range software distributed system reliance consistent application system; author/donor decide willing distribute software system licensee impose choice. section intended make thoroughly clear believed consequence rest License. 8. distribution /use Program restricted certain countries either patents copyrighted interfaces, original copyright holder places Program License may add explicit geographical distribution limitation excluding countries, distribution permitted among countries thus excluded. case, License incorporates limitation written body License. 9. Free Software Foundation may publish revised /new versions General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies version number License applies “later version”, option following terms conditions either version later version published Free Software Foundation. Program specify version number License, may choose version ever published Free Software Foundation. 10. wish incorporate parts Program free programs whose distribution conditions different, write author ask permission. software copyrighted Free Software Foundation, write Free Software Foundation; sometimes make exceptions . decision guided two goals preserving free status derivatives free software promoting sharing reuse software generally.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/LICENSE.html","id":"no-warranty","dir":"","previous_headings":"","what":"NO WARRANTY","title":"GNU General Public License","text":"11. PROGRAM LICENSED FREE CHARGE, WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION. 12. EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MAY MODIFY /REDISTRIBUTE PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES. END TERMS CONDITIONS","code":""},{"path":"https://andreanini.github.io/idiolect/dev/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively convey exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program interactive, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, commands use may called something show w show c; even mouse-clicks menu items–whatever suits program. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. sample; alter names: General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA. Gnomovision version 69, Copyright (C) year name of author Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details. Yoyodyne, Inc., hereby disclaims all copyright interest in the program `Gnomovision' (which makes passes at compilers) written by James Hacker.  <signature of Ty Coon>, 1 April 1989 Ty Coon, President of Vice"},{"path":"https://andreanini.github.io/idiolect/dev/articles/idiolect.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"idiolect","text":"Authorship Analysis defined task determining likelihood certain candidate author certain set questioned disputed texts. call Forensic Authorship Analysis task kind applied real forensic case. settings, disputed texts anonymous malicious documents, threatening letter, also text messages, emails, document , various reasons, becomes evidence forensic case. Forensic Linguistics, typically set disputed questioned text indicated QQ, set texts known origin, example texts written candidate author collected comparison material, labelled using KK. addition two datasets, analysis also necessitates comparison reference corpus call RR. classic case involving closed set suspects, texts written suspects minus candidate form RR. Authorship Verification cases involve one candidate author, reference dataset might compiled analyst specific case (Ishihara et al. 2024). crucial difference Authorship Analysis Forensic Authorship Analysis whereas former can treated classification task final answer binary (‘candidate author’ vs. ‘candidate author’), latter needs expression likelihood two competing propositions hypotheses, Prosecution Hypothesis HpH_p vs. Defence Hypothesis HdH_d, example: HpH_p: author KK author QQ individual.HdH_d: author KK author QQ two different individuals. job forensic linguist forensic context analyse linguistic evidence determine hypothesis supports degree strength, thus aiding trier--fact reaching conclusion. role forensic linguist therefore provide YES/answer rather express strength evidence favour two hypotheses. Given KK, QQ RR, workflow analysis involves four steps: Preparation: step involves pre-processing step necessary analysis chosen method; Validation: Carry analysis case data separate dataset designed similar case material order validate method particular case; Analysis: Carry analysis real KK, QQ, RR; Calibration: Turn output (3) Likelihood Ratio expresses strength evidence given two competing hypotheses.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/articles/idiolect.html","id":"preparation","dir":"Articles","previous_headings":"","what":"Preparation","title":"idiolect","text":"idiolect function import texts R called create_corpus(). function simply calling readtext (therefore package must installed) scanning name files metadata text, specifically name author name file. syntax follow name files authorname_textname.txt (e.g. smith_text1.txt). Assuming folder plain text files names according syntax ready user’s computer, following command (executed ) loads folder quanteda corpus object metadata docvars. vignette, instead, workflow demonstrated using small dataset Enron corpus included package (see ?enron.sample). corpus quanteda corpus object contains ten authors approximately amount data.","code":"corpus <- create_corpus(\"path/to/folder\") corpus <- enron.sample"},{"path":"https://andreanini.github.io/idiolect/dev/articles/idiolect.html","id":"content-masking","dir":"Articles","previous_headings":"Preparation","what":"Content masking","title":"idiolect","text":"highly recommended sometimes necessary pre-processing step content masking. step consists masking removing words tokens text likely create noise authorship analysis. Hiding content avoids incorrectly attributing text based correlation topics authors (Bischoff et al. 2020) also tends improve performance authorship analysis methods cross-topic cross-genre situations (Stamatatos 2017). Three content masking methods implemented idiolect: (1) POSnoise algorithm developed Halvani Graner (2021); (2) frame n-grams approach introduced Nini (2023); (3) implementation TextDistortion approach originally introduced Stamatatos (2017). options available contentmask() function. function depends spacyr requires downloading parsing model language automatic tagging Parts Speech (e.g. nouns, adjectives, adverbs), function run vignette. Instead, Enron sample already content-masked using POSnoise, can seen preview corpus POSnoise algorithm essentially replaces words tend contain meaning (nouns, verbs, adjectives, adverbs) Part Speech tag (N, V, J, B) words tokens left unchanged. addition operation, POSnoise contains white list content words mostly tend functional English, verbs like , , make adverbs consequently, therefore. following code used run contentmask() function. require installing initiating spacy parsing model language chosen. process happen automatically","code":"corpus #> Corpus consisting of 49 documents and 1 docvar. #> known [Kh Mail_1].txt : #> \"N N N N wants to be N when he V up likes N P , N for doing t...\" #>  #> known [Kh Mail_3].txt : #> \"i 've V a J one , but the only N N N i have is on a N N from...\" #>  #> known [Kh Mail_4].txt : #> \"this was J towards the N of a J N N N . in N , P P helped th...\" #>  #> known [Kh Mail_5].txt : #> \"V the N for more than D N may get you V . a N N with a N and...\" #>  #> unknown [Kh Mail_2].txt : #> \"P , here 's the J N on our P P N V to V the V needs of the P...\" #>  #> unknown [Kw Mail_3].txt : #> \"they also have J N at the J N of P D per N and only a D J ea...\" #>  #> [ reached max_ndoc ... 43 more documents ] posnoised.corpus <- contentmask(corpus, model = \"en_core_web_sm\", algorithm = \"POSnoise\")"},{"path":"https://andreanini.github.io/idiolect/dev/articles/idiolect.html","id":"data-labelling","dir":"Articles","previous_headings":"Preparation","what":"Data labelling","title":"idiolect","text":"example simulated first text written author Kw real QQ text (one labelled ‘unknown’) known texts written Kw (labelled ‘known’) therefore set known texts KK. remaining texts authors reference samples RR.","code":"Q <- corpus_subset(corpus, author == \"Kw\")[1] K <- corpus_subset(corpus, author == \"Kw\")[2:5] R <- corpus_subset(corpus, author != \"Kw\")"},{"path":"https://andreanini.github.io/idiolect/dev/articles/idiolect.html","id":"vectorisation","dir":"Articles","previous_headings":"Preparation","what":"Vectorisation","title":"idiolect","text":"applying certain authorship analysis methods, text sample must turned numerical representation called feature vector, process typically referred vectorisation. idiolect function vectorise corpus called vectorize(). features normally used many authorship analysis methods nn-grams words punctuation marks characters. example, QQ text can vectorised relative frequencies words using code. , frequent 1,000 character 4-grams relative frequencies, example, using output function quanteda document-feature matrix (dfm) can efficiently store even large matrices. vectorize() function mostly designed expert users different choices parameters vectorisation can made using single authorship analysis method function. addition, since authorship analysis methods already default setting parameters, already default authorship analysis functions. step therefore necessary unless specific requirements vectorisation handled functions apply authorship analysis methods.","code":"vectorize(Q, tokens = \"word\", remove_punct = F, remove_symbols = T, remove_numbers = T,           lowercase = T, n = 1, weighting = \"rel\", trim = F) |>    print(max_nfeat = 3) #> Document-feature matrix of: 1 document, 136 features (0.00% sparse) and 1 docvar. #>                          features #> docs                            they        also      have #>   unknown [Kw Mail_3].txt 0.00289296 0.009643202 0.0192864 #> [ reached max_nfeat ... 133 more features ] vectorize(Q, tokens = \"character\", remove_punct = F, remove_symbols = T, remove_numbers = T,           lowercase = T, n = 4, weighting = \"rel\", trim = T, threshold = 1000) |>    print(max_nfeat = 3) #> Document-feature matrix of: 1 document, 1,094 features (0.00% sparse) and 1 docvar. #>                          features #> docs                              they         hey          ey a #>   unknown [Kw Mail_3].txt 0.0009771987 0.0009771987 0.0003257329 #> [ reached max_nfeat ... 1,091 more features ]"},{"path":"https://andreanini.github.io/idiolect/dev/articles/idiolect.html","id":"validation","dir":"Articles","previous_headings":"","what":"Validation","title":"idiolect","text":"first step validation remove real QQ text. actual forensic sample analyse must therefore removed validating analysis. validation set therefore made KK RR datasets dataset can now re-divided ‘fake’ QQ texts ‘fake’ KK texts. text corpus labelled ‘unknown’ ‘known’ two new disjoint datasets, validation.Q validation.K can created selecting texts based label. way validation analysis can conducted. example, one adopt leave-one-approach taking single text treat QQ run authorship analysis method one . Alternatively, completely different dataset similar case data used. simpler approach suitable small example.","code":"validation <- K + R validation.Q <- corpus_subset(validation, grepl(\"^unknown\", docnames(validation))) validation.K <- corpus_subset(validation, grepl(\"^known\", docnames(validation)))"},{"path":"https://andreanini.github.io/idiolect/dev/articles/idiolect.html","id":"authorship-analysis","dir":"Articles","previous_headings":"Validation","what":"Authorship analysis","title":"idiolect","text":"analysis validated analysis applied QQ text. Therefore, choice method made depending right choice analyse QQ. example, scenario simulated verification: unknown QQ text written KK author, Kw? reason, method chosen one successful authorship verification methods available today, Impostors Method (Koppel Winter 2014), particular one latest variants called Rank-Based Impostors Method (Potha Stamatatos 2017, 2020). analysis can run idiolect using function impostors() selecting default parameter algorithm argument, “RBI”. main argument function q.data, set QQ texts test, k.data, set KK texts one authors going tested, finally set impostors data, cand.imps. example, impostors data RR set generally recommendation use another dataset possible. impostors() function accepts one author k.data also accepts dataset input k.data cand.imps. dataset used, impostors() test author k.data use texts written authors impostors. contrast authorship analysis functions like delta() ngram_tracing(), impostors() offer additional parameters modify vectorisation process Impostors Method algorithms already well-specified default setting. user wants change vectorise corpus separately using vectorize() use dfm input impostors(). RBI variant method also requires setting parameter called kk, number similar impostors texts sample wider set impostors. recommended setting k=100k=100 k=300k=300 simplicity set k=50k=50 example. analysis using Impostors Method can long run times, function can also parallelised using one core. output impostors() data frame showing results comparing KK author QQ text. variable target TRUE comparison -author one FALSE different-author one. variable score contains Impostors score, value ranges 0 1. authorship analysis functions return data frame type columns. variable score therefore represents different quantities depending analysis function used (e.g. delta(), Δ\\Delta coefficient, ). order assess results validation analysis, function performance() can used return series performance metrics. function can take one two result data frames input. two provided, one used training one test. one data frame provided, performance metrics calculated using leave-one-approach. procedure followed function held one text (leave-one-, otherwise test dataset entirety) use rest data (training dataset) calibration dataset calculate Log-Likelihood Ratio (LLRLLR). analysis done using calibrate_LLR() function, fits logistic regression model calibrate score LLRLLRIshihara (2021) using ROC library (Leeuwen 2015). output function following CllrC_{llr} CllrminC_{llr}^{min} coefficients used evaluate performance LLRLLR(Ramos et al. 2013). coefficients estimate cost LLRLLR, value 1 indicates information LLRLLR lower coefficient Cllr<1C_{llr}<1 suggests information LLRLLR, lower values CllrC_{llr} suggesting better performance. binary classification metrics returned, Precision, Recall, F1, calculated using LLR>0LLR > 0 threshold TRUE (-author case) classification. present example, Cllr=C_{llr}= 0.804 suggests enough information LLRLLR able proceed actual forensic analysis. CllrminC_{llr}^{min}, component CllrC_{llr} measuring amount discrimination, even lower, means substantial difference two distributions. confirmed Area Curve value 0.83. large disparity TRUE FALSE test cases, values Precision F1 misleading. Balanced Accuracy value 0.846, however, suggests substantial amount discrimination LLR=0LLR=0. results analysis can also plotted using density plot two distributions, TRUE FALSE. can done using density_plot() function  plot shows values score horizontal axis density TRUE (red) vs. FALSE (blue) vertical axis. findings evidence method validated dataset now possible analyse QQ text use results calibrate LLRLLR QQ.","code":"res <- impostors(validation.Q, validation.K, validation.K, algorithm = \"RBI\", k = 50) res[1:10,] #>     K                       Q target score #> 1  Kw unknown [Kh Mail_2].txt  FALSE 0.404 #> 2  Kw unknown [Lc Mail_1].txt  FALSE 0.243 #> 3  Kw unknown [Ld Mail_4].txt  FALSE 0.752 #> 4  Kw unknown [Lt Mail_2].txt  FALSE 0.215 #> 5  Kw unknown [Lk Mail_4].txt  FALSE 0.306 #> 6  Kw unknown [Lb Mail_3].txt  FALSE 0.975 #> 7  Kw unknown [La Mail_3].txt  FALSE 0.262 #> 8  Kw unknown [Mf Mail_1].txt  FALSE 0.933 #> 9  Kw unknown [Ml Mail_3].txt  FALSE 0.846 #> 10 Kh unknown [Kh Mail_2].txt   TRUE 0.555 p <- performance(res) #>   |                                                                              |                                                                      |   0%  |                                                                              |=                                                                     |   1%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  11%  |                                                                              |=========                                                             |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  15%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |=============                                                         |  19%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |=================                                                     |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  27%  |                                                                              |====================                                                  |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  31%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |========================                                              |  35%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |============================                                          |  40%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  44%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  48%  |                                                                              |===================================                                   |  49%  |                                                                              |===================================                                   |  51%  |                                                                              |====================================                                  |  52%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |=======================================                               |  56%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  65%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  69%  |                                                                              |=================================================                     |  70%  |                                                                              |==================================================                    |  71%  |                                                                              |==================================================                    |  72%  |                                                                              |===================================================                   |  73%  |                                                                              |====================================================                  |  74%  |                                                                              |=====================================================                 |  75%  |                                                                              |=====================================================                 |  76%  |                                                                              |======================================================                |  78%  |                                                                              |=======================================================               |  79%  |                                                                              |========================================================              |  80%  |                                                                              |=========================================================             |  81%  |                                                                              |=========================================================             |  82%  |                                                                              |==========================================================            |  83%  |                                                                              |===========================================================           |  84%  |                                                                              |============================================================          |  85%  |                                                                              |=============================================================         |  87%  |                                                                              |=============================================================         |  88%  |                                                                              |==============================================================        |  89%  |                                                                              |===============================================================       |  90%  |                                                                              |================================================================      |  91%  |                                                                              |================================================================      |  92%  |                                                                              |=================================================================     |  93%  |                                                                              |==================================================================    |  94%  |                                                                              |===================================================================   |  96%  |                                                                              |====================================================================  |  97%  |                                                                              |====================================================================  |  98%  |                                                                              |===================================================================== |  99%  |                                                                              |======================================================================| 100% p$evaluation #>        Cllr  Cllr_min      EER Mean TRUE LLR Mean FALSE LLR TRUE trials #> 1 0.8043863 0.6155326 19.04762      0.413398     -0.3864078          11 #>   FALSE trials      AUC Balanced Accuracy Precision    Recall        F1 TP FN #> 1           83 0.829904          0.845679 0.3333333 0.8888889 0.4848485  8  1 #>   FP TN #> 1 16 65 density_plot(res)"},{"path":"https://andreanini.github.io/idiolect/dev/articles/idiolect.html","id":"analysis-of-q","dir":"Articles","previous_headings":"","what":"Analysis of QQ","title":"idiolect","text":"point thing left analyse forensic data feeding real QQ, KK, RR impostors() function using settings used validation. one QQ text, final table results contains one row","code":"q.res <- impostors(Q, K, R, algorithm = \"RBI\", k = 50) q.res #>    K                       Q target score #> 1 Kw unknown [Kw Mail_3].txt   TRUE 0.975"},{"path":"https://andreanini.github.io/idiolect/dev/articles/idiolect.html","id":"qualitative-examination-of-evidence","dir":"Articles","previous_headings":"Analysis of QQ","what":"Qualitative examination of evidence","title":"idiolect","text":"reaching conclusions, often important inspect features algorithm considered analysis. forensic analysis, good knowledge data important best practice require analyst familiar dataset running computational analysis. Reading data familiar can lead addition pre-processing steps remove noise can help analyst spot problem mistakenly introduced algorithm. addition familiarise data, idiolect allows analyst explore important feature considered authorship analysis method used. example, using RBI algorithm impostors() parameter features can switched TRUE obtain list important features. Running parameter switched produces following results RBI method uses features character 4-grams list features clearly hard interpret human analyst. Despite complexity, impossible task. idiolect offers function aid exploration called concordance(), uses quanteda’s kwic() engine. concordance() takes input string representing one words (punctuation marks). example, important character 4-gram seems <, > search target. search reveals character sequence strong characteristic candidate author’s writing. However, real underlying pattern use comma followed possessive determiner token sequence [, ], used candidate author one author reference corpus. Another important feature represented two character 4-grams, <lso ,> <, >, likely refer token sequence [also ,]. correct referring use also beginning sentence immediately followed comma. Although searching features returned clearly significant amount work, inspecting list features carefully using concordance() explore features data analyst can spot patterns mistakes analysis (Ypma, Ramos, Meuwly 2023).","code":"q.res2 <- impostors(Q, K, R, algorithm = \"RBI\", k = 50, features = T) strwrap(q.res2$features, width = 70) #>  [1] \", her|lso ,|so , |P P f|, i w|N N .| too |our P|ur P |also |re is|i\"   #>  [2] \"jus|ll me|l me |ere i|st wa|P , h|u hav| yet | , he|ou ha|ust w|you\"   #>  [3] \"h| also|rom P|om P |ve no|u and|he J |N N N|the J|V our| here|ve a\"    #>  [4] \"|ith y|re al|few N|ew N | so i|ou an|e a N|P P i|P on |th yo| V my|V\"  #>  [5] \"my |e N w| , so|me to| few |eithe|ither| him |V it | P on|out w|at i\"  #>  [6] \"|t kno| may |P P V| N fr|u to |N fro|e is |V wit| V it|P for|, i h|\"   #>  [7] \"is o|u nee|ou to| V wi|s for| P fo|a N a|do no|h you| P in|P in | B ,\" #>  [8] \"| J N | me t| 'll |you n|ou ne|o not|, N a|t to |n N .|N in |o be |ke\" #>  [9] \"a |n P ,| N or|N or | look|t wan|ave a| it .|ave n|o V i|r J N|e an\"   #> [10] \"|as i |P D .|e tha|is J |o V w|r N w| , i |N who|t V a|e N s|se le| N\" #> [11] \"on|nd th|s you|ng th| is a|N N w|ase l| one | can |e hav| 's N|'s N |\" #> [12] \"i wo|i was|in V |e let| at t|are a| our |t the|N on |i hav|is V | as\"  #> [13] \"i|e J N|you t| to m|P 's |P P o|ake a|e thi|s N N| a N |in a |n you|\"  #> [14] \"you |hat P|o see| week|week |r N o|nt to| N if|N if |or V |N for|ut i\" #> [15] \"| N J |the N|he N | into|into |o hav| P 's|hat y|n our|if yo| N fo|to\" #> [16] \"do|o do | is J|P P .| my N| N in|me N |t you| i ha| do n| get |an V\"   #> [17] \"|e N N|V tha| i am|e P P|you N|ant t| on P| is V| i V |ome N|ore\"      #> [18] \"N|ave b|other|with | a J |at yo| N i | we w| the |did n|thing|hing |e\" #> [19] \"wil| J in|J in |on P |to me|o me |s the|ed to|or P |ve V |my N |to\"    #> [20] \"se| in a| but | , th| P , |t is | see |i wil|this |ou N |V thi|or N |\" #> [21] \"have|is a |N has|J to |have |i V t|o V o| V a | V fo|ther |e V i|i am\" #> [22] \"| J fo|J for|is N |a J N|V and|V on |as V |V for|N , h|can V|J N\"      #> [23] \"N|his N| J , |J N .| N th|f you|some | N wi| V an| with| be J|be J |d\" #> [24] \"the|V to |V in |but i| star|start|ng on|g on |a N t| N wh| just|r\"     #> [25] \"you| V to|re N | this| in P|P to |ow if|w if |e you| for |to be| V N\"  #> [26] \"|P P P|and t|in P | work|our J|ur J | to b| P to| and |e als|r N i|ou\" #> [27] \"wo|i 'll|ch of|o V .|ach o|r D N|ase V|se V |h of |o mak|e and|to\"     #> [28] \"ma|nning|e V y|o the|ill V|and g|y N N|o V u|P tha|look | out\"         #> [29] \"|ether|ve an| it t|t i w|ere w|o V y|ning |o J N| by N|e nee|t V i|,\"  #> [30] \"as |e to | if y|by N | we n|P , a| give| N P | , as|N we |just |e\"     #> [31] \"any|V you|we wo| N V | each|our N|ur N |each |ill h|pleas|lease|ease\"  #> [32] \"|you a|e wou|e in |for D|here | she |needs|eeds | P N |tart |d you|\"   #> [33] \"othe|for N| plea| both|your |ll V |eek .|make | , we|, we | V yo|or D\" #> [34] \"|r N a|both |V any|a N f|worki|orkin|rking|N V N|re J | P V |n the|we\" #> [35] \"ne|o V N|N N a|more | more|s to |ld li| afte| V on|s tha| P D | J\"     #> [36] \"to|in th|any N|ny N |P , w| me a|fter |V the|e V a|be V | be V| who |\" #> [37] \"in t|give | , yo|, you| need|is th| call| did |at we| what|d to |V a\"  #> [38] \"N| any |d lik|after|at P |r N .| has | N we|her N| N , |s a N|ing a|N\" #> [39] \"is | of N|of N |ou ar|u are| V in|would|P P a|ing o|of yo| two |you\"   #> [40] \"w|N N i| your|r N N|t thi|we ar| N is|N N ,| P is| is t|y N o| we V|\"  #> [41] \"a fe|a few|ke to|i wou|r our|uld l|th N |ike t|V P P|e J t|like |\"     #> [42] \"like|e if |ow wh| woul| N by|N by |e V f|ee if|for h|y hav|, ple| ,\"   #> [43] \"pl| a D |V N o| find|find |now w|hat h|ted t|you m|i nee|N , s|th P |\" #> [44] \", bu|, but|e V t|lso V|so V |ith P|eed t| to V|o V t|to V |to th|ill\"  #> [45] \"n| on t|ould | make| let |P and| per |f the|d P P| N yo|you s|V up |\"  #> [46] \"P ha|and h|s N w| P an|N you|or th|e N o|not V|V J N|nd N |ot V | to\"  #> [47] \"s|for t|er N |e bee|ve be|see i|and N| will|are V|n to |ne of|them |\"  #> [48] \"them|N tha|one o|me of|id no|d not|f N .|t wit|N so |o V a|to ta| N\"   #> [49] \"so|e J .|e of |ome o|king |m to |l hav| i wa| of y|ll ha|we ha|a N\"    #> [50] \"o|u V t|need | we '|, P ,| N ar|N are| was | to g|hat N| from|r the|\"  #> [51] \"N B |from | N N |it is|take |to P |at th| to P|e N .|e V .|he P |the\"  #> [52] \"P| V th\" concordance(Q, K, R, search = \", her\", token.type = \"character\") |>    dplyr::select(pre, node, post, authorship) #>      pre  node  post authorship #> 1   . P  , her e is           Q #> 2  we V  , her e is           Q #> 3   . P  , her e is           Q #> 4  ur N  , her e is           K #> 5   . P  , her e is           K #> 6   . P  , her e is           K #> 7  nd P  , her e is           K #> 8   . P  , her e is           K #> 9   N N  , her e is           K #> 10 st P  , her e is           K #> 11  N N  , her e is           K #> 12    P  , her e 's   Reference #> 13  P P  , her e out  Reference #> 14 nd P  , her e is   Reference #> 15  . P  , her e are  Reference #> 16  . P  , her e are  Reference concordance(Q, K, R, search = \", here is\", token.type = \"word\") |>    dplyr::select(pre, node, post, authorship) #>                 pre      node                   post authorship #> 1   each of you . P , here is           the P P P on          Q #> 2    on this N we V , here is          a N N of both          Q #> 3       N for P . P , here is            the P P P P          Q #> 4  all . per your N , here is          some J N on P          K #> 5       B and V . P , here is           the J N N on          K #> 6         N V N . P , here is            the P P P P          K #> 7       N . P and P , here is        the J for the P          K #> 8       N N yet . P , here is            the P P P P          K #> 9    out of the N N , here is the beginning of the N          K #> 10 N on this past P , here is  the N we talked about          K #> 11   as per our N N , here is            the V P P P          K #> 12   know . P and P , here is            a N of N in  Reference concordance(Q, K, R, search = \"lso ,\", token.type = \"character\") |>    dplyr::select(pre, node, post, authorship) #>      pre  node  post authorship #> 1  N ? a lso ,  ther          Q #> 2  V . a lso ,  i ha          Q #> 3  P . a lso ,  plea          Q #> 4  k . a lso ,  P ha          Q #> 5  N . a lso ,  P P           K #> 6  N . a lso ,  at t          K #> 7  N ? a lso ,  i 'v          K #> 8  s . a lso ,  let           K #> 9      a lso ,  V th          K #> 10 N . a lso ,  i V           K #> 11 N . a lso ,  woul  Reference #> 12 . P a lso ,  V to  Reference #> 13 N . a lso ,  any   Reference #> 14 N . a lso ,  have  Reference #> 15 P . a lso ,  P P   Reference #> 16 N . a lso ,  you   Reference #> 17 D . a lso ,  to V  Reference #> 18 N . a lso ,  tell  Reference #> 19 J . a lso ,  coul  Reference #> 20 N . a lso ,  we n  Reference #> 21 J . a lso ,  ther  Reference"},{"path":"https://andreanini.github.io/idiolect/dev/articles/idiolect.html","id":"conclusions","dir":"Articles","previous_headings":"Analysis of QQ","what":"Conclusions","title":"idiolect","text":"Although score assigned QQ high, depending calibration data, can correspond various magnitudes LLRLLR. LLRLLR value QQ can also plotted onto TRUE vs. FALSE distributions using second argument density_plot(). q argument can used draw black vertical line crosses two distributions horizontal axis corresponding score QQ.  perform calibration calibrate_LLR() function used using validation results calibration data function returns LLRLLR value also verbal labels interpretation (Marquis et al. 2016). final conclusion analysis therefore following: similarity score QQ given KK 0.975, corresponds LLR=LLR= 1.432. similarity 27.04 times likely observed case HpH_p case HdH_d. Therefore, linguistic analysis offers Moderate support HpH_p. conclusion can complemented explanation implication results trier facts showing table posterior probabilities assuming range prior probabilities. can done posterior() function inserting input value LLRLLR table reveals , assuming prior probability HpH_p 0.00001 (roughly, one population Manchester), LLRLLR transform probability posterior probability HpH_p 0.000027. words, make much substantial difference trial. However, prior probability HpH_p 0.5, results turn 0.96, substantial change. table shows present evidence change probability HpH_p true equal higher 0.9 prior greater 0.2.","code":"density_plot(res, q = q.res$score) q.llr <- calibrate_LLR(res, q.res, latex = T) q.llr$`Verbal label` #> [1] \"Moderate support for $H_p$\" strwrap(q.llr$Interpretation) #> [1] \"The similarity is 27.04 times more likely to be observed in the case of\" #> [2] \"$H_p$ than in the case of $H_d$\" posterior(q.llr$LLR) |>    dplyr::select(prosecution_prior_probs, prosecution_post_probs) #> # A tibble: 11 × 2 #>    prosecution_prior_probs prosecution_post_probs #>                      <dbl>                  <dbl> #>  1                0.000001              0.0000270 #>  2                0.01                  0.215     #>  3                0.1                   0.750     #>  4                0.2                   0.871     #>  5                0.3                   0.921     #>  6                0.4                   0.947     #>  7                0.5                   0.964     #>  8                0.6                   0.976     #>  9                0.7                   0.984     #> 10                0.8                   0.991     #> 11                0.9                   0.996"},{"path":"https://andreanini.github.io/idiolect/dev/articles/idiolect.html","id":"acknowledgements","dir":"Articles","previous_headings":"","what":"Acknowledgements","title":"idiolect","text":"like thank Shunichi Ishihara Marie Bojsen-Møller helpful comments first draft vignette.","code":""},{"path":[]},{"path":"https://andreanini.github.io/idiolect/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Andrea Nini. Author, maintainer, copyright holder. David van Leeuwen. Copyright holder.           Author bundled functions package ROC","code":""},{"path":"https://andreanini.github.io/idiolect/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Andrea Nini (2024). Idiolect: R package forensic authorship analysis. https://andreanini.github.io/idiolect/.","code":"@Manual{,   title = {Idiolect: An R package for forensic authorship analysis},   author = {{Andrea Nini}},   year = {2024},   url = {https://andreanini.github.io/idiolect/}, }"},{"path":"https://andreanini.github.io/idiolect/dev/index.html","id":"idiolect-","dir":"","previous_headings":"","what":"Forensic Authorship Analysis","title":"Forensic Authorship Analysis","text":"idiolect R package designed provide comprehensive suite tools performing comparative authorship analysis within forensic context using Likelihood Ratio Framework (e.g. Ishihara 2021; Nini 2023). package contains set authorship analysis functions take set texts input output scores can calibrated likelihood ratios. package dependent quanteda (Benoit et al. 2018) Natural Language Processing functions.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Forensic Authorship Analysis","text":"can install idiolect CRAN:","code":"install.packages(\"idiolect\")"},{"path":"https://andreanini.github.io/idiolect/dev/index.html","id":"workflow","dir":"","previous_headings":"","what":"Workflow","title":"Forensic Authorship Analysis","text":"main functions contained package reflect typical workflow authorship analysis forensic problems: Input data using create_corpus(); Optionally mask content/topic texts using contentmask(); Launch analysis (e.g. delta(), ngram_tracing(), impostors()); Test performance method ground truth data using performance(); Finally, apply method questioned text generate likelihood ratio calibrate_LLR(). Check website vignette examples.","code":""},{"path":[]},{"path":"https://andreanini.github.io/idiolect/dev/reference/calibrate_LLR.html","id":null,"dir":"Reference","previous_headings":"","what":"Calibrate scores into Log-Likelihood Ratios — calibrate_LLR","title":"Calibrate scores into Log-Likelihood Ratios — calibrate_LLR","text":"function used transform scores returned authorship analysis functions Log-Likelihood Ratio (LLR).","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/calibrate_LLR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calibrate scores into Log-Likelihood Ratios — calibrate_LLR","text":"","code":"calibrate_LLR(calibration.dataset, dataset, latex = FALSE)"},{"path":"https://andreanini.github.io/idiolect/dev/reference/calibrate_LLR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calibrate scores into Log-Likelihood Ratios — calibrate_LLR","text":"calibration.dataset data frame containing calibration dataset, typically output authorship analysis function like impostors(). dataset data frame containing scores calibrated LLRs using calibration dataset. typically result applying function like impostors() Q texts. latex logical value. FALSE (default), hypothesis labels printed plain text (Hp/Hd). TRUE labels written read LaTeX ($H_p$/$H_d$).","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/calibrate_LLR.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calibrate scores into Log-Likelihood Ratios — calibrate_LLR","text":"function returns data frame LLRs (base 10), well verbal label according Marquis et al (2016) verbal interpretation results.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/calibrate_LLR.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calibrate scores into Log-Likelihood Ratios — calibrate_LLR","text":"Marquis, Raymond, Alex Biedermann, Liv Cadola, Christophe Champod, Line Gueissaz, Geneviève Massonnet, Williams David Mazzella, Franco Taroni & Tacha Hicks. 2016. Discussion implement verbal scale forensic laboratory: Benefits, pitfalls suggestions avoid misunderstandings. Science & Justice 56(5). 364–370. https://doi.org/10.1016/j.scijus.2016.05.009.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/calibrate_LLR.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calibrate scores into Log-Likelihood Ratios — calibrate_LLR","text":"","code":"calib <- data.frame(score = c(0.5, 0.2, 0.8, 0.01, 0.6), target = c(TRUE, FALSE, TRUE, FALSE, TRUE)) q <- data.frame(score = c(0.6, 0.002)) calibrate_LLR(calib, q) #>   score     LLR                    Verbal label #> 1 0.600  16.135 Extremely strong support for Hp #> 2 0.002 -22.723 Extremely strong support for Hd #>                                                                                                     Interpretation #> 1    The similarity is 13645831365889294 times more likely to be observed in the case of Hp than in the case of Hd #> 2 The similarity is 5.28445251775179e+22 times more likely to be observed in the case of Hd than in the case of Hp"},{"path":"https://andreanini.github.io/idiolect/dev/reference/chunk_texts.html","id":null,"dir":"Reference","previous_headings":"","what":"Chunk a corpus — chunk_texts","title":"Chunk a corpus — chunk_texts","text":"function can used chunk corpus order control sample sizes.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/chunk_texts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chunk a corpus — chunk_texts","text":"","code":"chunk_texts(corpus, size)"},{"path":"https://andreanini.github.io/idiolect/dev/reference/chunk_texts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chunk a corpus — chunk_texts","text":"corpus quanteda corpus. size size chunks number tokens.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/chunk_texts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chunk a corpus — chunk_texts","text":"quanteda corpus object text chunk size requested.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/chunk_texts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chunk a corpus — chunk_texts","text":"","code":"corpus <- quanteda::corpus(c(\"The cat sat on the mat\", \"The dog sat on the chair\")) quanteda::docvars(corpus, \"author\") <- c(\"A\", \"B\") chunk_texts(corpus, size = 2) #> Corpus consisting of 6 documents and 1 docvar. #> text1.1 : #> \"The cat\" #>  #> text1.2 : #> \"sat on\" #>  #> text1.3 : #> \"the mat\" #>  #> text2.1 : #> \"The dog\" #>  #> text2.2 : #> \"sat on\" #>  #> text2.3 : #> \"the chair\" #>"},{"path":"https://andreanini.github.io/idiolect/dev/reference/concordance.html","id":null,"dir":"Reference","previous_headings":"","what":"Qualitative examination of evidence — concordance","title":"Qualitative examination of evidence — concordance","text":"function uses quanteda::kwic() return concordance search pattern. function takes input three datasets pattern returns data frame hits labelled authorship.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/concordance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qualitative examination of evidence — concordance","text":"","code":"concordance(   q.data,   k.data,   reference.data,   search,   token.type = \"word\",   window = 5,   case_insensitive = TRUE )"},{"path":"https://andreanini.github.io/idiolect/dev/reference/concordance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qualitative examination of evidence — concordance","text":"q.data quanteda corpus object, output create_corpus(), tokens object tokens sentences, output tokenize_sents(). k.data quanteda corpus object, output create_corpus(), tokens object tokens sentences, output tokenize_sents(). reference.data quanteda corpus object, output create_corpus(), tokens object tokens sentences, output tokenize_sents(). optional. search string. can sequence characters also accepts use * wildcard. special tokens sentence boundaries 'BOS' beginning sentence 'EOS' end sentence. token.type Choice \"word\" (default), searches word punctuation mark tokens, \"character\", instead uses single character search. window number context items displayed around keyword (quanteda::kwic() parameter). case_insensitive Logical; TRUE, ignore case (quanteda::kwic() parameter).","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/concordance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Qualitative examination of evidence — concordance","text":"function returns data frame containing concordances search pattern.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/concordance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qualitative examination of evidence — concordance","text":"","code":"concordance(enron.sample[1], enron.sample[2], enron.sample[3:49], \"wants to\", token.type = \"word\") #>                 docname from  to                 pre     node           post #> 1 known [Kh Mail_1].txt    5   6             N N N N wants to be N when he V #> 2 known [Ld Mail_5].txt  160 161          D S D . he wants to   V to V the N #> 3 known [Lb Mail_1].txt  573 574 our N . anyone that wants to    V us is J . #>   authorship #> 1          Q #> 2  Reference #> 3  Reference  #using wildcards concordance(enron.sample[1], enron.sample[2], enron.sample[3:49], \"want * to\", token.type = \"word\") #>                 docname from  to                pre       node #> 1 known [Kw Mail_2].txt  672 674 let me know if you want me to #> 2 known [Lc Mail_5].txt  175 177       s N . if you want me to #> 3 known [Ml Mail_5].txt  242 244  need . you do n't want me to #>                    post authorship #> 1       V on other N in  Reference #> 2        , i can put on  Reference #> 3 come work for you too  Reference  #searching character sequences with wildcards concordance(enron.sample[1], enron.sample[2], enron.sample[3:49], \"help*\", token.type = \"character\") #>                    docname from   to   pre  node  post authorship #> 1    known [Kh Mail_1].txt  703  707 need  help  V it           Q #> 2    known [Kh Mail_1].txt 2014 2018 want  help  V it           Q #> 3    known [Kh Mail_3].txt 1797 1801  N ,  helpe d the          K #> 4    known [Kh Mail_4].txt   52   56  P P  helpe d the  Reference #> 5  unknown [Kw Mail_3].txt 2756 2760 ding  help  in th  Reference #> 6    known [Kw Mail_5].txt   31   35 your  help  and N  Reference #> 7    known [Kw Mail_5].txt 1463 1467 need  help  in do  Reference #> 8    known [Lc Mail_2].txt 1600 1604 some  help  . why  Reference #> 9    known [Lc Mail_5].txt 1163 1167 d of  help  and B  Reference #> 10   known [Ld Mail_2].txt  285  289 ally  help  us ou  Reference #> 11   known [Lt Mail_1].txt  884  888 r be  helpi ng to  Reference #> 12   known [Lt Mail_1].txt  919  923 , or  help  V a N  Reference #> 13   known [Lt Mail_3].txt  910  914 your  help  as a   Reference #> 14   known [Lt Mail_4].txt 1611 1615 ttle  help  from   Reference #> 15 unknown [Lk Mail_4].txt 1243 1247 N to  help  V N f  Reference #> 16 unknown [Lk Mail_4].txt 1272 1276 N to  help  V the  Reference #> 17   known [Lk Mail_1].txt 1512 1516 ease  help  him w  Reference #> 18   known [Lk Mail_2].txt  387  391 ight  help  . ple  Reference #> 19   known [Lk Mail_3].txt  994  998 ease  help  him a  Reference #> 20 unknown [Lb Mail_3].txt 2279 2283 N to  help  our N  Reference #> 21 unknown [Lb Mail_3].txt 2405 2409  and  help  the N  Reference #> 22 unknown [Lb Mail_3].txt 2479 2483 g to  help  out w  Reference #> 23 unknown [Lb Mail_3].txt 2617 2621  and  help  them   Reference #> 24   known [Lb Mail_1].txt 1363 1367 d to  help  you i  Reference #> 25   known [Lb Mail_2].txt 1652 1656  and  help  in V   Reference #> 26   known [Lb Mail_2].txt 1676 1680  and  helpi ng ea  Reference #> 27   known [Lb Mail_4].txt 1038 1042 e to  help  P N a  Reference #> 28   known [Lb Mail_5].txt 1066 1070 this  helps  V ou  Reference #> 29   known [La Mail_2].txt 2086 2090 ould  help  V the  Reference #> 30   known [La Mail_2].txt 2494 2498 ould  help  get t  Reference #> 31   known [La Mail_4].txt 1908 1912 also  help  V N .  Reference #> 32   known [La Mail_5].txt 2424 2428 will  help  the N  Reference #> 33   known [Mf Mail_2].txt  805  809 your  help  . thi  Reference #> 34   known [Mf Mail_2].txt 2097 2101  any  help  you c  Reference #> 35   known [Mf Mail_2].txt 2458 2462  can  help  with   Reference #> 36   known [Ml Mail_1].txt  596  600  you  help  and V  Reference #> 37   known [Ml Mail_1].txt 1223 1227 your  help  and l  Reference #> 38   known [Ml Mail_1].txt 2492 2496  and  help  save   Reference #> 39   known [Ml Mail_1].txt 2598 2602 your  help  and V  Reference #> 40   known [Ml Mail_2].txt   12   16 your  help  and V  Reference #> 41   known [Ml Mail_4].txt  622  626 your  help  in V   Reference #> 42   known [Ml Mail_4].txt 1296 1300 ou N  helpe d us   Reference #> 43   known [Ml Mail_4].txt 1589 1593 your  help  so th  Reference #> 44   known [Ml Mail_4].txt 1962 1966 your  help  and V  Reference #> 45   known [Ml Mail_5].txt  475  479 an B  help  you V  Reference  #using sentences enron.sents <- tokens(enron.sample, \"sentence\") concordance(enron.sents[1], enron.sents[2], enron.sents[3:49], \". _EOS_\", token.type = \"word\") #>                     docname from   to                              pre    node #> 1     known [Kh Mail_1].txt  114  115                      N , but V D . _EOS_ #> 2     known [Kh Mail_1].txt  160  161                   D N in first N . _EOS_ #> 3     known [Kh Mail_1].txt  189  190                    N about a N N . _EOS_ #> 4     known [Kh Mail_1].txt  369  370                    and we V to V . _EOS_ #> 5     known [Kh Mail_1].txt  409  410                  ' re V with her . _EOS_ #> 6     known [Kh Mail_1].txt  545  546                   the N of the N . _EOS_ #> 7     known [Kh Mail_1].txt  698  699         V in you getting neither . _EOS_ #> 8     known [Kh Mail_1].txt  713  714              just the N it works . _EOS_ #> 9     known [Kh Mail_1].txt  735  736                       N is a J N . _EOS_ #> 10    known [Kh Mail_1].txt  873  874             have to say during N . _EOS_ #> 11    known [Kh Mail_3].txt  173  174                       J as a J N . _EOS_ #> 12    known [Kh Mail_3].txt  235  236              get V into the call . _EOS_ #> 13    known [Kh Mail_3].txt  285  286             , please let me know . _EOS_ #> 14    known [Kh Mail_3].txt  341  342                   you who V of N . _EOS_ #> 15    known [Kh Mail_3].txt  366  367                      B for a N N . _EOS_ #> 16    known [Kh Mail_3].txt  410  411            P went well last week . _EOS_ #> 17    known [Kh Mail_3].txt  560  561                    of N in its N . _EOS_ #> 18    known [Kh Mail_3].txt  599  600                    for the B J N . _EOS_ #> 19    known [Kh Mail_3].txt  625  626                      N N and N N . _EOS_ #> 20    known [Kh Mail_3].txt  715  716                        N J J J N . _EOS_ #> 21    known [Kh Mail_4].txt  222  223                   , and V your N . _EOS_ #> 22    known [Kh Mail_4].txt  302  303                   J N with the N . _EOS_ #> 23    known [Kh Mail_4].txt  666  667                    V for a few N . _EOS_ #> 24    known [Kh Mail_4].txt  716  717        especially on the first N . _EOS_ #> 25    known [Kh Mail_4].txt  754  755                    her to N on N . _EOS_ #> 26    known [Kh Mail_4].txt  774  775                after the N has V . _EOS_ #> 27    known [Kh Mail_4].txt  785  786                    to N on the N . _EOS_ #> 28    known [Kh Mail_5].txt   31   32                      can V a J N . _EOS_ #> 29    known [Kh Mail_5].txt  111  112                     to V B N too . _EOS_ #> 30    known [Kh Mail_5].txt  193  194                     the N on P N . _EOS_ #> 31    known [Kh Mail_5].txt  214  215                    J for the P N . _EOS_ #> 32    known [Kh Mail_5].txt  248  249                       P N N by N . _EOS_ #> 33    known [Kh Mail_5].txt  319  320                      was P D , D . _EOS_ #> 34    known [Kh Mail_5].txt  392  393                    in V on the N . _EOS_ #> 35    known [Kh Mail_5].txt  546  547                see if they are J . _EOS_ #> 36    known [Kh Mail_5].txt  563  564                a N a N afterward . _EOS_ #> 37    known [Kh Mail_5].txt  666  667                  N N B next week . _EOS_ #> 38    known [Kh Mail_5].txt  767  768                  N as this one N . _EOS_ #> 39    known [Kh Mail_5].txt  813  814                        P P P P N . _EOS_ #> 40    known [Kh Mail_5].txt  837  838                     s very J N N . _EOS_ #> 41    known [Kh Mail_5].txt  850  851                      N who B V N . _EOS_ #> 42  unknown [Kh Mail_2].txt   37   38               call me with any N . _EOS_ #> 43  unknown [Kh Mail_2].txt   54   55                 N you feel are J . _EOS_ #> 44  unknown [Kh Mail_2].txt  128  129                        P N N N N . _EOS_ #> 45  unknown [Kh Mail_2].txt  758  759                       V at J N N . _EOS_ #> 46  unknown [Kh Mail_2].txt  817  818                      , J N , etc . _EOS_ #> 47  unknown [Kh Mail_2].txt  833  834                        V J N N N . _EOS_ #> 48  unknown [Kw Mail_3].txt  171  172                   the N of the N . _EOS_ #> 49  unknown [Kw Mail_3].txt  288  289                   after D N of N . _EOS_ #> 50  unknown [Kw Mail_3].txt  400  401                   get V in the N . _EOS_ #> 51  unknown [Kw Mail_3].txt  445  446                   to V it to you . _EOS_ #> 52  unknown [Kw Mail_3].txt  477  478               V with each of you . _EOS_ #> 53  unknown [Kw Mail_3].txt  528  529                       to V a J N . _EOS_ #> 54  unknown [Kw Mail_3].txt  550  551                 a start to the P . _EOS_ #> 55  unknown [Kw Mail_3].txt  639  640                    an N from P P . _EOS_ #> 56  unknown [Kw Mail_3].txt  997  998                 few more N for P . _EOS_ #> 57  unknown [Kw Mail_3].txt 1072 1073                       N N on P D . _EOS_ #> 58    known [Kw Mail_1].txt   47   48                  V for at this N . _EOS_ #> 59    known [Kw Mail_1].txt   83   84                  of the N for us . _EOS_ #> 60    known [Kw Mail_1].txt  208  209              be getting all my N . _EOS_ #> 61    known [Kw Mail_1].txt  257  258                    V and V to me . _EOS_ #> 62    known [Kw Mail_1].txt  357  358              came in several N B . _EOS_ #> 63    known [Kw Mail_1].txt  551  552              J N starting next P . _EOS_ #> 64    known [Kw Mail_1].txt  632  633                 they were V in P . _EOS_ #> 65    known [Kw Mail_1].txt  707  708                could get B and V . _EOS_ #> 66    known [Kw Mail_1].txt  780  781                       on P , P D . _EOS_ #> 67    known [Kw Mail_1].txt  834  835                    my N last P N . _EOS_ #> 68    known [Kw Mail_1].txt  853  854                V that it was you . _EOS_ #> 69    known [Kw Mail_1].txt  993  994              are seeing on the N . _EOS_ #> 70    known [Kw Mail_2].txt   46   47                    we have N V N . _EOS_ #> 71    known [Kw Mail_2].txt   90   91                 we can V as well . _EOS_ #> 72    known [Kw Mail_2].txt  146  147                   you V on our N . _EOS_ #> 73    known [Kw Mail_2].txt  201  202                  who V all the N . _EOS_ #> 74    known [Kw Mail_2].txt  245  246               start the next N N . _EOS_ #> 75    known [Kw Mail_2].txt  372  373                       on P , P D . _EOS_ #> 76    known [Kw Mail_2].txt  418  419                if you have any N . _EOS_ #> 77    known [Kw Mail_2].txt  669  670           again if everyone is N . _EOS_ #> 78    known [Kw Mail_2].txt  715  716                     ' ll V the N . _EOS_ #> 79    known [Kw Mail_2].txt  765  766             want this N V around . _EOS_ #> 80    known [Kw Mail_2].txt  909  910                     N the N is V . _EOS_ #> 81    known [Kw Mail_4].txt   10   11                     the J N on D . _EOS_ #> 82    known [Kw Mail_4].txt   34   35                 this N in your N . _EOS_ #> 83    known [Kw Mail_4].txt   55   56                      a D N N yet . _EOS_ #> 84    known [Kw Mail_4].txt   78   79                if you have any N . _EOS_ #> 85    known [Kw Mail_4].txt  150  151                     the N by N N . _EOS_ #> 86    known [Kw Mail_4].txt  292  293               with N to the move . _EOS_ #> 87    known [Kw Mail_4].txt  364  365                   N when the N V . _EOS_ #> 88    known [Kw Mail_4].txt  464  465                      - N N , etc . _EOS_ #> 89    known [Kw Mail_4].txt  480  481                     N to V for N . _EOS_ #> 90    known [Kw Mail_4].txt  523  524                       to P P N N . _EOS_ #> 91    known [Kw Mail_4].txt  580  581               to V from you soon . _EOS_ #> 92    known [Kw Mail_4].txt  599  600                    P N in last N . _EOS_ #> 93    known [Kw Mail_4].txt  612  613                  the N of this N . _EOS_ #> 94    known [Kw Mail_4].txt  645  646                 with P as they V . _EOS_ #> 95    known [Kw Mail_4].txt  689  690           this if you would like . _EOS_ #> 96    known [Kw Mail_4].txt  808  809                  before i V to P . _EOS_ #> 97    known [Kw Mail_4].txt  849  850             well as the N itself . _EOS_ #> 98    known [Kw Mail_5].txt   62   63             i called them this N . _EOS_ #> 99    known [Kw Mail_5].txt  209  210                  P P P next week . _EOS_ #> 100   known [Kw Mail_5].txt  245  246                    i can be of N . _EOS_ #> 101   known [Kw Mail_5].txt  268  269                 the N for next P . _EOS_ #> 102   known [Kw Mail_5].txt  356  357                  and V up this N . _EOS_ #> 103   known [Kw Mail_5].txt  797  798                 n't V to call me . _EOS_ #> 104 unknown [Lc Mail_1].txt   51   52                    V was B one N . _EOS_ #> 105 unknown [Lc Mail_1].txt   93   94               through of the J N . _EOS_ #> 106 unknown [Lc Mail_1].txt  109  110                     J N of the N . _EOS_ #> 107 unknown [Lc Mail_1].txt  212  213                    J N of both N . _EOS_ #> 108 unknown [Lc Mail_1].txt  488  489                       V to P P N . _EOS_ #> 109 unknown [Lc Mail_1].txt  537  538                     and V by J N . _EOS_ #> 110 unknown [Lc Mail_1].txt  695  696                   like J N to me . _EOS_ #> 111 unknown [Lc Mail_1].txt  838  839                     N of the N N . _EOS_ #> 112   known [Lc Mail_2].txt  111  112                     will V N V N . _EOS_ #> 113   known [Lc Mail_2].txt  153  154                showing the N N N . _EOS_ #> 114   known [Lc Mail_2].txt  191  192                   the J N near P . _EOS_ #> 115   known [Lc Mail_2].txt  249  250                       on P D , D . _EOS_ #> 116   known [Lc Mail_2].txt  276  277                      J N N V etc . _EOS_ #> 117   known [Lc Mail_2].txt  392  393                   was V to the N . _EOS_ #> 118   known [Lc Mail_2].txt  486  487                    V on to the N . _EOS_ #> 119   known [Lc Mail_2].txt  511  512                    N of N were V . _EOS_ #> 120   known [Lc Mail_2].txt  528  529                        P J N N N . _EOS_ #> 121   known [Lc Mail_2].txt  542  543                        P J N N N . _EOS_ #> 122   known [Lc Mail_2].txt  562  563                        P J N N N . _EOS_ #> 123   known [Lc Mail_2].txt  778  779                      V V V the N . _EOS_ #> 124   known [Lc Mail_3].txt   13   14                     V on the P N . _EOS_ #> 125   known [Lc Mail_3].txt   98   99                   the N by the N . _EOS_ #> 126   known [Lc Mail_3].txt  326  327                   N would be B V . _EOS_ #> 127   known [Lc Mail_3].txt  411  412                     N N V as yet . _EOS_ #> 128   known [Lc Mail_3].txt  427  428                     N of the P N . _EOS_ #> 129   known [Lc Mail_3].txt  443  444                     of N J for N . _EOS_ #> 130   known [Lc Mail_3].txt  472  473                   to give P my N . _EOS_ #> 131   known [Lc Mail_3].txt  481  482              are B J around here . _EOS_ #> 132   known [Lc Mail_3].txt  874  875                 is part of the N . _EOS_ #> 133   known [Lc Mail_4].txt   52   53                   may V us to do . _EOS_ #> 134   known [Lc Mail_4].txt   93   94                      D and D N N . _EOS_ #> 135   known [Lc Mail_4].txt  121  122                     V with a J N . _EOS_ #> 136   known [Lc Mail_4].txt  282  283                the N of the week . _EOS_ #> 137   known [Lc Mail_4].txt  356  357                  have any N or N . _EOS_ #> 138   known [Lc Mail_4].txt  650  651                    N and V any N . _EOS_ #> 139   known [Lc Mail_4].txt  819  820             the N that are there . _EOS_ #> 140   known [Lc Mail_5].txt   29   30       once the N actually begins . _EOS_ #> 141   known [Lc Mail_5].txt   51   52              B and all went well . _EOS_ #> 142   known [Lc Mail_5].txt   85   86                 for that N and N . _EOS_ #> 143   known [Lc Mail_5].txt  131  132                  N but no more N . _EOS_ #> 144   known [Lc Mail_5].txt  213  214               their N and N also . _EOS_ #> 145   known [Lc Mail_5].txt  247  248                     N of the P P . _EOS_ #> 146   known [Lc Mail_5].txt  465  466                     V on how N V . _EOS_ #> 147   known [Lc Mail_5].txt  664  665                 J to take it out . _EOS_ #> 148   known [Lc Mail_5].txt  792  793                        P P N P P . _EOS_ #> 149   known [Lc Mail_5].txt  814  815                    P for his N N . _EOS_ #> 150 unknown [Ld Mail_4].txt  126  127                if you have any N . _EOS_ #> 151 unknown [Ld Mail_4].txt  152  153                  first N for V N . _EOS_ #> 152 unknown [Ld Mail_4].txt  209  210                N to her during N . _EOS_ #> 153 unknown [Ld Mail_4].txt  246  247                if you have any N . _EOS_ #> 154 unknown [Ld Mail_4].txt  297  298                   J N for your N . _EOS_ #> 155 unknown [Ld Mail_4].txt  329  330                        P P N N N . _EOS_ #> 156 unknown [Ld Mail_4].txt  389  390               through D with a D . _EOS_ #> 157 unknown [Ld Mail_4].txt  651  652                   m J to these N . _EOS_ #> 158 unknown [Ld Mail_4].txt  707  708          soon for N or something . _EOS_ #> 159 unknown [Ld Mail_4].txt  751  752                  to get it all B . _EOS_ #> 160   known [Ld Mail_1].txt   75   76                    of V up our N . _EOS_ #> 161   known [Ld Mail_1].txt   88   89          running this all past P . _EOS_ #> 162   known [Ld Mail_1].txt  135  136          need anything B from me . _EOS_ #> 163   known [Ld Mail_1].txt  170  171                there are any N N . _EOS_ #> 164   known [Ld Mail_1].txt  423  424             N we should get into . _EOS_ #> 165   known [Ld Mail_1].txt  587  588                  to look for a N . _EOS_ #> 166   known [Ld Mail_1].txt  706  707               i could get a look . _EOS_ #> 167   known [Ld Mail_1].txt  769  770                     ' m on D now . _EOS_ #> 168   known [Ld Mail_1].txt  796  797                  a N with this N . _EOS_ #> 169   known [Ld Mail_1].txt  809  810              with P about this N . _EOS_ #> 170   known [Ld Mail_2].txt  147  148             with the overall D N . _EOS_ #> 171   known [Ld Mail_2].txt  207  208           next week to V further . _EOS_ #> 172   known [Ld Mail_2].txt  236  237           you need any further N . _EOS_ #> 173   known [Ld Mail_2].txt  310  311              anything in the N N . _EOS_ #> 174   known [Ld Mail_2].txt  445  446                  in her N this N . _EOS_ #> 175   known [Ld Mail_2].txt  578  579                        s N ' s N . _EOS_ #> 176   known [Ld Mail_2].txt  716  717                      and P ' s N . _EOS_ #> 177   known [Ld Mail_3].txt  231  232                  in these N is J . _EOS_ #> 178   known [Ld Mail_3].txt  267  268                       N B d be V . _EOS_ #> 179   known [Ld Mail_3].txt  289  290                on P on another N . _EOS_ #> 180   known [Ld Mail_3].txt  359  360                  to put in the N . _EOS_ #> 181   known [Ld Mail_3].txt  424  425                V the N through D . _EOS_ #> 182   known [Ld Mail_3].txt  471  472                   to V P as well . _EOS_ #> 183   known [Ld Mail_5].txt   87   88                      , D in P no . _EOS_ #> 184   known [Ld Mail_5].txt  423  424                        , P P P N . _EOS_ #> 185 unknown [Lt Mail_2].txt   68   69                     to P for P P . _EOS_ #> 186 unknown [Lt Mail_2].txt  206  207                       J N to P N . _EOS_ #> 187 unknown [Lt Mail_2].txt  275  276                     have V D D N . _EOS_ #> 188 unknown [Lt Mail_2].txt  354  355            following the N and N . _EOS_ #> 189 unknown [Lt Mail_2].txt  365  366                      V the P D N . _EOS_ #> 190 unknown [Lt Mail_2].txt  450  451                   and N of the N . _EOS_ #> 191 unknown [Lt Mail_2].txt  500  501              N wanting to come N . _EOS_ #> 192 unknown [Lt Mail_2].txt  560  561                 and N from the N . _EOS_ #> 193 unknown [Lt Mail_2].txt  577  578                 of your N more B . _EOS_ #> 194 unknown [Lt Mail_2].txt  767  768                N in your N below . _EOS_ #> 195   known [Lt Mail_1].txt   15   16                   V me on this N . _EOS_ #> 196   known [Lt Mail_1].txt   47   48                    V this N to P . _EOS_ #> 197   known [Lt Mail_1].txt   63   64                   for P V this N . _EOS_ #> 198   known [Lt Mail_1].txt  109  110                  is V for your N . _EOS_ #> 199   known [Lt Mail_1].txt  155  156                  N V J between D . _EOS_ #> 200   known [Lt Mail_1].txt  175  176                      P , P and P . _EOS_ #> 201   known [Lt Mail_1].txt  220  221                   V or V these N . _EOS_ #> 202   known [Lt Mail_1].txt  260  261                        , P , P D . _EOS_ #> 203   known [Lt Mail_1].txt  425  426               talking to you V N . _EOS_ #> 204   known [Lt Mail_1].txt  488  489         N more frequently than P . _EOS_ #> 205   known [Lt Mail_1].txt  523  524                      P for a J N . _EOS_ #> 206   known [Lt Mail_1].txt  557  558                me in P this week . _EOS_ #> 207   known [Lt Mail_1].txt  577  578                      P P etc . , . _EOS_ #> 208   known [Lt Mail_1].txt  596  597                      J in N in P . _EOS_ #> 209   known [Lt Mail_1].txt  630  631                        N N S N N . _EOS_ #> 210   known [Lt Mail_3].txt   11   12                     P are J to V . _EOS_ #> 211   known [Lt Mail_3].txt  106  107                     V by all N N . _EOS_ #> 212   known [Lt Mail_3].txt  140  141                  up some N for P . _EOS_ #> 213   known [Lt Mail_3].txt  189  190            between now and the N . _EOS_ #> 214   known [Lt Mail_3].txt  257  258                , N N or whatever . _EOS_ #> 215   known [Lt Mail_3].txt  285  286                  P from this N N . _EOS_ #> 216   known [Lt Mail_3].txt  422  423                   is only at D N . _EOS_ #> 217   known [Lt Mail_3].txt  514  515                      N and J N N . _EOS_ #> 218   known [Lt Mail_3].txt  615  616                , please call P D . _EOS_ #> 219   known [Lt Mail_3].txt  651  652                 if you have an N . _EOS_ #> 220   known [Lt Mail_4].txt   68   69               V with both of you . _EOS_ #> 221   known [Lt Mail_4].txt   94   95                       on P , P D . _EOS_ #> 222   known [Lt Mail_4].txt  125  126                N that would be J . _EOS_ #> 223   known [Lt Mail_4].txt  142  143                      the D P ' N . _EOS_ #> 224   known [Lt Mail_4].txt  161  162                      N N and N N . _EOS_ #> 225   known [Lt Mail_4].txt  205  206                        N - N N N . _EOS_ #> 226   known [Lt Mail_4].txt  279  280           as we had previously V . _EOS_ #> 227   known [Lt Mail_4].txt  577  578                     for J N at P . _EOS_ #> 228   known [Lt Mail_4].txt  592  593                     N in the J N . _EOS_ #> 229   known [Lt Mail_4].txt  606  607                     in the J N D . _EOS_ #> 230   known [Lt Mail_4].txt  709  710                     can V on a N . _EOS_ #> 231   known [Lt Mail_4].txt  720  721                  for your N to V . _EOS_ #> 232   known [Lt Mail_4].txt  730  731            not V his N regularly . _EOS_ #> 233 unknown [Lk Mail_4].txt   51   52                      J N D N out . _EOS_ #> 234 unknown [Lk Mail_4].txt  120  121             get some of this off . _EOS_ #> 235 unknown [Lk Mail_4].txt  205  206                 needs of our J N . _EOS_ #> 236 unknown [Lk Mail_4].txt  274  275                      P and P P B . _EOS_ #> 237 unknown [Lk Mail_4].txt  352  353                   B with the J N . _EOS_ #> 238 unknown [Lk Mail_4].txt  488  489                 needs of our J N . _EOS_ #> 239 unknown [Lk Mail_4].txt  557  558                      P and P P B . _EOS_ #> 240 unknown [Lk Mail_4].txt  635  636                   B with the J N . _EOS_ #> 241 unknown [Lk Mail_4].txt  672  673                     N in the P P . _EOS_ #> 242 unknown [Lk Mail_4].txt  759  760          V doing the following D . _EOS_ #> 243 unknown [Lk Mail_4].txt  887  888                   go all the N B . _EOS_ #> 244 unknown [Lk Mail_4].txt  949  950                   N to look B to . _EOS_ #> 245   known [Lk Mail_1].txt   21   22              the V N including P . _EOS_ #> 246   known [Lk Mail_1].txt   71   72                if so let me know . _EOS_ #> 247   known [Lk Mail_1].txt  219  220       immediately but within a N . _EOS_ #> 248   known [Lk Mail_1].txt  262  263          N or anything like that . _EOS_ #> 249   known [Lk Mail_1].txt  277  278             need to V up quickly . _EOS_ #> 250   known [Lk Mail_1].txt  445  446                   N V around P D . _EOS_ #> 251   known [Lk Mail_1].txt  464  465                this week P and P . _EOS_ #> 252   known [Lk Mail_1].txt  534  535                    N for the N N . _EOS_ #> 253   known [Lk Mail_1].txt  646  647              you V this work out . _EOS_ #> 254   known [Lk Mail_1].txt  667  668               P N with another N . _EOS_ #> 255   known [Lk Mail_1].txt  698  699                 all those N to P . _EOS_ #> 256   known [Lk Mail_1].txt  709  710                    V N and N etc . _EOS_ #> 257   known [Lk Mail_1].txt  857  858                  off on V that N . _EOS_ #> 258   known [Lk Mail_2].txt   38   39                     have a B J N . _EOS_ #> 259   known [Lk Mail_2].txt  132  133                    will V B or B . _EOS_ #> 260   known [Lk Mail_2].txt  316  317                     not V my N N . _EOS_ #> 261   known [Lk Mail_2].txt  374  375                     to the N N N . _EOS_ #> 262   known [Lk Mail_2].txt  425  426              V anything with J N . _EOS_ #> 263   known [Lk Mail_2].txt  639  640                   them - J and J . _EOS_ #> 264   known [Lk Mail_2].txt  711  712              are J just before N . _EOS_ #> 265   known [Lk Mail_2].txt  745  746                  N will not be V . _EOS_ #> 266   known [Lk Mail_2].txt  781  782                      N is J to N . _EOS_ #> 267   known [Lk Mail_2].txt  804  805                      D N P and P . _EOS_ #> 268   known [Lk Mail_2].txt  849  850               are V inside the P . _EOS_ #> 269   known [Lk Mail_3].txt   46   47                       V to V N N . _EOS_ #> 270   known [Lk Mail_3].txt  165  166               N are also V below . _EOS_ #> 271   known [Lk Mail_3].txt  181  182               for those V from P . _EOS_ #> 272   known [Lk Mail_3].txt  201  202                  V to and from P . _EOS_ #> 273   known [Lk Mail_3].txt  467  468                    N for the J N . _EOS_ #> 274   known [Lk Mail_3].txt  493  494         N but nobody had started . _EOS_ #> 275   known [Lk Mail_3].txt  603  604            of that N as possible . _EOS_ #> 276   known [Lk Mail_3].txt  659  660                 we can V on this . _EOS_ #> 277   known [Lk Mail_3].txt  694  695              start V in one hour . _EOS_ #> 278   known [Lk Mail_5].txt  275  276                       to P D , D . _EOS_ #> 279   known [Lk Mail_5].txt  384  385                   need to V on P . _EOS_ #> 280   known [Lk Mail_5].txt  418  419                    are P P and P . _EOS_ #> 281   known [Lk Mail_5].txt  569  570                 N and other N do . _EOS_ #> 282   known [Lk Mail_5].txt  599  600             therefore V with N N . _EOS_ #> 283   known [Lk Mail_5].txt  631  632                     N is the J N . _EOS_ #> 284   known [Lk Mail_5].txt  672  673                 N is made with N . _EOS_ #> 285   known [Lk Mail_5].txt  759  760                     on our D S D . _EOS_ #> 286 unknown [Lb Mail_3].txt  161  162             , please let me know . _EOS_ #> 287 unknown [Lb Mail_3].txt  217  218                      J N for N N . _EOS_ #> 288 unknown [Lb Mail_3].txt  280  281                 , please let P P . _EOS_ #> 289 unknown [Lb Mail_3].txt  441  442             , please let me know . _EOS_ #> 290 unknown [Lb Mail_3].txt  484  485                if you have any N . _EOS_ #> 291 unknown [Lb Mail_3].txt  508  509           N with the following N . _EOS_ #> 292 unknown [Lb Mail_3].txt  667  668                  N to take any N . _EOS_ #> 293 unknown [Lb Mail_3].txt  679  680                    N for us to V . _EOS_ #> 294 unknown [Lb Mail_3].txt  827  828            them through this J N . _EOS_ #> 295 unknown [Lb Mail_3].txt  893  894                       N of P , D . _EOS_ #> 296   known [Lb Mail_1].txt   78   79             , please let me know . _EOS_ #> 297   known [Lb Mail_1].txt  282  283                    N this N to V . _EOS_ #> 298   known [Lb Mail_1].txt  430  431                   ask P for an N . _EOS_ #> 299   known [Lb Mail_1].txt  523  524                    B D for the N . _EOS_ #> 300   known [Lb Mail_1].txt  559  560                want to V these N . _EOS_ #> 301   known [Lb Mail_1].txt  725  726             , please let me know . _EOS_ #> 302   known [Lb Mail_1].txt  811  812             , please let me know . _EOS_ #> 303   known [Lb Mail_2].txt   70   71                  the week of P N . _EOS_ #> 304   known [Lb Mail_2].txt   88   89            will be out that week . _EOS_ #> 305   known [Lb Mail_2].txt  118  119               did put her N down . _EOS_ #> 306   known [Lb Mail_2].txt  150  151                    for the N N N . _EOS_ #> 307   known [Lb Mail_2].txt  177  178                if you have any N . _EOS_ #> 308   known [Lb Mail_2].txt  204  205                if you have any N . _EOS_ #> 309   known [Lb Mail_2].txt  233  234                if you have any N . _EOS_ #> 310   known [Lb Mail_2].txt  262  263                if you have any N . _EOS_ #> 311   known [Lb Mail_2].txt  290  291                if you have any N . _EOS_ #> 312   known [Lb Mail_2].txt  358  359                    V out the J N . _EOS_ #> 313   known [Lb Mail_2].txt  386  387                her up coming N N . _EOS_ #> 314   known [Lb Mail_2].txt  406  407           in N anything comes up . _EOS_ #> 315   known [Lb Mail_2].txt  456  457                 how many are J D . _EOS_ #> 316   known [Lb Mail_2].txt  465  466                       N of P N N . _EOS_ #> 317   known [Lb Mail_2].txt  589  590                  let P or i know . _EOS_ #> 318   known [Lb Mail_2].txt  615  616                     B to V the N . _EOS_ #> 319   known [Lb Mail_2].txt  744  745                 each other ' s N . _EOS_ #> 320   known [Lb Mail_2].txt  769  770                     P has P as P . _EOS_ #> 321   known [Lb Mail_2].txt  808  809                    need J B up P . _EOS_ #> 322   known [Lb Mail_4].txt   51   52             , please let me know . _EOS_ #> 323   known [Lb Mail_4].txt   91   92             V this N if possible . _EOS_ #> 324   known [Lb Mail_4].txt  128  129                     N in the P P . _EOS_ #> 325   known [Lb Mail_4].txt  157  158                    n't V to V me . _EOS_ #> 326   known [Lb Mail_4].txt  176  177                     on the V N D . _EOS_ #> 327   known [Lb Mail_4].txt  210  211                      the J J N D . _EOS_ #> 328   known [Lb Mail_4].txt  228  229                     N and N on P . _EOS_ #> 329   known [Lb Mail_4].txt  241  242                      P P and N D . _EOS_ #> 330   known [Lb Mail_4].txt  255  256               working with P P D . _EOS_ #> 331   known [Lb Mail_4].txt  281  282                        , P , P D . _EOS_ #> 332   known [Lb Mail_4].txt  305  306                       P P or P P . _EOS_ #> 333   known [Lb Mail_4].txt  356  357               on the N went well . _EOS_ #> 334   known [Lb Mail_4].txt  389  390 which were turned around quickly . _EOS_ #> 335   known [Lb Mail_4].txt  469  470                    then V to P P . _EOS_ #> 336   known [Lb Mail_4].txt  662  663                     N on the N N . _EOS_ #> 337   known [Lb Mail_4].txt  683  684                    not be J to V . _EOS_ #> 338   known [Lb Mail_4].txt  714  715             and get our N across . _EOS_ #> 339   known [Lb Mail_5].txt  129  130                  let P or i know . _EOS_ #> 340   known [Lb Mail_5].txt  155  156                    go to N and V . _EOS_ #> 341   known [Lb Mail_5].txt  273  274                       N at P V D . _EOS_ #> 342   known [Lb Mail_5].txt  348  349                if you have any N . _EOS_ #> 343   known [Lb Mail_5].txt  417  418                 N N during all N . _EOS_ #> 344   known [Lb Mail_5].txt  465  466                        P ' s N D . _EOS_ #> 345   known [Lb Mail_5].txt  540  541                   N with P and P . _EOS_ #> 346   known [Lb Mail_5].txt  560  561                    after P D , D . _EOS_ #> 347   known [Lb Mail_5].txt  596  597                       on P , P N . _EOS_ #> 348   known [Lb Mail_5].txt  620  621                      N N for N N . _EOS_ #> 349   known [Lb Mail_5].txt  683  684             to seeing you next P . _EOS_ #> 350   known [Lb Mail_5].txt  778  779                  go B the next N . _EOS_ #> 351 unknown [La Mail_3].txt   14   15                   up on N in may . _EOS_ #> 352 unknown [La Mail_3].txt  163  164               even some N were J . _EOS_ #> 353 unknown [La Mail_3].txt  273  274                   per N - hour N . _EOS_ #> 354 unknown [La Mail_3].txt  350  351              be needed to V that . _EOS_ #> 355 unknown [La Mail_3].txt  447  448          that does anybody any N . _EOS_ #> 356 unknown [La Mail_3].txt  460  461              V similarly J and J . _EOS_ #> 357 unknown [La Mail_3].txt  515  516                     N in the J N . _EOS_ #> 358 unknown [La Mail_3].txt  634  635                   make the N N V . _EOS_ #> 359 unknown [La Mail_3].txt  693  694                     V of V the N . _EOS_ #> 360 unknown [La Mail_3].txt  836  837                  V one that is J . _EOS_ #> 361 unknown [La Mail_3].txt  897  898                     to V the J N . _EOS_ #> 362 unknown [La Mail_3].txt  939  940                   N will V the N . _EOS_ #> 363   known [La Mail_1].txt  356  357                       J N to V N . _EOS_ #> 364   known [La Mail_1].txt  412  413                        J N - J N . _EOS_ #> 365   known [La Mail_1].txt  546  547                   for N , P said . _EOS_ #> 366   known [La Mail_1].txt  695  696                       by P ' s N . _EOS_ #> 367   known [La Mail_1].txt  716  717                      get a J N N . _EOS_ #> 368   known [La Mail_2].txt  173  174                    , when N is J . _EOS_ #> 369   known [La Mail_2].txt  222  223                      J P , are V . _EOS_ #> 370   known [La Mail_2].txt  291  292                  N might V the N . _EOS_ #> 371   known [La Mail_2].txt  333  334                     N J of her N . _EOS_ #> 372   known [La Mail_2].txt  380  381                       N N of N N . _EOS_ #> 373   known [La Mail_2].txt  578  579                       in P , ' N . _EOS_ #> 374   known [La Mail_2].txt  636  637                      P P P and P . _EOS_ #> 375   known [La Mail_2].txt  667  668           are J n further behind . _EOS_ #> 376   known [La Mail_2].txt  770  771                     on the J N N . _EOS_ #> 377   known [La Mail_2].txt  860  861                     ' N N next N . _EOS_ #> 378   known [La Mail_4].txt   20   21                   the N of the N . _EOS_ #> 379   known [La Mail_4].txt  117  118                   be V , he said . _EOS_ #> 380   known [La Mail_4].txt  198  199                    , the J N say . _EOS_ #> 381   known [La Mail_4].txt  230  231                       , J N of N . _EOS_ #> 382   known [La Mail_4].txt  260  261                        P P ' s N . _EOS_ #> 383   known [La Mail_4].txt  293  294                      V up as a N . _EOS_ #> 384   known [La Mail_4].txt  334  335            those who V will save . _EOS_ #> 385   known [La Mail_4].txt  406  407                     as J and J N . _EOS_ #> 386   known [La Mail_4].txt  496  497                  a N with four N . _EOS_ #> 387   known [La Mail_4].txt  516  517                  for the N and N . _EOS_ #> 388   known [La Mail_4].txt  648  649               will also help V N . _EOS_ #> 389   known [La Mail_4].txt  659  660                     J N with J N . _EOS_ #> 390   known [La Mail_4].txt  679  680                  N N down or off . _EOS_ #> 391   known [La Mail_4].txt  732  733                      , N and V N . _EOS_ #> 392   known [La Mail_5].txt   54   55                    N N and the N . _EOS_ #> 393   known [La Mail_5].txt  154  155                     V the N on P . _EOS_ #> 394   known [La Mail_5].txt  195  196                 V little or no N . _EOS_ #> 395   known [La Mail_5].txt  241  242                   all , P P said . _EOS_ #> 396   known [La Mail_5].txt  256  257                   D N , she said . _EOS_ #> 397   known [La Mail_5].txt  363  364                  P and other P P . _EOS_ #> 398   known [La Mail_5].txt  510  511                 while J N were V . _EOS_ #> 399   known [La Mail_5].txt  590  591                   many N as it V . _EOS_ #> 400   known [La Mail_5].txt  625  626                  hour N to the N . _EOS_ #> 401   known [La Mail_5].txt  712  713                N outside the N N . _EOS_ #> 402   known [La Mail_5].txt  765  766                 d look into V it . _EOS_ #> 403   known [La Mail_5].txt  800  801                 N will V about D . _EOS_ #> 404   known [La Mail_5].txt  885  886            did not V N afterward . _EOS_ #> 405   known [La Mail_5].txt  911  912                     N ' s move J . _EOS_ #> 406 unknown [Mf Mail_1].txt   64   65                    , J N to make . _EOS_ #> 407 unknown [Mf Mail_1].txt   90   91                   we did it in P . _EOS_ #> 408 unknown [Mf Mail_1].txt  622  623                 well on your N N . _EOS_ #> 409 unknown [Mf Mail_1].txt  710  711            in me from the inside . _EOS_ #> 410 unknown [Mf Mail_1].txt 1065 1066                     the P P N vs . _EOS_ #> 411   known [Mf Mail_2].txt   55   56                some N V this out . _EOS_ #> 412   known [Mf Mail_2].txt   69   70                the N are as well . _EOS_ #> 413   known [Mf Mail_2].txt  186  187            whether this V or not . _EOS_ #> 414   known [Mf Mail_2].txt  587  588                     P at D for N . _EOS_ #> 415   known [Mf Mail_2].txt  691  692                    if the N is J . _EOS_ #> 416   known [Mf Mail_2].txt  716  717                   P to V on this . _EOS_ #> 417   known [Mf Mail_2].txt  873  874                    N N per our N . _EOS_ #> 418   known [Mf Mail_3].txt   30   31              because the N was J . _EOS_ #> 419   known [Mf Mail_3].txt  268  269                   in the P and P . _EOS_ #> 420   known [Mf Mail_3].txt  396  397                 N within a few N . _EOS_ #> 421   known [Mf Mail_3].txt  470  471                       N of P P N . _EOS_ #> 422   known [Mf Mail_3].txt  534  535                   P P is under D . _EOS_ #> 423   known [Mf Mail_3].txt  617  618                       N N , as J . _EOS_ #> 424   known [Mf Mail_3].txt  660  661           we have something in J . _EOS_ #> 425   known [Mf Mail_3].txt  685  686                   , i would V it . _EOS_ #> 426   known [Mf Mail_3].txt  741  742                   very J to my N . _EOS_ #> 427   known [Mf Mail_3].txt  808  809              J N later this week . _EOS_ #> 428   known [Mf Mail_3].txt  833  834                   it is over a N . _EOS_ #> 429   known [Mf Mail_4].txt   62   63                      N for a J N . _EOS_ #> 430   known [Mf Mail_4].txt   83   84                  N for their J N . _EOS_ #> 431   known [Mf Mail_4].txt  222  223                 me , please V me . _EOS_ #> 432   known [Mf Mail_4].txt  266  267                   and B V some N . _EOS_ #> 433   known [Mf Mail_4].txt  310  311                      or B B to P . _EOS_ #> 434   known [Mf Mail_4].txt  606  607                      P and P P N . _EOS_ #> 435   known [Mf Mail_4].txt  799  800                   N for J than D . _EOS_ #> 436   known [Mf Mail_4].txt  941  942                      V us to V N . _EOS_ #> 437   known [Mf Mail_5].txt  431  432                      the N V P N . _EOS_ #> 438   known [Mf Mail_5].txt  512  513                   be J to this N . _EOS_ #> 439   known [Mf Mail_5].txt  669  670                more J for this N . _EOS_ #> 440   known [Mf Mail_5].txt  803  804                    the N is J of . _EOS_ #> 441   known [Mf Mail_5].txt  816  817                      the P P P N . _EOS_ #> 442   known [Mf Mail_5].txt  848  849                  can B V their N . _EOS_ #> 443 unknown [Ml Mail_3].txt   29   30                    soon as i V N . _EOS_ #> 444 unknown [Ml Mail_3].txt   48   49                     this N , J N . _EOS_ #> 445 unknown [Ml Mail_3].txt   64   65               not V it from work . _EOS_ #> 446 unknown [Ml Mail_3].txt  105  106                    soon as i V N . _EOS_ #> 447 unknown [Ml Mail_3].txt  124  125                     this N , J N . _EOS_ #> 448 unknown [Ml Mail_3].txt  140  141               not V it from work . _EOS_ #> 449 unknown [Ml Mail_3].txt  617  618                       P P by N N . _EOS_ #> 450 unknown [Ml Mail_3].txt  703  704                   are not B J in . _EOS_ #> 451 unknown [Ml Mail_3].txt  939  940                    you the N N P . _EOS_ #> 452   known [Ml Mail_1].txt  174  175                     P V like P P . _EOS_ #> 453   known [Ml Mail_1].txt  363  364                    J N we set up . _EOS_ #> 454   known [Ml Mail_1].txt  490  491                   and N in the N . _EOS_ #> 455   known [Ml Mail_1].txt  583  584                      in N in P P . _EOS_ #> 456   known [Ml Mail_1].txt  658  659                    be B V from N . _EOS_ #> 457   known [Ml Mail_1].txt  674  675                    N N for the P . _EOS_ #> 458   known [Ml Mail_1].txt  801  802              P P currently has N . _EOS_ #> 459   known [Ml Mail_1].txt  866  867                      V for a J N . _EOS_ #> 460   known [Ml Mail_2].txt   11   12                 and V with any N . _EOS_ #> 461   known [Ml Mail_2].txt   46   47                    N for the P P . _EOS_ #> 462   known [Ml Mail_2].txt  117  118                 V from V these N . _EOS_ #> 463   known [Ml Mail_2].txt  503  504                      N , the P P . _EOS_ #> 464   known [Ml Mail_2].txt  557  558                    soon as i V N . _EOS_ #> 465   known [Ml Mail_2].txt  576  577                     this N , J N . _EOS_ #> 466   known [Ml Mail_2].txt  592  593               not V it from work . _EOS_ #> 467   known [Ml Mail_2].txt  633  634                    soon as i V N . _EOS_ #> 468   known [Ml Mail_2].txt  652  653                     this N , J N . _EOS_ #> 469   known [Ml Mail_2].txt  680  681                that i can V them . _EOS_ #> 470   known [Ml Mail_2].txt  721  722                    soon as i V N . _EOS_ #> 471   known [Ml Mail_2].txt  740  741                     this N , J N . _EOS_ #> 472   known [Ml Mail_2].txt  756  757               not V it from work . _EOS_ #> 473   known [Ml Mail_4].txt  107  108                 from the N for J . _EOS_ #> 474   known [Ml Mail_4].txt  181  182          about it has not worked . _EOS_ #> 475   known [Ml Mail_4].txt  298  299                    or D on D off . _EOS_ #> 476   known [Ml Mail_4].txt  599  600                  N N for their N . _EOS_ #> 477   known [Ml Mail_4].txt  613  614                  if you need J N . _EOS_ #> 478   known [Ml Mail_4].txt  656  657                  J you V those N . _EOS_ #> 479   known [Ml Mail_5].txt   31   32                       P B to P P . _EOS_ #> 480   known [Ml Mail_5].txt  293  294                      in N N at P . _EOS_ #> 481   known [Ml Mail_5].txt  326  327                  in the N with D . _EOS_ #> 482   known [Ml Mail_5].txt  402  403               taking up your N N . _EOS_ #> 483   known [Ml Mail_5].txt  489  490                    soon as i V N . _EOS_ #> 484   known [Ml Mail_5].txt  703  704                   N you V from P . _EOS_ #> 485   known [Ml Mail_5].txt  817  818                     P - take a N . _EOS_ #>                          post authorship #> 1           _BOS_ P V us they          Q #> 2           _BOS_ N is in the          Q #> 3              _BOS_ P , V is          Q #> 4         _BOS_ J N is always          Q #> 5               _BOS_ P , P ,          Q #> 6            _BOS_ N is not a          Q #> 7             _BOS_ J N , but          Q #> 8        _BOS_ P is what your          Q #> 9             _BOS_ J N are B          Q #> 10                                     Q #> 11    _BOS_ P had already run          K #> 12         _BOS_ V the N from          K #> 13         _BOS_ P i am still          K #> 14              _BOS_ P P P N          K #> 15          _BOS_ P per our N          K #> 16            _BOS_ P V to me          K #> 17           _BOS_ N of the N          K #> 18             _BOS_ P P , in          K #> 19              _BOS_ P P V a          K #> 20                                     K #> 21           _BOS_ N , over J  Reference #> 22             _BOS_ B to D i  Reference #> 23        _BOS_ N and N under  Reference #> 24         _BOS_ V , with her  Reference #> 25        _BOS_ V N should be  Reference #> 26    _BOS_ V from talking to  Reference #> 27                             Reference #> 28         _BOS_ V your N for  Reference #> 29             _BOS_ P , V on  Reference #> 30              _BOS_ N P P P  Reference #> 31              _BOS_ N , P i  Reference #> 32              _BOS_ P , s P  Reference #> 33             _BOS_ P P P is  Reference #> 34       _BOS_ V below is the  Reference #> 35     _BOS_ N should last no  Reference #> 36           _BOS_ J N will V  Reference #> 37       _BOS_ V you for your  Reference #> 38         _BOS_ N for your V  Reference #> 39           _BOS_ N from a P  Reference #> 40             _BOS_ V by a D  Reference #> 41                             Reference #> 42         _BOS_ P , please V  Reference #> 43              _BOS_ N , P P  Reference #> 44              _BOS_ N P P P  Reference #> 45           _BOS_ P , i need  Reference #> 46            _BOS_ P is an N  Reference #> 47                             Reference #> 48          _BOS_ N will be V  Reference #> 49           _BOS_ J , i just  Reference #> 50            _BOS_ P P and P  Reference #> 51         _BOS_ N has also V  Reference #> 52          _BOS_ P , here is  Reference #> 53      _BOS_ P is possible ,  Reference #> 54          _BOS_ V you for V  Reference #> 55            _BOS_ P , may i  Reference #> 56          _BOS_ P , here is  Reference #> 57                             Reference #> 58            _BOS_ P has V N  Reference #> 59              _BOS_ P , i V  Reference #> 60         _BOS_ P , i wanted  Reference #> 61           _BOS_ P , i just  Reference #> 62             _BOS_ P P is V  Reference #> 63         _BOS_ J , i wanted  Reference #> 64     _BOS_ P said that they  Reference #> 65          _BOS_ P , here is  Reference #> 66         _BOS_ P has them V  Reference #> 67        _BOS_ N says it was  Reference #> 68           _BOS_ J to say ,  Reference #> 69                             Reference #> 70          _BOS_ P , here is  Reference #> 71       _BOS_ V you for your  Reference #> 72         _BOS_ P and P will  Reference #> 73             _BOS_ P , as a  Reference #> 74      _BOS_ J about all the  Reference #> 75          _BOS_ P will be V  Reference #> 76            _BOS_ P and P ,  Reference #> 77   _BOS_ P please talk with  Reference #> 78            _BOS_ P , you '  Reference #> 79           _BOS_ P , your N  Reference #> 80                             Reference #> 81            _BOS_ V in N on  Reference #> 82         _BOS_ J and P have  Reference #> 83          _BOS_ P , here is  Reference #> 84          _BOS_ P just V to  Reference #> 85         _BOS_ D with the P  Reference #> 86        _BOS_ P will also V  Reference #> 87         _BOS_ P , have you  Reference #> 88       _BOS_ N wants this N  Reference #> 89            _BOS_ P , P and  Reference #> 90            _BOS_ P P , per  Reference #> 91             _BOS_ V , my N  Reference #> 92            _BOS_ P P has V  Reference #> 93         _BOS_ P has been J  Reference #> 94      _BOS_ V , please find  Reference #> 95              _BOS_ P P P P  Reference #> 96      _BOS_ P , please take  Reference #> 97                             Reference #> 98           _BOS_ P is the N  Reference #> 99              _BOS_ P , P P  Reference #> 100          _BOS_ P , i have  Reference #> 101           _BOS_ P , N for  Reference #> 102         _BOS_ P , i would  Reference #> 103                            Reference #> 104            _BOS_ N N D of  Reference #> 105          _BOS_ J N were V  Reference #> 106          _BOS_ N D of the  Reference #> 107       _BOS_ J N will also  Reference #> 108           _BOS_ V the P P  Reference #> 109           _BOS_ V N was V  Reference #> 110          _BOS_ D N N only  Reference #> 111                            Reference #> 112          _BOS_ V , that V  Reference #> 113        _BOS_ N was V from  Reference #> 114          _BOS_ N N N have  Reference #> 115           _BOS_ P P , all  Reference #> 116       _BOS_ V to keep the  Reference #> 117           _BOS_ V , the N  Reference #> 118            _BOS_ V in a J  Reference #> 119          _BOS_ V to the N  Reference #> 120          _BOS_ V to the N  Reference #> 121          _BOS_ V to the N  Reference #> 122            _BOS_ P , i ve  Reference #> 123                            Reference #> 124      _BOS_ P said he will  Reference #> 125             _BOS_ P , i V  Reference #> 126          _BOS_ D N of the  Reference #> 127          _BOS_ N N were V  Reference #> 128           _BOS_ P is an N  Reference #> 129 _BOS_ J about not getting  Reference #> 130           _BOS_ N are B J  Reference #> 131            _BOS_ P is V P  Reference #> 132                            Reference #> 133           _BOS_ D and D N  Reference #> 134            _BOS_ J N as P  Reference #> 135          _BOS_ P had N of  Reference #> 136     _BOS_ P said he would  Reference #> 137           _BOS_ P P the N  Reference #> 138             _BOS_ P , i V  Reference #> 139                            Reference #> 140          _BOS_ P , we had  Reference #> 141          _BOS_ B to the N  Reference #> 142        _BOS_ P was very J  Reference #> 143       _BOS_ J N has shown  Reference #> 144        _BOS_ P , per your  Reference #> 145             _BOS_ P P P P  Reference #> 146          _BOS_ P also , V  Reference #> 147             _BOS_ P P , P  Reference #> 148           _BOS_ P P and i  Reference #> 149                            Reference #> 150             _BOS_ N , P P  Reference #> 151          _BOS_ V is the N  Reference #> 152            _BOS_ B i ' ll  Reference #> 153          _BOS_ N , P this  Reference #> 154         _BOS_ P P P asked  Reference #> 155       _BOS_ P , could you  Reference #> 156        _BOS_ V that the N  Reference #> 157   _BOS_ D since they have  Reference #> 158            _BOS_ N of P '  Reference #> 159         _BOS_ P , can you  Reference #> 160      _BOS_ P said she had  Reference #> 161             _BOS_ N P , i  Reference #> 162             _BOS_ N , P i  Reference #> 163             _BOS_ N , P i  Reference #> 164            _BOS_ P , am i  Reference #> 165    _BOS_ P just wanted to  Reference #> 166       _BOS_ P here is the  Reference #> 167           _BOS_ D on P in  Reference #> 168             _BOS_ N , P i  Reference #> 169                            Reference #> 170       _BOS_ P P are going  Reference #> 171     _BOS_ P , please find  Reference #> 172          _BOS_ P , we are  Reference #> 173            _BOS_ V is a N  Reference #> 174            _BOS_ V is a V  Reference #> 175         _BOS_ P is very J  Reference #> 176                            Reference #> 177          _BOS_ D is J for  Reference #> 178         _BOS_ N , we have  Reference #> 179        _BOS_ P and i will  Reference #> 180      _BOS_ P asked me for  Reference #> 181        _BOS_ P , for your  Reference #> 182             _BOS_ J N N P  Reference #> 183          _BOS_ P V on the  Reference #> 184                            Reference #> 185         _BOS_ N will V on  Reference #> 186      _BOS_ N will V their  Reference #> 187        _BOS_ N will now V  Reference #> 188          _BOS_ P P will V  Reference #> 189             _BOS_ P P , P  Reference #> 190        _BOS_ P , please V  Reference #> 191             _BOS_ P N P ,  Reference #> 192           _BOS_ N and P '  Reference #> 193     _BOS_ V you very much  Reference #> 194             _BOS_ P P N i  Reference #> 195           _BOS_ P is in P  Reference #> 196        _BOS_ N N i called  Reference #> 197            _BOS_ P is V P  Reference #> 198       _BOS_ P V from over  Reference #> 199             _BOS_ D N V V  Reference #> 200           _BOS_ V the N V  Reference #> 201             _BOS_ P P , P  Reference #> 202           _BOS_ P hey P ,  Reference #> 203           _BOS_ V J N get  Reference #> 204     _BOS_ N , please call  Reference #> 205             _BOS_ P S P ,  Reference #> 206             _BOS_ P ' s N  Reference #> 207          _BOS_ V to be in  Reference #> 208           _BOS_ N S N and  Reference #> 209                            Reference #> 210   _BOS_ N for giving some  Reference #> 211         _BOS_ V that P is  Reference #> 212          _BOS_ P P we are  Reference #> 213         _BOS_ B , P would  Reference #> 214             _BOS_ V P P P  Reference #> 215             _BOS_ P P , i  Reference #> 216       _BOS_ V below are N  Reference #> 217       _BOS_ P has V below  Reference #> 218             _BOS_ P P ' s  Reference #> 219                            Reference #> 220        _BOS_ P will be in  Reference #> 221       _BOS_ P there are a  Reference #> 222           _BOS_ P has J P  Reference #> 223     _BOS_ P would like to  Reference #> 224    _BOS_ V below are some  Reference #> 225       _BOS_ J N should be  Reference #> 226      _BOS_ P will have to  Reference #> 227             _BOS_ P ' s J  Reference #> 228           _BOS_ N of N of  Reference #> 229            _BOS_ N of P '  Reference #> 230     _BOS_ N very much for  Reference #> 231        _BOS_ P does not V  Reference #> 232                            Reference #> 233          _BOS_ P P will N  Reference #> 234           _BOS_ N P P and  Reference #> 235             _BOS_ P S P P  Reference #> 236             _BOS_ P P P P  Reference #> 237           _BOS_ P P and P  Reference #> 238             _BOS_ P S P P  Reference #> 239             _BOS_ P P P P  Reference #> 240           _BOS_ P P and P  Reference #> 241           _BOS_ P P and P  Reference #> 242            _BOS_ P P at P  Reference #> 243       _BOS_ P P will have  Reference #> 244                            Reference #> 245             _BOS_ P B P P  Reference #> 246             _BOS_ P P - P  Reference #> 247             _BOS_ J N - N  Reference #> 248           _BOS_ P P was V  Reference #> 249        _BOS_ P has some N  Reference #> 250          _BOS_ N due P we  Reference #> 251       _BOS_ P P is taking  Reference #> 252             _BOS_ N - i V  Reference #> 253             _BOS_ P P P P  Reference #> 254           _BOS_ P P has V  Reference #> 255        _BOS_ P - please V  Reference #> 256          _BOS_ P P P will  Reference #> 257                            Reference #> 258       _BOS_ P and i would  Reference #> 259           _BOS_ P so if i  Reference #> 260       _BOS_ D plus i have  Reference #> 261     _BOS_ P - please work  Reference #> 262        _BOS_ P to say our  Reference #> 263       _BOS_ P P they know  Reference #> 264         _BOS_ P V you for  Reference #> 265          _BOS_ N may be V  Reference #> 266           _BOS_ N N N are  Reference #> 267           _BOS_ N N V one  Reference #> 268                            Reference #> 269           _BOS_ N N to be  Reference #> 270           _BOS_ N the J N  Reference #> 271         _BOS_ P P will be  Reference #> 272     _BOS_ N you each have  Reference #> 273           _BOS_ V the N N  Reference #> 274     _BOS_ P is working up  Reference #> 275           _BOS_ P V to be  Reference #> 276        _BOS_ J N for some  Reference #> 277                            Reference #> 278        _BOS_ N N you have  Reference #> 279        _BOS_ P P needs to  Reference #> 280      _BOS_ P needs to get  Reference #> 281        _BOS_ N N may also  Reference #> 282         _BOS_ P , which V  Reference #> 283       _BOS_ P and other N  Reference #> 284             _BOS_ P , N N  Reference #> 285                            Reference #> 286             _BOS_ P , i V  Reference #> 287             _BOS_ P , i V  Reference #> 288         _BOS_ P , i would  Reference #> 289           _BOS_ P and P ,  Reference #> 290           _BOS_ P and P ,  Reference #> 291             _BOS_ P P P P  Reference #> 292          _BOS_ P will V V  Reference #> 293            _BOS_ V P B to  Reference #> 294        _BOS_ P P keep the  Reference #> 295                            Reference #> 296             _BOS_ P , P P  Reference #> 297           _BOS_ N P N and  Reference #> 298           _BOS_ P , P and  Reference #> 299   _BOS_ N for not working  Reference #> 300             _BOS_ P , P ,  Reference #> 301           _BOS_ P , i had  Reference #> 302                            Reference #> 303           _BOS_ P P and i  Reference #> 304           _BOS_ P , i had  Reference #> 305          _BOS_ P i had to  Reference #> 306           _BOS_ P , i had  Reference #> 307           _BOS_ P , i had  Reference #> 308           _BOS_ P , i had  Reference #> 309           _BOS_ P , i had  Reference #> 310           _BOS_ P , i had  Reference #> 311           _BOS_ P , i had  Reference #> 312            _BOS_ P , V is  Reference #> 313       _BOS_ P gave me his  Reference #> 314             _BOS_ N - D -  Reference #> 315            _BOS_ V N of P  Reference #> 316       _BOS_ P P needs the  Reference #> 317     _BOS_ V for N working  Reference #> 318            _BOS_ P , V is  Reference #> 319           _BOS_ P and P V  Reference #> 320           _BOS_ P and P V  Reference #> 321                            Reference #> 322           _BOS_ P and P ,  Reference #> 323            _BOS_ P , i am  Reference #> 324     _BOS_ P , just wanted  Reference #> 325            _BOS_ N of P N  Reference #> 326             _BOS_ P J N N  Reference #> 327     _BOS_ P P are putting  Reference #> 328     _BOS_ P and i working  Reference #> 329             _BOS_ N P P -  Reference #> 330          _BOS_ N with P P  Reference #> 331           _BOS_ V P and P  Reference #> 332        _BOS_ P and P will  Reference #> 333             _BOS_ P ' s N  Reference #> 334        _BOS_ P , here are  Reference #> 335        _BOS_ P , here are  Reference #> 336           _BOS_ P , V you  Reference #> 337         _BOS_ P , i would  Reference #> 338          _BOS_ P , it was  Reference #> 339           _BOS_ P , the P  Reference #> 340   _BOS_ P , following are  Reference #> 341            _BOS_ P , i am  Reference #> 342           _BOS_ P , P and  Reference #> 343         _BOS_ P may set a  Reference #> 344       _BOS_ P can still V  Reference #> 345       _BOS_ P , would you  Reference #> 346       _BOS_ P , would you  Reference #> 347         _BOS_ P and P are  Reference #> 348         _BOS_ P and P can  Reference #> 349             _BOS_ P ' s N  Reference #> 350                            Reference #> 351           _BOS_ P P , who  Reference #> 352          _BOS_ P ' most J  Reference #> 353            _BOS_ N V to a  Reference #> 354           _BOS_ P P , who  Reference #> 355            _BOS_ N of N P  Reference #> 356            _BOS_ P P , an  Reference #> 357             _BOS_ N P P P  Reference #> 358      _BOS_ N later V them  Reference #> 359            _BOS_ N in P '  Reference #> 360           _BOS_ V P and P  Reference #> 361           _BOS_ N V the N  Reference #> 362                            Reference #> 363             _BOS_ N - P ,  Reference #> 364             _BOS_ P P P P  Reference #> 365             _BOS_ P N N P  Reference #> 366       _BOS_ N that do not  Reference #> 367                            Reference #> 368         _BOS_ P and his N  Reference #> 369             _BOS_ P P P P  Reference #> 370             _BOS_ P , a J  Reference #> 371      _BOS_ P could have V  Reference #> 372      _BOS_ P has said the  Reference #> 373         _BOS_ D to say he  Reference #> 374          _BOS_ \" N over a  Reference #> 375          _BOS_ P has V to  Reference #> 376         _BOS_ P V aside N  Reference #> 377                            Reference #> 378           _BOS_ P , who V  Reference #> 379       _BOS_ N V about the  Reference #> 380       _BOS_ P will V from  Reference #> 381         _BOS_ P was V two  Reference #> 382        _BOS_ V and save a  Reference #> 383          _BOS_ N of the N  Reference #> 384      _BOS_ N who use less  Reference #> 385          _BOS_ P is the N  Reference #> 386            _BOS_ N V V on  Reference #> 387         _BOS_ N who use J  Reference #> 388          _BOS_ V your N J  Reference #> 389        _BOS_ V those N at  Reference #> 390          _BOS_ V the N of  Reference #> 391                            Reference #> 392             _BOS_ P P ' s  Reference #> 393       _BOS_ P P said that  Reference #> 394      _BOS_ N will see the  Reference #> 395          _BOS_ J N will V  Reference #> 396        _BOS_ V with the J  Reference #> 397       _BOS_ J N N already  Reference #> 398             _BOS_ J J N N  Reference #> 399           _BOS_ N N , who  Reference #> 400       _BOS_ N for each of  Reference #> 401          _BOS_ P P also V  Reference #> 402             _BOS_ P P , a  Reference #> 403           _BOS_ D D for P  Reference #> 404          _BOS_ N N were J  Reference #> 405                            Reference #> 406           _BOS_ B , the P  Reference #> 407        _BOS_ B , that was  Reference #> 408           _BOS_ B i can V  Reference #> 409       _BOS_ J , however ,  Reference #> 410                            Reference #> 411         _BOS_ N is out of  Reference #> 412             _BOS_ N N S P  Reference #> 413          _BOS_ D N and we  Reference #> 414            _BOS_ P N is V  Reference #> 415            _BOS_ V at P -  Reference #> 416          _BOS_ V , i will  Reference #> 417                            Reference #> 418      _BOS_ P called N and  Reference #> 419            _BOS_ P P is B  Reference #> 420        _BOS_ P is said to  Reference #> 421          _BOS_ V to V the  Reference #> 422           _BOS_ V the N N  Reference #> 423          _BOS_ B , i have  Reference #> 424         _BOS_ N for the N  Reference #> 425       _BOS_ N again and i  Reference #> 426        _BOS_ N for your N  Reference #> 427    _BOS_ N please see the  Reference #> 428                            Reference #> 429           _BOS_ P , the P  Reference #> 430      _BOS_ P gave me this  Reference #> 431        _BOS_ N for N with  Reference #> 432         _BOS_ N again , P  Reference #> 433          _BOS_ N for V my  Reference #> 434            _BOS_ P V to V  Reference #> 435    _BOS_ P also said that  Reference #> 436                            Reference #> 437       _BOS_ V that we are  Reference #> 438           _BOS_ N D N the  Reference #> 439            _BOS_ P P P to  Reference #> 440             _BOS_ D N N V  Reference #> 441            _BOS_ P P is V  Reference #> 442                            Reference #> 443     _BOS_ N again and use  Reference #> 444           _BOS_ N is my N  Reference #> 445    _BOS_ N again and look  Reference #> 446     _BOS_ N again and use  Reference #> 447           _BOS_ N is my N  Reference #> 448    _BOS_ N again and look  Reference #> 449        _BOS_ N we B begin  Reference #> 450          _BOS_ V some N N  Reference #> 451                            Reference #> 452      _BOS_ N for you help  Reference #> 453     _BOS_ N for your help  Reference #> 454           _BOS_ V you P P  Reference #> 455         _BOS_ P P will be  Reference #> 456       _BOS_ N should be V  Reference #> 457          _BOS_ P P and it  Reference #> 458     _BOS_ N for your help  Reference #> 459                            Reference #> 460        _BOS_ P is now the  Reference #> 461      _BOS_ N and please V  Reference #> 462        _BOS_ P P is going  Reference #> 463         _BOS_ N is for my  Reference #> 464     _BOS_ N again and use  Reference #> 465           _BOS_ N is my N  Reference #> 466    _BOS_ N again and look  Reference #> 467     _BOS_ N again and use  Reference #> 468           _BOS_ N is my N  Reference #> 469    _BOS_ N again and look  Reference #> 470     _BOS_ N again and use  Reference #> 471           _BOS_ N is my N  Reference #> 472                            Reference #> 473             _BOS_ B a N N  Reference #> 474     _BOS_ N for your help  Reference #> 475            _BOS_ J V B to  Reference #> 476     _BOS_ N for your help  Reference #> 477         _BOS_ D with N in  Reference #> 478                            Reference #> 479   _BOS_ P should not need  Reference #> 480        _BOS_ P had V that  Reference #> 481         _BOS_ B to N with  Reference #> 482           _BOS_ N and i V  Reference #> 483          _BOS_ N N will V  Reference #> 484             _BOS_ N , N ,  Reference #> 485                            Reference"},{"path":"https://andreanini.github.io/idiolect/dev/reference/contentmask.html","id":null,"dir":"Reference","previous_headings":"","what":"Content masking — contentmask","title":"Content masking — contentmask","text":"function offers three algorithms topic/content masking. order run masking algorithms, spacy tokenizer POS-tagger run first (via spacyr). information masking algorithms see Details .","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/contentmask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Content masking — contentmask","text":"","code":"contentmask(   corpus,   model = \"en_core_web_sm\",   algorithm = \"POSnoise\",   fw_list = \"eng_halvani\",   replace_non_ascii = TRUE )"},{"path":"https://andreanini.github.io/idiolect/dev/reference/contentmask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Content masking — contentmask","text":"corpus quanteda corpus object, typically output create_corpus() function. model spacy model use. default \"en_core_web_sm\". algorithm string, either \"POSnoise\" (default), \"frames\", \"textdistortion\". fw_list list function words use textdistortion algorithm. either default (\"eng_halvani\") list function words used POSnoise can vector strings string function word keep. replace_non_ascii logical value indicating whether remove non-ASCII characters (including emojis). default.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/contentmask.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Content masking — contentmask","text":"quanteda corpus object containing functional tokens, depending algorithm chosen. corpus contains docvars input. Email addresses URLs treated like nouns.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/contentmask.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Content masking — contentmask","text":"default algorithm content masking function applies POSnoise (Halvani Graner 2021). algorithm works English transforms text masking tokens using POS tag tokens : nouns, verbs, adjectives, adverbs, digits, symbols leaving rest unchanged. POSnoise uses list function words English also includes frequent words belonging masked Part Speech tags tend mostly functional (e.g. make, recently, well). Another algorithm implemented Nini's (2023) frames frame n-grams. algorithm involve special list tokens therefore can potentially work language provided correct spacy model loaded. algorithm consists masking tokens using POS tag nouns, verbs, personal pronouns. Finally, last algorithm implemented version textdistortion, originally proposed Stamatatos (2017). version algorithm essentially POSnoise without POS tag information. default implementation uses list function words used POSnoise. addition function words provided, function treats punctuation marks new line breaks function words keep. basic tokenization done using spacyr right model language analysed selected. never used spacyr please follow instructions set install model using function. removal non-ASCII characters done using textclean package.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/contentmask.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Content masking — contentmask","text":"Halvani, Oren & Lukas Graner. 2021. POSNoise: Effective Countermeasure Topic Biases Authorship Analysis. Proceedings 16th International Conference Availability, Reliability Security, 1–12. Vienna, Austria: Association Computing Machinery. https://doi.org/10.1145/3465481.3470050. Nini, Andrea. 2023. Theory Linguistic Individuality Authorship Analysis (Elements Forensic Linguistics). Cambridge, UK: Cambridge University Press. Stamatatos, Efstathios. 2017. Masking topic-related information enhance authorship attribution. Journal Association Information Science Technology. https://doi.org/10.1002/asi.23968.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/contentmask.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Content masking — contentmask","text":"","code":"if (FALSE) { # \\dontrun{ text <- \"The cat was on the chair. He didn't move\\ncat@pets.com;\\nhttp://quanteda.io/. i.e. a test \" toy.corpus <- quanteda::corpus(text) contentmask(toy.corpus, algorithm = \"POSnoise\") contentmask(toy.corpus, algorithm = \"textdistortion\") } # }"},{"path":"https://andreanini.github.io/idiolect/dev/reference/create_corpus.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a corpus — create_corpus","title":"Create a corpus — create_corpus","text":"Function read text data turn quanteda corpus object.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/create_corpus.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a corpus — create_corpus","text":"","code":"create_corpus(path)"},{"path":"https://andreanini.github.io/idiolect/dev/reference/create_corpus.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a corpus — create_corpus","text":"path string containing path folder plain text files (ending .txt) name structured following: authorname_textname.txt (e.g. smith_text1.txt).","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/create_corpus.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a corpus — create_corpus","text":"quanteda corpus object authors' names docvar.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/create_corpus.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a corpus — create_corpus","text":"","code":"if (FALSE) { # \\dontrun{ path <- \"path/to/data\" create_corpus(path) } # }"},{"path":"https://andreanini.github.io/idiolect/dev/reference/delta.html","id":null,"dir":"Reference","previous_headings":"","what":"Delta — delta","title":"Delta — delta","text":"function runs Cosine Delta analysis (Smith Aldridge 2011; Evert et al. 2017).","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/delta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delta — delta","text":"","code":"delta(   q.data,   k.data,   tokens = \"word\",   remove_punct = FALSE,   remove_symbols = TRUE,   remove_numbers = TRUE,   lowercase = TRUE,   n = 1,   trim = TRUE,   threshold = 150,   features = FALSE,   cores = NULL )"},{"path":"https://andreanini.github.io/idiolect/dev/reference/delta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delta — delta","text":"q.data questioned disputed data, either corpus (output create_corpus()) quanteda dfm (output vectorize()). k.data known undisputed data, either corpus (output create_corpus()) quanteda dfm (output vectorize()). tokens type tokens extract, either \"word\" (default) \"character\". remove_punct logical value. FALSE (default) keeps punctuation marks. remove_symbols logical value. TRUE (default) removes symbols. remove_numbers logical value. TRUE (default) removes numbers lowercase logical value. TRUE (default) transforms tokens lower case. n order size n-grams extracted. Default 1. trim logical value. TRUE (default) frequent tokens kept. threshold numeric value indicating many frequent tokens keep trim = TRUE. default 150. features Logical default FALSE. TRUE, output contain features used. cores number cores use parallel processing (default one).","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/delta.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Delta — delta","text":"features set FALSE output data frame containing results comparisons Q texts K texts. features set TRUE output list containing results data frame vector features used analysis.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/delta.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Delta — delta","text":"Evert, Stefan, Thomas Proisl, Fotis Jannidis, Isabella Reger, Steffen Pielström, Christof Schöch & Thorsten Vitt. 2017. Understanding explaining Delta measures authorship attribution. Digital Scholarship Humanities 32. ii4–ii16. https://doi.org/10.1093/llc/fqx023. Smith, Peter W H & W Aldridge. 2011. Improving Authorship Attribution: Optimizing Burrows’ Delta Method*. Journal Quantitative Linguistics 18(1). 63–88. https://doi.org/10.1080/09296174.2011.533591.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/delta.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Delta — delta","text":"","code":"Q <- enron.sample[c(5:6)] K <- enron.sample[-c(5:6)] delta(Q, K) #>                          Q                       K target  score #> 1  unknown [Kh Mail_2].txt   known [Kh Mail_1].txt   TRUE -0.039 #> 2  unknown [Kw Mail_3].txt   known [Kh Mail_1].txt  FALSE -0.061 #> 3  unknown [Kh Mail_2].txt   known [Kh Mail_3].txt   TRUE  0.177 #> 4  unknown [Kw Mail_3].txt   known [Kh Mail_3].txt  FALSE  0.075 #> 5  unknown [Kh Mail_2].txt   known [Kh Mail_4].txt   TRUE -0.068 #> 6  unknown [Kw Mail_3].txt   known [Kh Mail_4].txt  FALSE -0.078 #> 7  unknown [Kh Mail_2].txt   known [Kh Mail_5].txt   TRUE -0.018 #> 8  unknown [Kw Mail_3].txt   known [Kh Mail_5].txt  FALSE -0.161 #> 9  unknown [Kh Mail_2].txt   known [Kw Mail_1].txt  FALSE -0.144 #> 10 unknown [Kw Mail_3].txt   known [Kw Mail_1].txt   TRUE  0.110 #> 11 unknown [Kh Mail_2].txt   known [Kw Mail_2].txt  FALSE -0.091 #> 12 unknown [Kw Mail_3].txt   known [Kw Mail_2].txt   TRUE  0.195 #> 13 unknown [Kh Mail_2].txt   known [Kw Mail_4].txt  FALSE -0.072 #> 14 unknown [Kw Mail_3].txt   known [Kw Mail_4].txt   TRUE  0.315 #> 15 unknown [Kh Mail_2].txt   known [Kw Mail_5].txt  FALSE -0.113 #> 16 unknown [Kw Mail_3].txt   known [Kw Mail_5].txt   TRUE  0.243 #> 17 unknown [Kh Mail_2].txt unknown [Lc Mail_1].txt  FALSE  0.177 #> 18 unknown [Kw Mail_3].txt unknown [Lc Mail_1].txt  FALSE -0.073 #> 19 unknown [Kh Mail_2].txt   known [Lc Mail_2].txt  FALSE -0.084 #> 20 unknown [Kw Mail_3].txt   known [Lc Mail_2].txt  FALSE -0.192 #> 21 unknown [Kh Mail_2].txt   known [Lc Mail_3].txt  FALSE -0.059 #> 22 unknown [Kw Mail_3].txt   known [Lc Mail_3].txt  FALSE -0.009 #> 23 unknown [Kh Mail_2].txt   known [Lc Mail_4].txt  FALSE  0.073 #> 24 unknown [Kw Mail_3].txt   known [Lc Mail_4].txt  FALSE  0.028 #> 25 unknown [Kh Mail_2].txt   known [Lc Mail_5].txt  FALSE  0.087 #> 26 unknown [Kw Mail_3].txt   known [Lc Mail_5].txt  FALSE -0.172 #> 27 unknown [Kh Mail_2].txt unknown [Ld Mail_4].txt  FALSE  0.017 #> 28 unknown [Kw Mail_3].txt unknown [Ld Mail_4].txt  FALSE -0.021 #> 29 unknown [Kh Mail_2].txt   known [Ld Mail_1].txt  FALSE -0.155 #> 30 unknown [Kw Mail_3].txt   known [Ld Mail_1].txt  FALSE  0.082 #> 31 unknown [Kh Mail_2].txt   known [Ld Mail_2].txt  FALSE -0.046 #> 32 unknown [Kw Mail_3].txt   known [Ld Mail_2].txt  FALSE -0.086 #> 33 unknown [Kh Mail_2].txt   known [Ld Mail_3].txt  FALSE -0.168 #> 34 unknown [Kw Mail_3].txt   known [Ld Mail_3].txt  FALSE -0.084 #> 35 unknown [Kh Mail_2].txt   known [Ld Mail_5].txt  FALSE -0.018 #> 36 unknown [Kw Mail_3].txt   known [Ld Mail_5].txt  FALSE  0.032 #> 37 unknown [Kh Mail_2].txt unknown [Lt Mail_2].txt  FALSE  0.032 #> 38 unknown [Kw Mail_3].txt unknown [Lt Mail_2].txt  FALSE -0.143 #> 39 unknown [Kh Mail_2].txt   known [Lt Mail_1].txt  FALSE  0.113 #> 40 unknown [Kw Mail_3].txt   known [Lt Mail_1].txt  FALSE  0.116 #> 41 unknown [Kh Mail_2].txt   known [Lt Mail_3].txt  FALSE  0.139 #> 42 unknown [Kw Mail_3].txt   known [Lt Mail_3].txt  FALSE  0.080 #> 43 unknown [Kh Mail_2].txt   known [Lt Mail_4].txt  FALSE  0.145 #> 44 unknown [Kw Mail_3].txt   known [Lt Mail_4].txt  FALSE -0.032 #> 45 unknown [Kh Mail_2].txt unknown [Lk Mail_4].txt  FALSE  0.030 #> 46 unknown [Kw Mail_3].txt unknown [Lk Mail_4].txt  FALSE -0.154 #> 47 unknown [Kh Mail_2].txt   known [Lk Mail_1].txt  FALSE -0.038 #> 48 unknown [Kw Mail_3].txt   known [Lk Mail_1].txt  FALSE -0.012 #> 49 unknown [Kh Mail_2].txt   known [Lk Mail_2].txt  FALSE -0.065 #> 50 unknown [Kw Mail_3].txt   known [Lk Mail_2].txt  FALSE -0.104 #> 51 unknown [Kh Mail_2].txt   known [Lk Mail_3].txt  FALSE  0.152 #> 52 unknown [Kw Mail_3].txt   known [Lk Mail_3].txt  FALSE -0.002 #> 53 unknown [Kh Mail_2].txt   known [Lk Mail_5].txt  FALSE -0.008 #> 54 unknown [Kw Mail_3].txt   known [Lk Mail_5].txt  FALSE -0.060 #> 55 unknown [Kh Mail_2].txt unknown [Lb Mail_3].txt  FALSE -0.203 #> 56 unknown [Kw Mail_3].txt unknown [Lb Mail_3].txt  FALSE  0.072 #> 57 unknown [Kh Mail_2].txt   known [Lb Mail_1].txt  FALSE -0.111 #> 58 unknown [Kw Mail_3].txt   known [Lb Mail_1].txt  FALSE  0.114 #> 59 unknown [Kh Mail_2].txt   known [Lb Mail_2].txt  FALSE -0.094 #> 60 unknown [Kw Mail_3].txt   known [Lb Mail_2].txt  FALSE  0.052 #> 61 unknown [Kh Mail_2].txt   known [Lb Mail_4].txt  FALSE  0.042 #> 62 unknown [Kw Mail_3].txt   known [Lb Mail_4].txt  FALSE  0.132 #> 63 unknown [Kh Mail_2].txt   known [Lb Mail_5].txt  FALSE -0.106 #> 64 unknown [Kw Mail_3].txt   known [Lb Mail_5].txt  FALSE -0.070 #> 65 unknown [Kh Mail_2].txt unknown [La Mail_3].txt  FALSE -0.006 #> 66 unknown [Kw Mail_3].txt unknown [La Mail_3].txt  FALSE -0.278 #> 67 unknown [Kh Mail_2].txt   known [La Mail_1].txt  FALSE -0.004 #> 68 unknown [Kw Mail_3].txt   known [La Mail_1].txt  FALSE -0.193 #> 69 unknown [Kh Mail_2].txt   known [La Mail_2].txt  FALSE -0.044 #> 70 unknown [Kw Mail_3].txt   known [La Mail_2].txt  FALSE -0.134 #> 71 unknown [Kh Mail_2].txt   known [La Mail_4].txt  FALSE  0.137 #> 72 unknown [Kw Mail_3].txt   known [La Mail_4].txt  FALSE -0.270 #> 73 unknown [Kh Mail_2].txt   known [La Mail_5].txt  FALSE  0.081 #> 74 unknown [Kw Mail_3].txt   known [La Mail_5].txt  FALSE -0.220 #> 75 unknown [Kh Mail_2].txt unknown [Mf Mail_1].txt  FALSE -0.242 #> 76 unknown [Kw Mail_3].txt unknown [Mf Mail_1].txt  FALSE  0.024 #> 77 unknown [Kh Mail_2].txt   known [Mf Mail_2].txt  FALSE -0.096 #> 78 unknown [Kw Mail_3].txt   known [Mf Mail_2].txt  FALSE -0.023 #> 79 unknown [Kh Mail_2].txt   known [Mf Mail_3].txt  FALSE  0.123 #> 80 unknown [Kw Mail_3].txt   known [Mf Mail_3].txt  FALSE -0.017 #> 81 unknown [Kh Mail_2].txt   known [Mf Mail_4].txt  FALSE -0.148 #> 82 unknown [Kw Mail_3].txt   known [Mf Mail_4].txt  FALSE  0.057 #> 83 unknown [Kh Mail_2].txt   known [Mf Mail_5].txt  FALSE  0.058 #> 84 unknown [Kw Mail_3].txt   known [Mf Mail_5].txt  FALSE -0.135 #> 85 unknown [Kh Mail_2].txt unknown [Ml Mail_3].txt  FALSE -0.214 #> 86 unknown [Kw Mail_3].txt unknown [Ml Mail_3].txt  FALSE  0.053 #> 87 unknown [Kh Mail_2].txt   known [Ml Mail_1].txt  FALSE  0.009 #> 88 unknown [Kw Mail_3].txt   known [Ml Mail_1].txt  FALSE  0.051 #> 89 unknown [Kh Mail_2].txt   known [Ml Mail_2].txt  FALSE -0.143 #> 90 unknown [Kw Mail_3].txt   known [Ml Mail_2].txt  FALSE -0.046 #> 91 unknown [Kh Mail_2].txt   known [Ml Mail_4].txt  FALSE  0.024 #> 92 unknown [Kw Mail_3].txt   known [Ml Mail_4].txt  FALSE -0.032 #> 93 unknown [Kh Mail_2].txt   known [Ml Mail_5].txt  FALSE  0.036 #> 94 unknown [Kw Mail_3].txt   known [Ml Mail_5].txt  FALSE  0.064"},{"path":"https://andreanini.github.io/idiolect/dev/reference/density_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot density of TRUE/FALSE distributions — density_plot","title":"Plot density of TRUE/FALSE distributions — density_plot","text":"Plot density TRUE/FALSE distributions","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/density_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot density of TRUE/FALSE distributions — density_plot","text":"","code":"density_plot(dataset, q = NULL)"},{"path":"https://andreanini.github.io/idiolect/dev/reference/density_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot density of TRUE/FALSE distributions — density_plot","text":"dataset data frame containing calibration dataset, typically output authorship analysis function like impostors(). q optional argument one value vector values contain score disputed text(s). plotted lines crossing density distributions.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/density_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot density of TRUE/FALSE distributions — density_plot","text":"ggplot2 plot density distributions scores TRUE (typically, '-author') vs. FALSE (typically, 'different-author').","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/density_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot density of TRUE/FALSE distributions — density_plot","text":"","code":"res <- data.frame(score = c(0.5, 0.2, 0.8, 0.01, 0.6), target = c(TRUE, FALSE, TRUE, FALSE, TRUE)) q <- c(0.11, 0.7) density_plot(res, q)"},{"path":"https://andreanini.github.io/idiolect/dev/reference/enron.sample.html","id":null,"dir":"Reference","previous_headings":"","what":"Enron sample — enron.sample","title":"Enron sample — enron.sample","text":"small sample Enron corpus comprising ten authors approximately amount data. author one text labelled 'unknown' texts labelled 'known'. data pre-processed using POSnoise algorithm mask content (see contentmask()).","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/enron.sample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enron sample — enron.sample","text":"","code":"enron.sample"},{"path":"https://andreanini.github.io/idiolect/dev/reference/enron.sample.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Enron sample — enron.sample","text":"quanteda corpus object.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/enron.sample.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Enron sample — enron.sample","text":"Halvani, Oren. 2021. Practice-Oriented Authorship Verification. Technical University Darmstadt PhD Thesis. https://tuprints.ulb.tu-darmstadt.de/19861/","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/impostors.html","id":null,"dir":"Reference","previous_headings":"","what":"Impostors Method — impostors","title":"Impostors Method — impostors","text":"function runs Impostors Method authorship verification. Impostors Method based calculating similarity score , using corpus impostor texts, perform bootstrapping analysis sampling random subsets features impostors order test robustness similarity.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/impostors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Impostors Method — impostors","text":"","code":"impostors(   q.data,   k.data,   cand.imps,   algorithm = \"RBI\",   coefficient = \"minmax\",   k = 300,   m = 100,   n = 25,   features = FALSE,   cores = NULL )"},{"path":"https://andreanini.github.io/idiolect/dev/reference/impostors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Impostors Method — impostors","text":"q.data questioned disputed data, either corpus (output create_corpus()) quanteda dfm (output vectorize()). k.data known undisputed data, either corpus (output create_corpus()) quanteda dfm (output vectorize()). one sample candidate author accepted algorithms except IM. cand.imps impostors data candidate authors, either corpus (output create_corpus()) quanteda dfm (output vectorize()). can object k.data (e.g. recycle impostors). algorithm string specifying impostors algorithm use, either \"RBI\" (deafult), \"KGI\", \"IM\". coefficient string indicating coefficient use, either \"minmax\" (default) \"cosine\". apply algorithm KGI, distance \"minmax\". k k parameters RBI algorithm. used algorithms. default 300. m m parameter IM algorithm. used algorithms. default 100. n n parameter IM algorithm. used algorithms. default 25. features logical value indicating whether important features retrieved . default FALSE. applies RBI algorithm. cores number cores use parallel processing (default one).","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/impostors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Impostors Method — impostors","text":"function test possible combinations Q texts candidate authors return data frame containing score ranging 0 1, higher score indicating higher likelihood author produced two sets texts. data frame contains column called \"target\" logical value TRUE author Q text candidate FALSE otherwise. RBI algorithm selected features parameter TRUE data frame also contain column features likely impact score. features consistently found shared candidate author's data questioned data also tend rare dataset impostors.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/impostors.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Impostors Method — impostors","text":"several variants Impostors Method function can run three : IM: original Impostors Method proposed Koppel Winter (2014). KGI: Kestemont's et al. (2016) version, popular implementation Impostors Method stylometry. inspired IM generalized version, General Impostors Method proposed Seidman (2013). RBI: Rank-Based Impostors Method (Potha Stamatatos 2017, 2020), default option recent tends outperform original. two data sets q.data, k.data, must disjunct terms texts contain otherwise error returned. However, cand.imps k.data can object, example, use candidates' texts impostors. function always exclude impostor texts author Q K texts considered.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/impostors.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Impostors Method — impostors","text":"Kestemont, Mike, Justin Stover, Moshe Koppel, Folgert Karsdorp & Walter Daelemans. 2016. Authenticating writings Julius Caesar. Expert Systems Applications 63. 86–96. https://doi.org/10.1016/j.eswa.2016.06.029. Koppel, Moshe & Yaron Winter. 2014. Determining two documents written author. Journal Association Information Science Technology 65(1). 178–187. Potha, Nektaria & Efstathios Stamatatos. 2017. Improved Impostors Method Authorship Verification. Gareth J.FALSE. Jones, Séamus Lawless, Julio Gonzalo, Liadh Kelly, Lorraine Goeuriot, Thomas Mandl, Linda Cappellato & Nicola Ferro (eds.), Experimental IR Meets Multilinguality, Multimodality, Interaction (Lecture Notes Computer Science), vol. 10456, 138–144. Springer, Cham. https://doi.org/10.1007/978-3-319-65813-1_14. (5 September, 2017). Potha, Nektaria & Efstathios Stamatatos. 2020. Improved algorithms extrinsic author verification. Knowledge Information Systems 62(5). 1903–1921. https://doi.org/10.1007/s10115-019-01408-4. Seidman, Shachar. 2013. Authorship Verification Using Impostors Method. Pamela Forner, Roberto Navigli, Dan Tufis & Nicola Ferro (eds.), Proceedings CLEF 2013 Evaluation Labs Workshop – Working Notes Papers, 23–26. Valencia, Spain. https://ceur-ws.org/Vol-1179/.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/impostors.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Impostors Method — impostors","text":"","code":"Q <- enron.sample[1] K <- enron.sample[2:3] imps <- enron.sample[4:9] impostors(Q, K, imps, algorithm = \"KGI\") #>    K                     Q target score #> 1 Kh known [Kh Mail_1].txt   TRUE  0.41"},{"path":"https://andreanini.github.io/idiolect/dev/reference/lambdaG.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply the LambdaG algorithm — lambdaG","title":"Apply the LambdaG algorithm — lambdaG","text":"function calculates likelihood ratio grammar models, \\(\\lambda_G\\), Nini et al. (review). order run analysis paper, data must preprocessed using contentmask() \"algorithm\" parameter set \"POSnoise\".","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/lambdaG.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply the LambdaG algorithm — lambdaG","text":"","code":"lambdaG(q.data, k.data, ref.data, N = 10, r = 30, cores = NULL)"},{"path":"https://andreanini.github.io/idiolect/dev/reference/lambdaG.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply the LambdaG algorithm — lambdaG","text":"q.data questioned disputed data quanteda tokens object tokens sentences (e.g. output tokenize_sents()). k.data known undisputed data quanteda tokens object tokens sentences (e.g. output tokenize_sents()). ref.data reference dataset quanteda tokens object tokens sentences (e.g. output tokenize_sents()). can object k.data. N order model. Default 10. r number iterations. Default 30. cores number cores use parallel processing (default one).","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/lambdaG.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply the LambdaG algorithm — lambdaG","text":"function test possible combinations Q texts candidate authors return data frame containing \\(\\lambda_G\\), uncalibrated log-likelihood ratio (base 10). \\(\\lambda_G\\) can calibrated likelihood ratio expresses strength evidence using calibrate_LLR(). data frame contains column called \"target\" logical value TRUE author Q text candidate FALSE otherwise.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/lambdaG.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Apply the LambdaG algorithm — lambdaG","text":"Nini, ., Halvani, O., Graner, L., Gherardi, V., Ishihara, S. Authorship Verification based Likelihood Ratio Grammar Models. https://arxiv.org/abs/2403.08462v1","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/lambdaG.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply the LambdaG algorithm — lambdaG","text":"","code":"q.data <- enron.sample[1] |> quanteda::tokens(\"sentence\") k.data <- enron.sample[2:10] |> quanteda::tokens(\"sentence\") ref.data <- enron.sample[11:ndoc(enron.sample)] |> quanteda::tokens(\"sentence\") lambdaG(q.data, k.data, ref.data) #>    K                     Q target  score #> 1 Kh known [Kh Mail_1].txt   TRUE 39.186 #> 2 Kw known [Kh Mail_1].txt  FALSE -6.720"},{"path":"https://andreanini.github.io/idiolect/dev/reference/lambdaG_visualize.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize the output of the LambdaG algorithm — lambdaG_visualize","title":"Visualize the output of the LambdaG algorithm — lambdaG_visualize","text":"function outputs colour-coded list sentences belonging input Q text ordered highest lowest \\(\\lambda_G\\), shown Nini et al. (review).","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/lambdaG_visualize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize the output of the LambdaG algorithm — lambdaG_visualize","text":"","code":"lambdaG_visualize(   q.data,   k.data,   ref.data,   N = 10,   r = 30,   output = \"html\",   print = \"\",   scale = \"absolute\",   negative = FALSE,   order.by = \"importance\",   cores = NULL )"},{"path":"https://andreanini.github.io/idiolect/dev/reference/lambdaG_visualize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize the output of the LambdaG algorithm — lambdaG_visualize","text":"q.data single questioned disputed text quanteda tokens object tokens sentences (e.g. output tokenize_sents()). k.data known undisputed corpus containing exclusively single candidate author's texts quanteda tokens object tokens sentences (e.g. output tokenize_sents()). ref.data reference dataset quanteda tokens object tokens sentences (e.g. output tokenize_sents()). N order model. Default 10. r number iterations. Default 30. output string detailing file type colour-coded text output. Either \"html\" (default) \"latex\". print string indicating path filename save colour-coded text file. left empty (default), nothing printed. scale string indicating scale use colour-code text file. \"absolute\" (default) raw \\(\\lambda_G\\) used; \"relative\", z-score \\(\\lambda_G\\) Q data used instead, thus showing relative importance. negative Logical. TRUE negative values \\(\\lambda_G\\) color-coded blue, otherwise (default) positive values \\(\\lambda_G\\) displayed red. applies HTML output. order.string indicating order output. \"importance\" (default) output ordered sentence \\(\\lambda_G\\) descending order, otherwise text displayed ordered appears. cores number cores use parallel processing (default one).","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/lambdaG_visualize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize the output of the LambdaG algorithm — lambdaG_visualize","text":"function outputs list two objects: data frame row token Q text values \\(\\lambda_G\\) token sentences, decreasing order sentence \\(\\lambda_G\\) relative contribution token sentence final \\(\\lambda_G\\) percentage; raw code html LaTeX generates colour-coded file. path provided print argument function also save colour-coded text html plain text file.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/lambdaG_visualize.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Visualize the output of the LambdaG algorithm — lambdaG_visualize","text":"Nini, ., Halvani, O., Graner, L., Gherardi, V., Ishihara, S. Authorship Verification based Likelihood Ratio Grammar Models. https://arxiv.org/abs/2403.08462v1","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/lambdaG_visualize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize the output of the LambdaG algorithm — lambdaG_visualize","text":"","code":"q.data <- corpus_trim(enron.sample[1], \"sentences\", max_ntoken = 10) |> quanteda::tokens(\"sentence\") k.data <- enron.sample[2:5]|> quanteda::tokens(\"sentence\") ref.data <- enron.sample[6:ndoc(enron.sample)] |> quanteda::tokens(\"sentence\") outputs <- lambdaG_visualize(q.data, k.data, ref.data, r = 2) outputs$table #> # A tibble: 13 × 8 #>    sentence_id token_id t         lambdaG sentence_lambdaG zlambdaG #>          <int>    <int> <chr>       <dbl>            <dbl>    <dbl> #>  1           1        1 J          0.771             0.726    1.40  #>  2           1        2 N          0.220             0.726    0.323 #>  3           1        3 ,         -1.16              0.726   -2.38  #>  4           1        4 but        0.326             0.726    0.531 #>  5           1        5 that      -0.0955            0.726   -0.297 #>  6           1        6 's         0.991             0.726    1.84  #>  7           1        7 just      -0.0210            0.726   -0.151 #>  8           1        8 the        0.0194            0.726   -0.072 #>  9           1        9 N          0.0259            0.726   -0.059 #> 10           1       10 it        -0.252             0.726   -0.605 #> 11           1       11 works     -0.102             0.726   -0.31  #> 12           1       12 .          0.0365            0.726   -0.038 #> 13           1       13 ___EOS___ -0.0385            0.726   -0.185 #> # ℹ 2 more variables: token_contribution <dbl>, sent_contribution <dbl>"},{"path":"https://andreanini.github.io/idiolect/dev/reference/most_similar.html","id":null,"dir":"Reference","previous_headings":"","what":"Select the most similar texts to a specific text — most_similar","title":"Select the most similar texts to a specific text — most_similar","text":"Select similar texts specific text","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/most_similar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select the most similar texts to a specific text — most_similar","text":"","code":"most_similar(sample, pool, coefficient, n)"},{"path":"https://andreanini.github.io/idiolect/dev/reference/most_similar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select the most similar texts to a specific text — most_similar","text":"sample single row quanteda dfm representing sample match. pool dfm containing possible samples select top n. coefficient coefficient use similarity. Either \"minmax\", \"cosine\", \"Phi\". n number rows extract pool potential samples.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/most_similar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select the most similar texts to a specific text — most_similar","text":"function returns dfm containing top n similar rows input sample using minmax distance.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/most_similar.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select the most similar texts to a specific text — most_similar","text":"","code":"text1 <- \"The cat sat on the mat\" text2 <- \"The dog sat on the chair\" text3 <- \"Violence is the last refuge of the incompetent\" c <- quanteda::corpus(c(text1, text2, text3)) d <- quanteda::tokens(c) |> quanteda::dfm() |> quanteda::dfm_weight(scheme = \"prop\") most_similar(d[1,], d[-1,], coefficient = \"minmax\", n = 1) #> Document-feature matrix of: 1 document, 13 features (61.54% sparse) and 0 docvars. #>        features #> docs          the cat       sat        on mat       dog     chair violence is #>   text2 0.3333333   0 0.1666667 0.1666667   0 0.1666667 0.1666667        0  0 #>        features #> docs    last #>   text2    0 #> [ reached max_nfeat ... 3 more features ]"},{"path":"https://andreanini.github.io/idiolect/dev/reference/ngram_tracing.html","id":null,"dir":"Reference","previous_headings":"","what":"N-gram tracing — ngram_tracing","title":"N-gram tracing — ngram_tracing","text":"function runs authorship analysis method called n-gram tracing, can used attribution verification.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/ngram_tracing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"N-gram tracing — ngram_tracing","text":"","code":"ngram_tracing(   q.data,   k.data,   tokens = \"character\",   remove_punct = FALSE,   remove_symbols = TRUE,   remove_numbers = TRUE,   lowercase = TRUE,   n = 9,   coefficient = \"simpson\",   features = FALSE,   cores = NULL )"},{"path":"https://andreanini.github.io/idiolect/dev/reference/ngram_tracing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"N-gram tracing — ngram_tracing","text":"q.data questioned disputed data, either corpus (output create_corpus()) quanteda dfm (output vectorize()). k.data known undisputed data, either corpus (output create_corpus()) quanteda dfm (output vectorize()). one sample candidate author accepted function combine make profile. tokens type tokens extract, either \"word\" \"character\" (default). remove_punct logical value. FALSE (default) keeps punctuation marks. remove_symbols logical value. TRUE (default) removes symbols. remove_numbers logical value. TRUE (default) removes numbers. lowercase logical value. TRUE (default) transforms tokens lower case. n order size n-grams extracted. Default 9. coefficient coefficient use compare texts, one : \"simpson\" (default), \"phi\", \"jaccard\", \"kulczynski\", \"cole\". features Logical default FALSE. TRUE result table contain features overlap unique overlap corpus. two texts present return n-grams common. cores number cores use parallel processing (default one).","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/ngram_tracing.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"N-gram tracing — ngram_tracing","text":"function test possible combinations Q texts candidate authors return data frame containing value similarity coefficient selected called 'score' optional column overlapping features occur Q candidate considered Qs (ordered length n-gram variable length). data frame contains column called 'target' logical value TRUE author Q text candidate FALSE otherwise.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/ngram_tracing.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"N-gram tracing — ngram_tracing","text":"N-gram tracing originally proposed Grieve et al (2019). Nini (2023) proposed mathematical reinterpretation compatible Cognitive Linguistic theories language processing. tested several variants method found original version, uses Simpson's coefficient, tends outperformed versions using Phi coefficient, Kulczynski's coefficient, Cole coefficient. function can run n-gram tracing method using coefficients plus Jaccard coefficient reference, coefficient applied several forensic linguistic studies.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/ngram_tracing.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"N-gram tracing — ngram_tracing","text":"Grieve, Jack, Emily Chiang, Isobelle Clarke, Hannah Gideon, Aninna Heini, Andrea Nini & Emily Waibel. 2019. Attributing Bixby Letter using n-gram tracing. Digital Scholarship Humanities 34(3). 493–512. Nini, Andrea. 2023. Theory Linguistic Individuality Authorship Analysis (Elements Forensic Linguistics). Cambridge, UK: Cambridge University Press.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/ngram_tracing.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"N-gram tracing — ngram_tracing","text":"","code":"Q <- enron.sample[c(5:6)] K <- enron.sample[-c(5:6)] ngram_tracing(Q, K, coefficient = 'phi') #>                          Q  K target  score #> 1  unknown [Kh Mail_2].txt Kh   TRUE  0.032 #> 2  unknown [Kw Mail_3].txt Kh  FALSE  0.030 #> 3  unknown [Kh Mail_2].txt Kw  FALSE  0.031 #> 4  unknown [Kw Mail_3].txt Kw   TRUE  0.086 #> 5  unknown [Kh Mail_2].txt Lc  FALSE  0.028 #> 6  unknown [Kw Mail_3].txt Lc  FALSE  0.027 #> 7  unknown [Kh Mail_2].txt Ld  FALSE  0.026 #> 8  unknown [Kw Mail_3].txt Ld  FALSE  0.046 #> 9  unknown [Kh Mail_2].txt Lt  FALSE  0.033 #> 10 unknown [Kw Mail_3].txt Lt  FALSE  0.048 #> 11 unknown [Kh Mail_2].txt Lk  FALSE  0.014 #> 12 unknown [Kw Mail_3].txt Lk  FALSE  0.027 #> 13 unknown [Kh Mail_2].txt Lb  FALSE  0.017 #> 14 unknown [Kw Mail_3].txt Lb  FALSE  0.040 #> 15 unknown [Kh Mail_2].txt La  FALSE  0.006 #> 16 unknown [Kw Mail_3].txt La  FALSE -0.018 #> 17 unknown [Kh Mail_2].txt Mf  FALSE  0.021 #> 18 unknown [Kw Mail_3].txt Mf  FALSE  0.038 #> 19 unknown [Kh Mail_2].txt Ml  FALSE  0.016 #> 20 unknown [Kw Mail_3].txt Ml  FALSE  0.047"},{"path":"https://andreanini.github.io/idiolect/dev/reference/performance.html","id":null,"dir":"Reference","previous_headings":"","what":"Performance evaluation — performance","title":"Performance evaluation — performance","text":"function used test performance authorship analysis method.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/performance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Performance evaluation — performance","text":"","code":"performance(training, test = NULL, by = \"case\", progress = TRUE)"},{"path":"https://andreanini.github.io/idiolect/dev/reference/performance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Performance evaluation — performance","text":"training data frame results evaluate, typically output authorship analysis function, impostors(). training present function perform leave-one-cross-validation. test Optional data frame results. present calibration model extracted training performance evaluated data set. Either \"case\" \"author\". performance evaluated leave-one-, \"case\" go table row row , \"author\" selected, performance calculated taking author (identified value K column). progress Logical. TRUE (default) progress bar diplayed.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/performance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Performance evaluation — performance","text":"function returns list containing data frame performance statistics, including object can used make tippet plot using tippet.plot() function ROC package (https://github.com/davidavdav/ROC).","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/performance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Performance evaluation — performance","text":"applying method real authorship case, good practice test known ground truth data. function performs test taking input either single table results two tables, one training one test, returning output list following performance statistics: log-likelihood ratio cost (\\(C_{llr}\\) \\(C_{llr}^{min}\\)), Equal Error Rate (ERR), mean values log-likelihood ratio -author (TRUE) different-author (FALSE) cases, Area Curve (AUC), Balanced Accuracy, Precision, Recall, F1, full confusion matrix. binary classification statistics calculated considering Log-Likelihood Ratio score 0 threshold.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/performance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Performance evaluation — performance","text":"","code":"results <- data.frame(score = c(0.5, 0.2, 0.8, 0.01), target = c(TRUE, FALSE, TRUE, FALSE)) perf <- performance(results) #>    |                                                                               |                                                                      |   0%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================================| 100% perf$evaluation #>        Cllr  Cllr_min EER Mean TRUE LLR Mean FALSE LLR TRUE trials FALSE trials #> 1 0.2422848 0.4150375  25      14.91206      -12.77601           4            4 #>   AUC Balanced Accuracy Precision Recall F1 TP FN FP TN #> 1   1                 1         1      1  1  2  0  0  2"},{"path":"https://andreanini.github.io/idiolect/dev/reference/posterior.html","id":null,"dir":"Reference","previous_headings":"","what":"Posterior prosecution probabilities and odds — posterior","title":"Posterior prosecution probabilities and odds — posterior","text":"function takes input value Log-Likelihood Ratio returns table shows impact simulated prior probabilities prosecution hypothesis.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/posterior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Posterior prosecution probabilities and odds — posterior","text":"","code":"posterior(LLR)"},{"path":"https://andreanini.github.io/idiolect/dev/reference/posterior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Posterior prosecution probabilities and odds — posterior","text":"LLR One single numeric value corresponding Log-Likelihood Ratio (base 10).","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/posterior.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Posterior prosecution probabilities and odds — posterior","text":"data frame containing simulated prior probabilities/odds prosecution resulting posterior probabilities/odds LLR.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/posterior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Posterior prosecution probabilities and odds — posterior","text":"","code":"posterior(LLR = 0) #> # A tibble: 11 × 6 #>    prosecution_prior_probs prior_odds   LLR    LR  post_odds #>                      <dbl>      <dbl> <dbl> <dbl>      <dbl> #>  1                0.000001 0.00000100     0     1 0.00000100 #>  2                0.01     0.0101         0     1 0.0101     #>  3                0.1      0.111          0     1 0.111      #>  4                0.2      0.25           0     1 0.25       #>  5                0.3      0.429          0     1 0.429      #>  6                0.4      0.667          0     1 0.667      #>  7                0.5      1              0     1 1          #>  8                0.6      1.5            0     1 1.5        #>  9                0.7      2.33           0     1 2.33       #> 10                0.8      4              0     1 4          #> 11                0.9      9              0     1 9          #> # ℹ 1 more variable: prosecution_post_probs <dbl> posterior(LLR = 1.8) #> # A tibble: 11 × 6 #>    prosecution_prior_probs prior_odds   LLR    LR   post_odds #>                      <dbl>      <dbl> <dbl> <dbl>       <dbl> #>  1                0.000001 0.00000100   1.8  63.1   0.0000631 #>  2                0.01     0.0101       1.8  63.1   0.637     #>  3                0.1      0.111        1.8  63.1   7.01      #>  4                0.2      0.25         1.8  63.1  15.8       #>  5                0.3      0.429        1.8  63.1  27.0       #>  6                0.4      0.667        1.8  63.1  42.1       #>  7                0.5      1            1.8  63.1  63.1       #>  8                0.6      1.5          1.8  63.1  94.6       #>  9                0.7      2.33         1.8  63.1 147.        #> 10                0.8      4            1.8  63.1 252.        #> 11                0.9      9            1.8  63.1 568.        #> # ℹ 1 more variable: prosecution_post_probs <dbl> posterior(LLR = -0.5) #> # A tibble: 11 × 6 #>    prosecution_prior_probs prior_odds   LLR    LR   post_odds #>                      <dbl>      <dbl> <dbl> <dbl>       <dbl> #>  1                0.000001 0.00000100  -0.5 0.316 0.000000316 #>  2                0.01     0.0101      -0.5 0.316 0.00319     #>  3                0.1      0.111       -0.5 0.316 0.0351      #>  4                0.2      0.25        -0.5 0.316 0.0791      #>  5                0.3      0.429       -0.5 0.316 0.136       #>  6                0.4      0.667       -0.5 0.316 0.211       #>  7                0.5      1           -0.5 0.316 0.316       #>  8                0.6      1.5         -0.5 0.316 0.474       #>  9                0.7      2.33        -0.5 0.316 0.738       #> 10                0.8      4           -0.5 0.316 1.26        #> 11                0.9      9           -0.5 0.316 2.85        #> # ℹ 1 more variable: prosecution_post_probs <dbl> posterior(LLR = 4) #> # A tibble: 11 × 6 #>    prosecution_prior_probs prior_odds   LLR    LR  post_odds #>                      <dbl>      <dbl> <dbl> <dbl>      <dbl> #>  1                0.000001 0.00000100     4 10000     0.0100 #>  2                0.01     0.0101         4 10000   101.     #>  3                0.1      0.111          4 10000  1111.     #>  4                0.2      0.25           4 10000  2500      #>  5                0.3      0.429          4 10000  4286.     #>  6                0.4      0.667          4 10000  6667.     #>  7                0.5      1              4 10000 10000      #>  8                0.6      1.5            4 10000 15000      #>  9                0.7      2.33           4 10000 23333.     #> 10                0.8      4              4 10000 40000      #> 11                0.9      9              4 10000 90000      #> # ℹ 1 more variable: prosecution_post_probs <dbl>"},{"path":"https://andreanini.github.io/idiolect/dev/reference/tokenize_sents.html","id":null,"dir":"Reference","previous_headings":"","what":"Tokenize to sentences — tokenize_sents","title":"Tokenize to sentences — tokenize_sents","text":"function turns corpus texts quanteda tokens object sentences.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/tokenize_sents.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tokenize to sentences — tokenize_sents","text":"","code":"tokenize_sents(corpus, model = \"en_core_web_sm\")"},{"path":"https://andreanini.github.io/idiolect/dev/reference/tokenize_sents.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tokenize to sentences — tokenize_sents","text":"corpus quanteda corpus object, typically output create_corpus() function output contentmask(). model spacy model use. default \"en_core_web_sm\".","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/tokenize_sents.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tokenize to sentences — tokenize_sents","text":"quanteda tokens object token sentence.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/tokenize_sents.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tokenize to sentences — tokenize_sents","text":"function first split text paragraphs splitting new line markers uses spacy tokenize paragraph sentences. function accepts plain text corpus input output contentmask(). function necessary prepare data lambdaG().","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/tokenize_sents.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tokenize to sentences — tokenize_sents","text":"","code":"if (FALSE) { # \\dontrun{ toy.pos <- corpus(\"the N was on the N . he did n't move \\n N ; \\n N N\") tokenize_sents(toy.pos) } # }"},{"path":"https://andreanini.github.io/idiolect/dev/reference/vectorize.html","id":null,"dir":"Reference","previous_headings":"","what":"Vectorize data — vectorize","title":"Vectorize data — vectorize","text":"function turns texts feature vectors.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/vectorize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Vectorize data — vectorize","text":"","code":"vectorize(   input,   tokens,   remove_punct,   remove_symbols,   remove_numbers,   lowercase,   n,   weighting,   trim,   threshold )"},{"path":"https://andreanini.github.io/idiolect/dev/reference/vectorize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Vectorize data — vectorize","text":"input quanteda corpus object author names docvar called \"author\". Typically, output create_corpus() function. tokens type tokens extract, either \"character\" \"word\". remove_punct logical value. FALSE keep punctuation marks TRUE remove . remove_symbols logical value. TRUE removes symbols FALSE keeps . remove_numbers logical value. TRUE removes numbers FALSE keeps . lowercase logical value. TRUE transforms tokens lower case. n order size n-grams extracted. weighting type weighting use, \"rel\" relative frequencies, \"tf-idf\", \"boolean\". trim logical value. TRUE frequent tokens kept. threshold numeric value indicating many frequent tokens keep.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/vectorize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Vectorize data — vectorize","text":"dfm (document-feature matrix) containing text feature vector. N-gram tokenisation cross sentence boundaries.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/vectorize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Vectorize data — vectorize","text":"authorship analysis functions call vectorize() standard parameters algorithm selected. function therefore left users want modify parameters convenience dfm reused algorithms avoid vectorizing data many times. users need run standard analysis need use function.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/reference/vectorize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Vectorize data — vectorize","text":"","code":"mycorpus <- quanteda::corpus(\"The cat sat on the mat.\") quanteda::docvars(mycorpus, \"author\") <- \"author1\" matrix <- vectorize(mycorpus, tokens = \"character\", remove_punct = FALSE, remove_symbols = TRUE, remove_numbers = TRUE, lowercase = TRUE, n = 5, weighting = \"rel\", trim = TRUE, threshold = 1500)"},{"path":"https://andreanini.github.io/idiolect/dev/news/index.html","id":"idiolect-development-version","dir":"Changelog","previous_headings":"","what":"idiolect (development version)","title":"idiolect (development version)","text":"add possibility visualize text heatmap either sentences ordered lambdaG values (default) order sentences text add possibility visualize negative lambdaG values html file concordance() now can take sentences input also show sentence boundaries changed progress bar performance() optional added option leave-one-author performance() bug fixes","code":""},{"path":"https://andreanini.github.io/idiolect/dev/news/index.html","id":"idiolect-101","dir":"Changelog","previous_headings":"","what":"idiolect 1.0.1","title":"idiolect 1.0.1","text":"CRAN release: 2024-08-28 Fixed issues CRAN review.","code":""},{"path":"https://andreanini.github.io/idiolect/dev/news/index.html","id":"idiolect-100","dir":"Changelog","previous_headings":"","what":"idiolect 1.0.0","title":"idiolect 1.0.0","text":"Initial CRAN submission.","code":""}]
