<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>idiolect • idiolect</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="idiolect">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">idiolect</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">1.0.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="../articles/idiolect.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/andreanini/idiolect/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>idiolect</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/andreanini/idiolect/blob/v1.0.1/vignettes/idiolect.Rmd" class="external-link"><code>vignettes/idiolect.Rmd</code></a></small>
      <div class="d-none name"><code>idiolect.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/andreanini/idiolect" class="external-link">idiolect</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading required package: quanteda</span></span>
<span><span class="co">#&gt; Package version: 4.0.2</span></span>
<span><span class="co">#&gt; Unicode version: 14.0</span></span>
<span><span class="co">#&gt; ICU version: 70.1</span></span>
<span><span class="co">#&gt; Parallel computing: disabled</span></span>
<span><span class="co">#&gt; See https://quanteda.io for tutorials and examples.</span></span></code></pre></div>
<p><code>idiolect</code> is a package that depends on
<code>quanteda</code> for all the main Natural Language Processing
functions. Although the basic object types and functions are described
in detail in the documentation of this package, familiarity with
<code>quanteda</code> is highly recommended. More information about
<code>quanteda</code> can be found on its <a href="http://quanteda.io" class="external-link">website</a>.</p>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Authorship Analysis is defined as the task of determining the
likelihood that a certain candidate is the author of a certain set of
questioned or disputed texts. We call <em>Forensic</em> Authorship
Analysis a task of this kind applied in a real forensic case. In such
settings, the disputed texts could be anonymous malicious documents,
such as a threatening letter, but could also be text messages, emails,
or any other document that, for various reasons, becomes evidence in a
forensic case. In Forensic Linguistics, typically a set of disputed or
<em>questioned</em> text is indicated as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>,
while a set of texts of known origin, for example the texts written by
the candidate author and collected as comparison material, is labelled
using
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>.
In addition to these two datasets, the analysis also necessitates of a
comparison reference corpus that we call
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>.
In a classic case involving a closed set of suspects, the texts written
by the suspects minus the candidate form
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>.
In <em>Authorship Verification</em> cases that only involve one
candidate author, then the reference dataset might have to be compiled
by the analyst for the specific case <span class="citation">(Ishihara et
al. 2024)</span>.</p>
<p>A crucial difference between Authorship Analysis and Forensic
Authorship Analysis is that whereas the former can be treated as a
classification task where the final answer is binary (‘candidate is the
author’ vs. ‘candidate is NOT the author’), the latter needs an
expression of likelihood for the two competing propositions or
hypotheses, the Prosecution Hypothesis
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mi>p</mi></msub><annotation encoding="application/x-tex">H_p</annotation></semantics></math>
vs. the Defence Hypothesis
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mi>d</mi></msub><annotation encoding="application/x-tex">H_d</annotation></semantics></math>,
for example:</p>
<div class="line-block">
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mi>p</mi></msub><annotation encoding="application/x-tex">H_p</annotation></semantics></math>:
The author of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
and the author of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
are the same individual.<br><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mi>d</mi></msub><annotation encoding="application/x-tex">H_d</annotation></semantics></math>:
The author of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
and the author of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
are two different individuals.</div>
<p>The job of the forensic linguist in a forensic context is to analyse
the linguistic evidence and determine which hypothesis it supports and
with what degree of strength, thus aiding the trier-of-fact in reaching
a conclusion. The role of the forensic linguist is therefore not to
provide a YES/NO answer but rather to express the strength of the
evidence in favour of each of these two hypotheses.</p>
<p>Given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>,
the workflow for this analysis involves four steps:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Preparation</strong>: This step involves any pre-processing
step that is necessary for the analysis with the chosen method;</li>
<li>
<strong>Validation</strong>: Carry out an analysis on the case data
or on a separate dataset that has been designed to be similar to the
case material in order to validate the method for this particular
case;</li>
<li>
<strong>Analysis</strong>: Carry out the analysis on the real
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>;</li>
<li>
<strong>Calibration</strong>: Turn the output of (3) into a
Likelihood Ratio that expresses the strength of the evidence given the
two competing hypotheses.</li>
</ol>
</div>
<div class="section level2">
<h2 id="preparation">Preparation<a class="anchor" aria-label="anchor" href="#preparation"></a>
</h2>
<p><code>idiolect</code> has a function to import texts into
<code>R</code> called <code><a href="../reference/create_corpus.html">create_corpus()</a></code>. This function is
simply calling <code>readtext</code> (therefore this package must be
installed) while scanning the name of the files for the metadata of each
text, specifically the name of the author and the name of the file. The
syntax to follow to name the files is</p>
<div class="line-block">authorname_textname.txt
(e.g. smith_text1.txt).</div>
<p>Assuming that a folder of plain text files with names according to
this syntax are ready on the user’s computer, then the following command
(not executed here) loads the folder as a <code>quanteda</code> corpus
object with the metadata as <code>docvars</code>.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">corpus</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/create_corpus.html">create_corpus</a></span><span class="op">(</span><span class="st">"path/to/folder"</span><span class="op">)</span></span></code></pre></div>
<p>In this vignette, instead, the workflow is demonstrated using a small
dataset of the Enron corpus that is included in this package (see
<code><a href="../reference/enron.sample.html">?enron.sample</a></code>).</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">corpus</span> <span class="op">&lt;-</span> <span class="va">enron.sample</span></span></code></pre></div>
<p>This corpus is a <code>quanteda</code> corpus object that contains
ten authors with approximately the same amount of data.</p>
<div class="section level3">
<h3 id="content-masking">Content masking<a class="anchor" aria-label="anchor" href="#content-masking"></a>
</h3>
<p>A highly recommended and sometimes necessary pre-processing step is
content masking. This step consists in masking or removing words or
other tokens in the text that are likely to create noise for an
authorship analysis. Hiding content not only avoids incorrectly
attributing a text based on the correlation between topics and authors
<span class="citation">(Bischoff et al., n.d.)</span> but also tends to
improve the performance of authorship analysis methods in cross-topic
and cross-genre situations <span class="citation">(Stamatatos
2017)</span>.</p>
<p>Three content masking methods are implemented in
<code>idiolect</code>: (1) the <em>POSnoise</em> algorithm developed by
<span class="citation">Halvani and Graner (2021)</span>; (2) the
<em>frame n-grams</em> approach introduced by <span class="citation">Nini (2023)</span>; and (3) an implementation of the
<em>TextDistortion</em> approach originally introduced by <span class="citation">Stamatatos (2017)</span>. These options are available
in the <code><a href="../reference/contentmask.html">contentmask()</a></code> function. Because this function
depends on <a href="https://spacyr.quanteda.io/reference/spacyr-package.html" class="external-link"><code>spacyr</code></a>
and this requires downloading a parsing model for a language for the
automatic tagging of Parts of Speech (e.g. nouns, adjectives, adverbs),
this function is not run in this vignette. Instead, the Enron sample has
already been content-masked using <em>POSnoise</em>, as can be seen from
the preview of the corpus</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">corpus</span></span>
<span><span class="co">#&gt; Corpus consisting of 49 documents and 1 docvar.</span></span>
<span><span class="co">#&gt; known [Kh Mail_1].txt :</span></span>
<span><span class="co">#&gt; "N N N N wants to be N when he V up likes N P , N for doing t..."</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; known [Kh Mail_3].txt :</span></span>
<span><span class="co">#&gt; "i 've V a J one , but the only N N N i have is on a N N from..."</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; known [Kh Mail_4].txt :</span></span>
<span><span class="co">#&gt; "this was J towards the N of a J N N N . in N , P P helped th..."</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; known [Kh Mail_5].txt :</span></span>
<span><span class="co">#&gt; "V the N for more than D N may get you V . a N N with a N and..."</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; unknown [Kh Mail_2].txt :</span></span>
<span><span class="co">#&gt; "P , here 's the J N on our P P N V to V the V needs of the P..."</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; unknown [Kw Mail_3].txt :</span></span>
<span><span class="co">#&gt; "they also have J N at the J N of P D per N and only a D J ea..."</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [ reached max_ndoc ... 43 more documents ]</span></span></code></pre></div>
<p>The <em>POSnoise</em> algorithm essentially replaces all words that
tend to contain meaning (nouns, verbs, adjectives, adverbs) with their
Part of Speech tag (N, V, J, B) while all the other words or tokens are
left unchanged. In addition to this operation, <em>POSnoise</em>
contains a white list of content words that mostly tend to be functional
in English, such as verbs like <em>do, have, make</em> or adverbs such
as <em>consequently</em>, <em>therefore</em>.</p>
<p>The following code should be used to run the
<code><a href="../reference/contentmask.html">contentmask()</a></code> function. This will require installing and
initiating a <a href="https://spacy.io" class="external-link"><em>spacy</em></a> parsing model
for the language chosen. This process should happen automatically</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">posnoised.corpus</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/contentmask.html">contentmask</a></span><span class="op">(</span><span class="va">corpus</span>, model <span class="op">=</span> <span class="st">"en_core_web_sm"</span>, algorithm <span class="op">=</span> <span class="st">"POSnoise"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="data-labelling">Data labelling<a class="anchor" aria-label="anchor" href="#data-labelling"></a>
</h3>
<p>In this example it is simulated that the first text written by the
author <em>Kw</em> is the real
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
text (the one labelled as ‘unknown’) and all the other known texts
written by <em>Kw</em> (labelled as ‘known’) are therefore the set of
known texts
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>.
The remaining texts from the other authors are the reference samples
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://quanteda.io/reference/corpus_subset.html" class="external-link">corpus_subset</a></span><span class="op">(</span><span class="va">corpus</span>, <span class="va">author</span> <span class="op">==</span> <span class="st">"Kw"</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">K</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://quanteda.io/reference/corpus_subset.html" class="external-link">corpus_subset</a></span><span class="op">(</span><span class="va">corpus</span>, <span class="va">author</span> <span class="op">==</span> <span class="st">"Kw"</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span></span>
<span><span class="va">R</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://quanteda.io/reference/corpus_subset.html" class="external-link">corpus_subset</a></span><span class="op">(</span><span class="va">corpus</span>, <span class="va">author</span> <span class="op">!=</span> <span class="st">"Kw"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="vectorisation">Vectorisation<a class="anchor" aria-label="anchor" href="#vectorisation"></a>
</h3>
<p>Before applying certain authorship analysis methods, each text or
sample must be turned into a numerical representation called a
<em>feature vector</em>, a process typically referred to as
<em>vectorisation</em>. <code>idiolect</code> has a function to
vectorise a corpus called <code><a href="../reference/vectorize.html">vectorize()</a></code>. The features
normally used by many authorship analysis methods are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-grams
of words and punctuation marks or characters. For example, the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
text can be vectorised into the relative frequencies of its words using
this code.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/vectorize.html">vectorize</a></span><span class="op">(</span><span class="va">Q</span>, tokens <span class="op">=</span> <span class="st">"word"</span>, remove_punct <span class="op">=</span> <span class="cn">F</span>, remove_symbols <span class="op">=</span> <span class="cn">T</span>, remove_numbers <span class="op">=</span> <span class="cn">T</span>,</span>
<span>          lowercase <span class="op">=</span> <span class="cn">T</span>, n <span class="op">=</span> <span class="fl">1</span>, weighting <span class="op">=</span> <span class="st">"rel"</span>, trim <span class="op">=</span> <span class="cn">F</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span>max_nfeat <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt; Document-feature matrix of: 1 document, 136 features (0.00% sparse) and 1 docvar.</span></span>
<span><span class="co">#&gt;                          features</span></span>
<span><span class="co">#&gt; docs                            they        also      have</span></span>
<span><span class="co">#&gt;   unknown [Kw Mail_3].txt 0.00289296 0.009643202 0.0192864</span></span>
<span><span class="co">#&gt; [ reached max_nfeat ... 133 more features ]</span></span></code></pre></div>
<p>or, as the most frequent 1,000 character 4-grams relative
frequencies, for example, using</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/vectorize.html">vectorize</a></span><span class="op">(</span><span class="va">Q</span>, tokens <span class="op">=</span> <span class="st">"character"</span>, remove_punct <span class="op">=</span> <span class="cn">F</span>, remove_symbols <span class="op">=</span> <span class="cn">T</span>, remove_numbers <span class="op">=</span> <span class="cn">T</span>,</span>
<span>          lowercase <span class="op">=</span> <span class="cn">T</span>, n <span class="op">=</span> <span class="fl">4</span>, weighting <span class="op">=</span> <span class="st">"rel"</span>, trim <span class="op">=</span> <span class="cn">T</span>, threshold <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span>max_nfeat <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt; Document-feature matrix of: 1 document, 1,094 features (0.00% sparse) and 1 docvar.</span></span>
<span><span class="co">#&gt;                          features</span></span>
<span><span class="co">#&gt; docs                              they         hey          ey a</span></span>
<span><span class="co">#&gt;   unknown [Kw Mail_3].txt 0.0009771987 0.0009771987 0.0003257329</span></span>
<span><span class="co">#&gt; [ reached max_nfeat ... 1,091 more features ]</span></span></code></pre></div>
<p>The output of the function is a <code>quanteda</code>
<em>document-feature matrix</em> (or <em>dfm</em>) that can efficiently
store even very large matrices.</p>
<p>This <code><a href="../reference/vectorize.html">vectorize()</a></code> function is mostly designed for expert
users because different choices in the parameters of the vectorisation
can be made using each single authorship analysis method function. In
addition, since most authorship analysis methods already have a default
setting of these parameters, these are already the default for the
authorship analysis functions.</p>
<p>This step is therefore not necessary unless there are specific
requirements as any vectorisation is handled by the functions that apply
the authorship analysis methods.</p>
</div>
</div>
<div class="section level2">
<h2 id="validation">Validation<a class="anchor" aria-label="anchor" href="#validation"></a>
</h2>
<p>The first step of the validation is to remove the real
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
text. This is the actual forensic sample to analyse and it must be
therefore removed when validating the analysis. The validation set is
therefore made up of only the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>
datasets</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">validation</span> <span class="op">&lt;-</span> <span class="va">K</span> <span class="op">+</span> <span class="va">R</span></span></code></pre></div>
<p>This dataset can now be re-divided into ‘fake’
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
texts and ‘fake’
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
texts. Each text in this corpus is labelled as ‘unknown’ or ‘known’ so
two new disjoint datasets, <code>validation.Q</code> and
<code>validation.K</code> can be created by selecting the texts based on
this label.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">validation.Q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://quanteda.io/reference/corpus_subset.html" class="external-link">corpus_subset</a></span><span class="op">(</span><span class="va">validation</span>, <span class="fu"><a href="https://rdrr.io/r/base/grep.html" class="external-link">grepl</a></span><span class="op">(</span><span class="st">"^unknown"</span>, <span class="fu"><a href="https://quanteda.io/reference/docnames.html" class="external-link">docnames</a></span><span class="op">(</span><span class="va">validation</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">validation.K</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://quanteda.io/reference/corpus_subset.html" class="external-link">corpus_subset</a></span><span class="op">(</span><span class="va">validation</span>, <span class="fu"><a href="https://rdrr.io/r/base/grep.html" class="external-link">grepl</a></span><span class="op">(</span><span class="st">"^known"</span>, <span class="fu"><a href="https://quanteda.io/reference/docnames.html" class="external-link">docnames</a></span><span class="op">(</span><span class="va">validation</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>This is not the only way in which a validation analysis can be
conducted. For example, one could adopt a leave-one-out approach by
taking each single text and treat it as a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
and then run an authorship analysis method for each one of them.
Alternatively, a completely different dataset that is similar to the
case data could be used. This simpler approach is more suitable for this
small example.</p>
<div class="section level3">
<h3 id="authorship-analysis">Authorship analysis<a class="anchor" aria-label="anchor" href="#authorship-analysis"></a>
</h3>
<p>The analysis that is being validated is the same analysis that will
be applied to the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
text. Therefore, a choice of method has to be made depending on the
right choice to analyse
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>.
In this example, the scenario simulated is a <em>verification</em>: was
the unknown
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
text written by the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
author, <em>Kw</em>? For this reason, the method chosen is one of the
most successful authorship verification methods available today, the
<em>Impostors Method</em> <span class="citation">(Koppel and Winter
2014)</span>, and in particular one of its latest variants called the
<em>Rank-Based Impostors Method</em> <span class="citation">(Potha and
Stamatatos 2017, 2020)</span>.</p>
<p>This analysis can be run in <code>idiolect</code> using the function
<code><a href="../reference/impostors.html">impostors()</a></code> and then selecting the default parameter for
the <em>algorithm</em> argument, “<em>RBI</em>”. The main argument of
this function are the <em>q.data</em>, which is the set of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
texts to test, and the <em>k.data</em>, which is the set of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
texts from one or more authors that are going to be tested, and finally
the set of impostors data, <em>cand.imps</em>. For this example, the
impostors data is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>
set but generally the recommendation is to use another dataset when
possible.</p>
<p>The <code><a href="../reference/impostors.html">impostors()</a></code> function accepts more than one author in
<em>k.data</em> and it also accepts the same dataset as input for both
<em>k.data</em> and <em>cand.imps</em>. When the same dataset is used,
<code><a href="../reference/impostors.html">impostors()</a></code> will test each author in <em>k.data</em> and
use the texts written by other authors as impostors.</p>
<p>In contrast to other authorship analysis functions like
<code><a href="../reference/delta.html">delta()</a></code> and <code><a href="../reference/ngram_tracing.html">ngram_tracing()</a></code>,
<code><a href="../reference/impostors.html">impostors()</a></code> does not offer additional parameters to modify
the vectorisation process because all the Impostors Method algorithms
already have a well-specified default setting. If the user wants to
change that they should then vectorise the corpus separately using
<code><a href="../reference/vectorize.html">vectorize()</a></code> and then use the <em>dfm</em> as the input of
<code><a href="../reference/impostors.html">impostors()</a></code>.</p>
<p>The RBI variant of the method also requires setting a parameter
called
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>,
which is the number of most similar impostors texts to sample from the
wider set of impostors. The recommended setting is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">k=100</annotation></semantics></math>
or
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>300</mn></mrow><annotation encoding="application/x-tex">k=300</annotation></semantics></math>
but for simplicity this is set to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>50</mn></mrow><annotation encoding="application/x-tex">k=50</annotation></semantics></math>
in this example.</p>
<p>Because an analysis using the Impostors Method can have long run
times, this function can also be parallelised using more than one
core.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/impostors.html">impostors</a></span><span class="op">(</span><span class="va">validation.Q</span>, <span class="va">validation.K</span>, <span class="va">validation.K</span>, algorithm <span class="op">=</span> <span class="st">"RBI"</span>, k <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></span></code></pre></div>
<p>The output of <code><a href="../reference/impostors.html">impostors()</a></code> is a data frame showing the
results of comparing each
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
author with each
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
text. The variable <em>target</em> is TRUE if the comparison is a
same-author one or FALSE if it is a different-author one. The variable
<em>score</em> contains the Impostors score, which is a value that
ranges from 0 to 1. Other authorship analysis functions return the same
data frame type with the same columns. The variable <em>score</em>
therefore represents different quantities depending on the analysis
function used (e.g. for <code><a href="../reference/delta.html">delta()</a></code>, this is the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Δ</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math>
coefficient, and so on).</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">res</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span>,<span class="op">]</span></span>
<span><span class="co">#&gt;     K                       Q target score</span></span>
<span><span class="co">#&gt; 1  Kw unknown [Kh Mail_2].txt  FALSE 0.404</span></span>
<span><span class="co">#&gt; 2  Kw unknown [Lc Mail_1].txt  FALSE 0.243</span></span>
<span><span class="co">#&gt; 3  Kw unknown [Ld Mail_4].txt  FALSE 0.752</span></span>
<span><span class="co">#&gt; 4  Kw unknown [Lt Mail_2].txt  FALSE 0.215</span></span>
<span><span class="co">#&gt; 5  Kw unknown [Lk Mail_4].txt  FALSE 0.306</span></span>
<span><span class="co">#&gt; 6  Kw unknown [Lb Mail_3].txt  FALSE 0.975</span></span>
<span><span class="co">#&gt; 7  Kw unknown [La Mail_3].txt  FALSE 0.262</span></span>
<span><span class="co">#&gt; 8  Kw unknown [Mf Mail_1].txt  FALSE 0.933</span></span>
<span><span class="co">#&gt; 9  Kw unknown [Ml Mail_3].txt  FALSE 0.846</span></span>
<span><span class="co">#&gt; 10 Kh unknown [Kh Mail_2].txt   TRUE 0.555</span></span></code></pre></div>
<p>In order to assess the results of this validation analysis, the
function <code><a href="../reference/performance.html">performance()</a></code> can be used to return a series of
performance metrics. This function can take one or two result data
frames as input. If two are provided, then one is used as training and
the other one as test. If only one data frame is provided, then the
performance metrics are calculated using a leave-one-out approach.</p>
<p>The procedure followed by this function is to held out one text (if
leave-one-out, otherwise the test dataset in its entirety) and then use
the rest of the data (or the training dataset) as a calibration dataset
to calculate a <em>Log-Likelihood Ratio</em>
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>L</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">LLR</annotation></semantics></math>).
This analysis is done using the <code><a href="../reference/calibrate_LLR.html">calibrate_LLR()</a></code> function,
which fits a logistic regression model to calibrate the score into a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>L</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">LLR</annotation></semantics></math><span class="citation">Ishihara (2021)</span> using the <code>ROC</code>
library <span class="citation">(Leeuwen 2015)</span>.</p>
<p>The output of the function is the following</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/performance.html">performance</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="co">#&gt;   |                                                                              |                                                                      |   0%  |                                                                              |=                                                                     |   1%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  11%  |                                                                              |=========                                                             |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  15%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |=============                                                         |  19%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |=================                                                     |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  27%  |                                                                              |====================                                                  |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  31%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |========================                                              |  35%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |============================                                          |  40%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  44%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  48%  |                                                                              |===================================                                   |  49%  |                                                                              |===================================                                   |  51%  |                                                                              |====================================                                  |  52%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |=======================================                               |  56%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  65%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  69%  |                                                                              |=================================================                     |  70%  |                                                                              |==================================================                    |  71%  |                                                                              |==================================================                    |  72%  |                                                                              |===================================================                   |  73%  |                                                                              |====================================================                  |  74%  |                                                                              |=====================================================                 |  75%  |                                                                              |=====================================================                 |  76%  |                                                                              |======================================================                |  78%  |                                                                              |=======================================================               |  79%  |                                                                              |========================================================              |  80%  |                                                                              |=========================================================             |  81%  |                                                                              |=========================================================             |  82%  |                                                                              |==========================================================            |  83%  |                                                                              |===========================================================           |  84%  |                                                                              |============================================================          |  85%  |                                                                              |=============================================================         |  87%  |                                                                              |=============================================================         |  88%  |                                                                              |==============================================================        |  89%  |                                                                              |===============================================================       |  90%  |                                                                              |================================================================      |  91%  |                                                                              |================================================================      |  92%  |                                                                              |=================================================================     |  93%  |                                                                              |==================================================================    |  94%  |                                                                              |===================================================================   |  96%  |                                                                              |====================================================================  |  97%  |                                                                              |====================================================================  |  98%  |                                                                              |===================================================================== |  99%  |                                                                              |======================================================================| 100%</span></span>
<span><span class="va">p</span><span class="op">$</span><span class="va">evaluation</span></span>
<span><span class="co">#&gt;        Cllr  Cllr_min      EER Mean TRUE LLR Mean FALSE LLR TRUE trials</span></span>
<span><span class="co">#&gt; 1 0.8043863 0.6155326 19.04762      0.413398     -0.3864078          11</span></span>
<span><span class="co">#&gt;   FALSE trials      AUC Balanced Accuracy Precision    Recall        F1 TP FN</span></span>
<span><span class="co">#&gt; 1           83 0.829904          0.845679 0.3333333 0.8888889 0.4848485  8  1</span></span>
<span><span class="co">#&gt;   FP TN</span></span>
<span><span class="co">#&gt; 1 16 65</span></span></code></pre></div>
<p>The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>C</mi><mrow><mi>l</mi><mi>l</mi><mi>r</mi></mrow></msub><annotation encoding="application/x-tex">C_{llr}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>C</mi><mrow><mi>l</mi><mi>l</mi><mi>r</mi></mrow><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msubsup><annotation encoding="application/x-tex">C_{llr}^{min}</annotation></semantics></math>
coefficients are used to evaluate the performance of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>L</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">LLR</annotation></semantics></math><span class="citation">(Ramos et al. 2013)</span>. These coefficients
estimate the cost of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>L</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">LLR</annotation></semantics></math>,
where a value of 1 indicates no information in the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>L</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">LLR</annotation></semantics></math>
and a lower coefficient
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>l</mi><mi>l</mi><mi>r</mi></mrow></msub><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">C_{llr}&lt;1</annotation></semantics></math>
suggests that there is information in the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>L</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">LLR</annotation></semantics></math>,
with lower values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>C</mi><mrow><mi>l</mi><mi>l</mi><mi>r</mi></mrow></msub><annotation encoding="application/x-tex">C_{llr}</annotation></semantics></math>
suggesting better performance. The other binary classification metrics
returned, such as Precision, Recall, and F1, are all calculated using
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>L</mi><mi>R</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">LLR &gt; 0</annotation></semantics></math>
as the threshold for a TRUE (or same-author in this case)
classification.</p>
<p>In the present example, a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>l</mi><mi>l</mi><mi>r</mi></mrow></msub><mo>=</mo></mrow><annotation encoding="application/x-tex">C_{llr}=</annotation></semantics></math>
0.804 suggests that there is enough information in the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>L</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">LLR</annotation></semantics></math>
to be able to proceed with the actual forensic analysis. The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>C</mi><mrow><mi>l</mi><mi>l</mi><mi>r</mi></mrow><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msubsup><annotation encoding="application/x-tex">C_{llr}^{min}</annotation></semantics></math>,
which is the component of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>C</mi><mrow><mi>l</mi><mi>l</mi><mi>r</mi></mrow></msub><annotation encoding="application/x-tex">C_{llr}</annotation></semantics></math>
measuring the amount of discrimination, is even lower, which means that
there is a substantial difference in the two distributions. This is
confirmed by the Area Under the Curve value of 0.83. Because of the
large disparity between the TRUE and FALSE test cases, the values of
Precision and F1 are misleading. The Balanced Accuracy value of 0.846,
however, again suggests a substantial amount of discrimination at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>L</mi><mi>R</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">LLR=0</annotation></semantics></math>.</p>
<p>The results of the analysis can also be plotted using a density plot
for each of the two distributions, TRUE and FALSE. This can be done
using the <code><a href="../reference/density_plot.html">density_plot()</a></code> function</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/density_plot.html">density_plot</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span></code></pre></div>
<p><img src="idiolect_files/figure-html/unnamed-chunk-15-1.png" width="660"></p>
<p>This plot shows the values of the score on the horizontal axis and
the density for TRUE (red) vs. FALSE (blue) on the vertical axis.</p>
<p>These findings are evidence that the method is validated for this
dataset and it is now possible to analyse the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
text and use these results to calibrate the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>L</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">LLR</annotation></semantics></math>
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>.</p>
</div>
</div>
<div class="section level2">
<h2 id="analysis-of-q">Analysis of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math><a class="anchor" aria-label="anchor" href="#analysis-of-q"></a>
</h2>
<p>At this point the only thing left to do is to analyse the forensic
data by feeding the real
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>
into the <code><a href="../reference/impostors.html">impostors()</a></code> function using the same settings used
for the validation.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">q.res</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/impostors.html">impostors</a></span><span class="op">(</span><span class="va">Q</span>, <span class="va">K</span>, <span class="va">R</span>, algorithm <span class="op">=</span> <span class="st">"RBI"</span>, k <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></span></code></pre></div>
<p>Because there is only one
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
text, the final table of results only contains one row</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">q.res</span></span>
<span><span class="co">#&gt;    K                       Q target score</span></span>
<span><span class="co">#&gt; 1 Kw unknown [Kw Mail_3].txt   TRUE 0.975</span></span></code></pre></div>
<div class="section level3">
<h3 id="qualitative-examination-of-evidence">Qualitative examination of evidence<a class="anchor" aria-label="anchor" href="#qualitative-examination-of-evidence"></a>
</h3>
<p>Before reaching the conclusions, it is often important to inspect the
features that the algorithm has considered for the analysis. In a
forensic analysis, good knowledge of the data is important and best
practice require the analyst to be very familiar with the dataset before
running a computational analysis. Reading the data and being familiar
with it can lead to the addition of more pre-processing steps to remove
noise and can help the analyst spot any problem mistakenly introduced by
the algorithm.</p>
<p>In addition to familiarise themselves with the data,
<code>idiolect</code> allows the analyst to explore the most important
feature considered by the authorship analysis method used. For example,
when using the RBI algorithm with <code><a href="../reference/impostors.html">impostors()</a></code> then the
parameter <em>features</em> can be switched to TRUE to obtain a list of
important features. Running this again with this parameter switched on
produces the following results</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">q.res2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/impostors.html">impostors</a></span><span class="op">(</span><span class="va">Q</span>, <span class="va">K</span>, <span class="va">R</span>, algorithm <span class="op">=</span> <span class="st">"RBI"</span>, k <span class="op">=</span> <span class="fl">50</span>, features <span class="op">=</span> <span class="cn">T</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/strwrap.html" class="external-link">strwrap</a></span><span class="op">(</span><span class="va">q.res2</span><span class="op">$</span><span class="va">features</span>, width <span class="op">=</span> <span class="fl">70</span><span class="op">)</span></span>
<span><span class="co">#&gt;  [1] ", her|lso ,|so , |P P f|, i w|N N .| too |our P|ur P |also |re is|i"  </span></span>
<span><span class="co">#&gt;  [2] "jus|ll me|l me |ere i|st wa|P , h|u hav| yet | , he|ou ha|ust w|you"  </span></span>
<span><span class="co">#&gt;  [3] "h| also|rom P|om P |ve no|u and|he J |N N N|the J|V our| here|ve a"   </span></span>
<span><span class="co">#&gt;  [4] "|ith y|re al|few N|ew N | so i|ou an|e a N|P P i|P on |th yo| V my|V" </span></span>
<span><span class="co">#&gt;  [5] "my |e N w| , so|me to| few |eithe|ither| him |V it | P on|out w|at i" </span></span>
<span><span class="co">#&gt;  [6] "|t kno| may |P P V| N fr|u to |N fro|e is |V wit| V it|P for|, i h|"  </span></span>
<span><span class="co">#&gt;  [7] "is o|u nee|ou to| V wi|s for| P fo|a N a|do no|h you| P in|P in | B ,"</span></span>
<span><span class="co">#&gt;  [8] "| J N | me t| 'll |you n|ou ne|o not|, N a|t to |n N .|N in |o be |ke"</span></span>
<span><span class="co">#&gt;  [9] "a |n P ,| N or|N or | look|t wan|ave a| it .|ave n|o V i|r J N|e an"  </span></span>
<span><span class="co">#&gt; [10] "|as i |P D .|e tha|is J |o V w|r N w| , i |N who|t V a|e N s|se le| N"</span></span>
<span><span class="co">#&gt; [11] "on|nd th|s you|ng th| is a|N N w|ase l| one | can |e hav| 's N|'s N |"</span></span>
<span><span class="co">#&gt; [12] "i wo|i was|in V |e let| at t|are a| our |t the|N on |i hav|is V | as" </span></span>
<span><span class="co">#&gt; [13] "i|e J N|you t| to m|P 's |P P o|ake a|e thi|s N N| a N |in a |n you|" </span></span>
<span><span class="co">#&gt; [14] "you |hat P|o see| week|week |r N o|nt to| N if|N if |or V |N for|ut i"</span></span>
<span><span class="co">#&gt; [15] "| N J |the N|he N | into|into |o hav| P 's|hat y|n our|if yo| N fo|to"</span></span>
<span><span class="co">#&gt; [16] "do|o do | is J|P P .| my N| N in|me N |t you| i ha| do n| get |an V"  </span></span>
<span><span class="co">#&gt; [17] "|e N N|V tha| i am|e P P|you N|ant t| on P| is V| i V |ome N|ore"     </span></span>
<span><span class="co">#&gt; [18] "N|ave b|other|with | a J |at yo| N i | we w| the |did n|thing|hing |e"</span></span>
<span><span class="co">#&gt; [19] "wil| J in|J in |on P |to me|o me |s the|ed to|or P |ve V |my N |to"   </span></span>
<span><span class="co">#&gt; [20] "se| in a| but | , th| P , |t is | see |i wil|this |ou N |V thi|or N |"</span></span>
<span><span class="co">#&gt; [21] "have|is a |N has|J to |have |i V t|o V o| V a | V fo|ther |e V i|i am"</span></span>
<span><span class="co">#&gt; [22] "| J fo|J for|is N |a J N|V and|V on |as V |V for|N , h|can V|J N"     </span></span>
<span><span class="co">#&gt; [23] "N|his N| J , |J N .| N th|f you|some | N wi| V an| with| be J|be J |d"</span></span>
<span><span class="co">#&gt; [24] "the|V to |V in |but i| star|start|ng on|g on |a N t| N wh| just|r"    </span></span>
<span><span class="co">#&gt; [25] "you| V to|re N | this| in P|P to |ow if|w if |e you| for |to be| V N" </span></span>
<span><span class="co">#&gt; [26] "|P P P|and t|in P | work|our J|ur J | to b| P to| and |e als|r N i|ou"</span></span>
<span><span class="co">#&gt; [27] "wo|i 'll|ch of|o V .|ach o|r D N|ase V|se V |h of |o mak|e and|to"    </span></span>
<span><span class="co">#&gt; [28] "ma|nning|e V y|o the|ill V|and g|y N N|o V u|P tha|look | out"        </span></span>
<span><span class="co">#&gt; [29] "|ether|ve an| it t|t i w|ere w|o V y|ning |o J N| by N|e nee|t V i|," </span></span>
<span><span class="co">#&gt; [30] "as |e to | if y|by N | we n|P , a| give| N P | , as|N we |just |e"    </span></span>
<span><span class="co">#&gt; [31] "any|V you|we wo| N V | each|our N|ur N |each |ill h|pleas|lease|ease" </span></span>
<span><span class="co">#&gt; [32] "|you a|e wou|e in |for D|here | she |needs|eeds | P N |tart |d you|"  </span></span>
<span><span class="co">#&gt; [33] "othe|for N| plea| both|your |ll V |eek .|make | , we|, we | V yo|or D"</span></span>
<span><span class="co">#&gt; [34] "|r N a|both |V any|a N f|worki|orkin|rking|N V N|re J | P V |n the|we"</span></span>
<span><span class="co">#&gt; [35] "ne|o V N|N N a|more | more|s to |ld li| afte| V on|s tha| P D | J"    </span></span>
<span><span class="co">#&gt; [36] "to|in th|any N|ny N |P , w| me a|fter |V the|e V a|be V | be V| who |"</span></span>
<span><span class="co">#&gt; [37] "in t|give | , yo|, you| need|is th| call| did |at we| what|d to |V a" </span></span>
<span><span class="co">#&gt; [38] "N| any |d lik|after|at P |r N .| has | N we|her N| N , |s a N|ing a|N"</span></span>
<span><span class="co">#&gt; [39] "is | of N|of N |ou ar|u are| V in|would|P P a|ing o|of yo| two |you"  </span></span>
<span><span class="co">#&gt; [40] "w|N N i| your|r N N|t thi|we ar| N is|N N ,| P is| is t|y N o| we V|" </span></span>
<span><span class="co">#&gt; [41] "a fe|a few|ke to|i wou|r our|uld l|th N |ike t|V P P|e J t|like |"    </span></span>
<span><span class="co">#&gt; [42] "like|e if |ow wh| woul| N by|N by |e V f|ee if|for h|y hav|, ple| ,"  </span></span>
<span><span class="co">#&gt; [43] "pl| a D |V N o| find|find |now w|hat h|ted t|you m|i nee|N , s|th P |"</span></span>
<span><span class="co">#&gt; [44] ", bu|, but|e V t|lso V|so V |ith P|eed t| to V|o V t|to V |to th|ill" </span></span>
<span><span class="co">#&gt; [45] "n| on t|ould | make| let |P and| per |f the|d P P| N yo|you s|V up |" </span></span>
<span><span class="co">#&gt; [46] "P ha|and h|s N w| P an|N you|or th|e N o|not V|V J N|nd N |ot V | to" </span></span>
<span><span class="co">#&gt; [47] "s|for t|er N |e bee|ve be|see i|and N| will|are V|n to |ne of|them |" </span></span>
<span><span class="co">#&gt; [48] "them|N tha|one o|me of|id no|d not|f N .|t wit|N so |o V a|to ta| N"  </span></span>
<span><span class="co">#&gt; [49] "so|e J .|e of |ome o|king |m to |l hav| i wa| of y|ll ha|we ha|a N"   </span></span>
<span><span class="co">#&gt; [50] "o|u V t|need | we '|, P ,| N ar|N are| was | to g|hat N| from|r the|" </span></span>
<span><span class="co">#&gt; [51] "N B |from | N N |it is|take |to P |at th| to P|e N .|e V .|he P |the" </span></span>
<span><span class="co">#&gt; [52] "P| V th"</span></span></code></pre></div>
<p>The RBI method uses as features character 4-grams and a list of these
features is clearly hard to interpret by a human analyst. Despite the
complexity, this is not an impossible task. <code>idiolect</code> offers
a function to aid exploration called <code><a href="../reference/concordance.html">concordance()</a></code>, which
uses <code>quanteda</code>’s <code><a href="https://quanteda.io/reference/kwic.html" class="external-link">kwic()</a></code> as its engine.</p>
<p><code><a href="../reference/concordance.html">concordance()</a></code> takes as input a string representing one
or more words (or punctuation marks). For example, the most important
character 4-gram seems to be &lt;<em>, her</em>&gt; so this could be the
search target.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/concordance.html">concordance</a></span><span class="op">(</span><span class="va">Q</span>, <span class="va">K</span>, <span class="va">R</span>, search <span class="op">=</span> <span class="st">", her"</span>, token.type <span class="op">=</span> <span class="st">"character"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="va">pre</span>, <span class="va">node</span>, <span class="va">post</span>, <span class="va">authorship</span><span class="op">)</span></span>
<span><span class="co">#&gt;      pre  node  post authorship</span></span>
<span><span class="co">#&gt; 1   . P  , her e is           Q</span></span>
<span><span class="co">#&gt; 2  we V  , her e is           Q</span></span>
<span><span class="co">#&gt; 3   . P  , her e is           Q</span></span>
<span><span class="co">#&gt; 4  ur N  , her e is           K</span></span>
<span><span class="co">#&gt; 5   . P  , her e is           K</span></span>
<span><span class="co">#&gt; 6   . P  , her e is           K</span></span>
<span><span class="co">#&gt; 7  nd P  , her e is           K</span></span>
<span><span class="co">#&gt; 8   . P  , her e is           K</span></span>
<span><span class="co">#&gt; 9   N N  , her e is           K</span></span>
<span><span class="co">#&gt; 10 st P  , her e is           K</span></span>
<span><span class="co">#&gt; 11  N N  , her e is           K</span></span>
<span><span class="co">#&gt; 12    P  , her e 's   Reference</span></span>
<span><span class="co">#&gt; 13  P P  , her e out  Reference</span></span>
<span><span class="co">#&gt; 14 nd P  , her e is   Reference</span></span>
<span><span class="co">#&gt; 15  . P  , her e are  Reference</span></span>
<span><span class="co">#&gt; 16  . P  , her e are  Reference</span></span></code></pre></div>
<p>The search reveals that this character sequence is a strong
characteristic of the candidate author’s writing. However, the real
underlying pattern is not the use of a comma followed by the possessive
determiner <em>her</em> but the token sequence [<em>, here is</em>],
which is only used by the candidate author and one other author in the
reference corpus.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/concordance.html">concordance</a></span><span class="op">(</span><span class="va">Q</span>, <span class="va">K</span>, <span class="va">R</span>, search <span class="op">=</span> <span class="st">", here is"</span>, token.type <span class="op">=</span> <span class="st">"word"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="va">pre</span>, <span class="va">node</span>, <span class="va">post</span>, <span class="va">authorship</span><span class="op">)</span></span>
<span><span class="co">#&gt;                 pre      node                   post authorship</span></span>
<span><span class="co">#&gt; 1   each of you . P , here is           the P P P on          Q</span></span>
<span><span class="co">#&gt; 2    on this N we V , here is          a N N of both          Q</span></span>
<span><span class="co">#&gt; 3       N for P . P , here is            the P P P P          Q</span></span>
<span><span class="co">#&gt; 4  all . per your N , here is          some J N on P          K</span></span>
<span><span class="co">#&gt; 5       B and V . P , here is           the J N N on          K</span></span>
<span><span class="co">#&gt; 6         N V N . P , here is            the P P P P          K</span></span>
<span><span class="co">#&gt; 7       N . P and P , here is        the J for the P          K</span></span>
<span><span class="co">#&gt; 8       N N yet . P , here is            the P P P P          K</span></span>
<span><span class="co">#&gt; 9    out of the N N , here is the beginning of the N          K</span></span>
<span><span class="co">#&gt; 10 N on this past P , here is  the N we talked about          K</span></span>
<span><span class="co">#&gt; 11   as per our N N , here is            the V P P P          K</span></span>
<span><span class="co">#&gt; 12   know . P and P , here is            a N of N in  Reference</span></span></code></pre></div>
<p>Another important feature is represented by the two character
4-grams, &lt;<em>lso ,</em>&gt; and &lt;<em>so ,</em> &gt;, which are
likely to refer to the token sequence [<em>also ,</em>].</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/concordance.html">concordance</a></span><span class="op">(</span><span class="va">Q</span>, <span class="va">K</span>, <span class="va">R</span>, search <span class="op">=</span> <span class="st">"lso ,"</span>, token.type <span class="op">=</span> <span class="st">"character"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="va">pre</span>, <span class="va">node</span>, <span class="va">post</span>, <span class="va">authorship</span><span class="op">)</span></span>
<span><span class="co">#&gt;      pre  node  post authorship</span></span>
<span><span class="co">#&gt; 1  N ? a lso ,  ther          Q</span></span>
<span><span class="co">#&gt; 2  V . a lso ,  i ha          Q</span></span>
<span><span class="co">#&gt; 3  P . a lso ,  plea          Q</span></span>
<span><span class="co">#&gt; 4  k . a lso ,  P ha          Q</span></span>
<span><span class="co">#&gt; 5  N . a lso ,  P P           K</span></span>
<span><span class="co">#&gt; 6  N . a lso ,  at t          K</span></span>
<span><span class="co">#&gt; 7  N ? a lso ,  i 'v          K</span></span>
<span><span class="co">#&gt; 8  s . a lso ,  let           K</span></span>
<span><span class="co">#&gt; 9      a lso ,  V th          K</span></span>
<span><span class="co">#&gt; 10 N . a lso ,  i V           K</span></span>
<span><span class="co">#&gt; 11 N . a lso ,  woul  Reference</span></span>
<span><span class="co">#&gt; 12 . P a lso ,  V to  Reference</span></span>
<span><span class="co">#&gt; 13 N . a lso ,  any   Reference</span></span>
<span><span class="co">#&gt; 14 N . a lso ,  have  Reference</span></span>
<span><span class="co">#&gt; 15 P . a lso ,  P P   Reference</span></span>
<span><span class="co">#&gt; 16 N . a lso ,  you   Reference</span></span>
<span><span class="co">#&gt; 17 D . a lso ,  to V  Reference</span></span>
<span><span class="co">#&gt; 18 N . a lso ,  tell  Reference</span></span>
<span><span class="co">#&gt; 19 J . a lso ,  coul  Reference</span></span>
<span><span class="co">#&gt; 20 N . a lso ,  we n  Reference</span></span>
<span><span class="co">#&gt; 21 J . a lso ,  ther  Reference</span></span></code></pre></div>
<p>This is correct and it is referring to the use of <em>also</em> at
the beginning of a sentence and immediately followed by a comma.</p>
<p>Although searching all the features returned is clearly a significant
amount of work, by inspecting the list of features carefully and by
using <code><a href="../reference/concordance.html">concordance()</a></code> to explore the features in the data the
analyst can spot patterns or mistakes in the analysis <span class="citation">(Ypma, Ramos, and Meuwly 2023)</span>.</p>
</div>
<div class="section level3">
<h3 id="conclusions">Conclusions<a class="anchor" aria-label="anchor" href="#conclusions"></a>
</h3>
<p>Although the score assigned to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
is high, depending on the calibration data, it can correspond to various
magnitudes of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>L</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">LLR</annotation></semantics></math>.</p>
<p>The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>L</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">LLR</annotation></semantics></math>
value for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
can also be plotted onto the TRUE vs. FALSE distributions using the
second argument of <code><a href="../reference/density_plot.html">density_plot()</a></code>. The <em>q</em> argument
can be used to draw a black vertical line that crosses the two
distributions at the horizontal axis corresponding to the score of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/density_plot.html">density_plot</a></span><span class="op">(</span><span class="va">res</span>, q <span class="op">=</span> <span class="va">q.res</span><span class="op">$</span><span class="va">score</span><span class="op">)</span></span></code></pre></div>
<p><img src="idiolect_files/figure-html/unnamed-chunk-24-1.png" width="660"></p>
<p>To perform this calibration the <code><a href="../reference/calibrate_LLR.html">calibrate_LLR()</a></code> function
is used again by using the validation results as calibration data</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">q.llr</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/calibrate_LLR.html">calibrate_LLR</a></span><span class="op">(</span><span class="va">res</span>, <span class="va">q.res</span>, latex <span class="op">=</span> <span class="cn">T</span><span class="op">)</span></span>
<span><span class="va">q.llr</span><span class="op">$</span><span class="va">`Verbal label`</span></span>
<span><span class="co">#&gt; [1] "Moderate support for $H_p$"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/strwrap.html" class="external-link">strwrap</a></span><span class="op">(</span><span class="va">q.llr</span><span class="op">$</span><span class="va">Interpretation</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "The similarity is 27.04 times more likely to be observed in the case of"</span></span>
<span><span class="co">#&gt; [2] "$H_p$ than in the case of $H_d$"</span></span></code></pre></div>
<p>This function not only returns the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>L</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">LLR</annotation></semantics></math>
value but also the verbal labels and their interpretation <span class="citation">(Marquis et al. 2016)</span>.</p>
<p>The final conclusion of the analysis is therefore the following:</p>
<blockquote>
<p>The similarity score of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
is 0.975, which corresponds to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>L</mi><mi>R</mi><mo>=</mo></mrow><annotation encoding="application/x-tex">LLR=</annotation></semantics></math>
1.432. The similarity is 27.04 times more likely to be observed in the
case of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mi>p</mi></msub><annotation encoding="application/x-tex">H_p</annotation></semantics></math>
than in the case of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mi>d</mi></msub><annotation encoding="application/x-tex">H_d</annotation></semantics></math>.
Therefore, the linguistic analysis offers <strong>Moderate support for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mi>p</mi></msub><annotation encoding="application/x-tex">H_p</annotation></semantics></math></strong>.</p>
</blockquote>
<p>This conclusion can be complemented with an explanation of the
implication of these results for the trier of facts by showing a table
of posterior probabilities assuming a range of prior probabilities. This
can be done with the <code><a href="../reference/posterior.html">posterior()</a></code> function by inserting as
input the value of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>L</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">LLR</annotation></semantics></math></p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/posterior.html">posterior</a></span><span class="op">(</span><span class="va">q.llr</span><span class="op">$</span><span class="va">LLR</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="va">prosecution_prior_probs</span>, <span class="va">prosecution_post_probs</span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 11 × 2</span></span></span>
<span><span class="co">#&gt;    prosecution_prior_probs prosecution_post_probs</span></span>
<span><span class="co">#&gt;                      <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>                  <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span>                0.000<span style="text-decoration: underline;">001</span>              0.000<span style="text-decoration: underline;">027</span>0</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span>                0.01                  0.215    </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span>                0.1                   0.750    </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span>                0.2                   0.871    </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span>                0.3                   0.921    </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span>                0.4                   0.947    </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span>                0.5                   0.964    </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span>                0.6                   0.976    </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span>                0.7                   0.984    </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span>                0.8                   0.991    </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">11</span>                0.9                   0.996</span></span></code></pre></div>
<p>The table above reveals that, assuming a prior probability for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mi>p</mi></msub><annotation encoding="application/x-tex">H_p</annotation></semantics></math>
of 0.00001 (roughly, one out of the population of Manchester), then this
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>L</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">LLR</annotation></semantics></math>
would transform this probability to a posterior probability for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mi>p</mi></msub><annotation encoding="application/x-tex">H_p</annotation></semantics></math>
of 0.000027. In other words, it would not make much substantial
difference for the trial.</p>
<p>However, if the prior probability of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mi>p</mi></msub><annotation encoding="application/x-tex">H_p</annotation></semantics></math>
was 0.5, then these results would turn it to 0.96, which is a
substantial change.</p>
<p>The table shows that the present evidence could change the
probability that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mi>p</mi></msub><annotation encoding="application/x-tex">H_p</annotation></semantics></math>
is true to equal or higher than 0.9 only with a prior greater than
0.2.</p>
</div>
</div>
<div class="section level2">
<h2 id="acknowledgements">Acknowledgements<a class="anchor" aria-label="anchor" href="#acknowledgements"></a>
</h2>
<p>I would like to thank Shunichi Ishihara and Marie Bojsen-Møller for
helpful comments on the first draft of this vignette.</p>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-bischoff" class="csl-entry">
Bischoff, Sebastian, Niklas Deckers, Marcel Schliebs, Ben Thies,
Matthias Hagen, Efstathios Stamatatos, Benno Stein, and Martin Potthast.
n.d. <span>“The Importance of Suppressing Domain Style in Authorship
Analysis.”</span>
</div>
<div id="ref-halvani2021" class="csl-entry">
Halvani, Oren, and Lukas Graner. 2021. <span>“16th International
Conference on Availability, Reliability and Security.”</span> In, 1–12.
Vienna, Austria: Association for Computing Machinery. <a href="https://doi.org/10.1145/3465481.3470050" class="external-link">https://doi.org/10.1145/3465481.3470050</a>.
</div>
<div id="ref-ishihara2021" class="csl-entry">
Ishihara, Shunichi. 2021. <span>“Score-Based Likelihood Ratios for
Linguistic Text Evidence with a Bag-of-Words Model.”</span> <em>Forensic
Science International</em> 327: 110980. <a href="https://doi.org/10.1016/j.forsciint.2021.110980" class="external-link">https://doi.org/10.1016/j.forsciint.2021.110980</a>.
</div>
<div id="ref-ishihara2024" class="csl-entry">
Ishihara, Shunichi, Sonia Kulkarni, Michael Carne, Sabine Ehrhardt, and
Andrea Nini. 2024. <span>“Validation in Forensic Text Comparison: Issues
and Opportunities.”</span> <em>Languages</em> 9 (2): 47. <a href="https://doi.org/10.3390/languages9020047" class="external-link">https://doi.org/10.3390/languages9020047</a>.
</div>
<div id="ref-koppel2014" class="csl-entry">
Koppel, Moshe, and Yaron Winter. 2014. <span>“Determining If Two
Documents Are Written by the Same Author.”</span> <em>Journal of the
Association for Information Science and Technology</em> 65 (1): 178–87.
</div>
<div id="ref-vanleeuwen2015" class="csl-entry">
Leeuwen, David A. van. 2015. <span>“ROC: Compute Structures to Compute
ROC and DET Plots and Metrics for 2-Class Classifiers.”</span> <a href="https://rdrr.io/github/davidavdav/ROC/" class="external-link">https://rdrr.io/github/davidavdav/ROC/</a>.
</div>
<div id="ref-marquis2016" class="csl-entry">
Marquis, Raymond, Alex Biedermann, Liv Cadola, Christophe Champod, Line
Gueissaz, Geneviève Massonnet, Williams David Mazzella, Franco Taroni,
and Tacha Hicks. 2016. <span>“Discussion on How to Implement a Verbal
Scale in a Forensic Laboratory: Benefits, Pitfalls and Suggestions to
Avoid Misunderstandings.”</span> <em>Science &amp; Justice</em> 56 (5):
364–70. <a href="https://doi.org/10.1016/j.scijus.2016.05.009" class="external-link">https://doi.org/10.1016/j.scijus.2016.05.009</a>.
</div>
<div id="ref-morrison2013" class="csl-entry">
Morrison, Geoffrey Stewart. 2013. <span>“Tutorial on Logistic-Regression
Calibration and Fusion:converting a Score to a Likelihood Ratio.”</span>
<em>Australian Journal of Forensic Sciences</em> 45 (2): 173–97. <a href="https://doi.org/10.1080/00450618.2012.733025" class="external-link">https://doi.org/10.1080/00450618.2012.733025</a>.
</div>
<div id="ref-nini2023" class="csl-entry">
Nini, Andrea. 2023. <em>A Theory of Linguistic Individuality for
Authorship Analysis</em>. Elements in Forensic Linguistics. Cambridge,
UK: Cambridge University Press.
</div>
<div id="ref-potha2017" class="csl-entry">
Potha, Nektaria, and Efstathios Stamatatos. 2017. <span>“An Improved
Impostors Method for Authorship Verification.”</span> In, edited by
Gareth J. F. Jones, Séamus Lawless, Julio Gonzalo, Liadh Kelly, Lorraine
Goeuriot, Thomas Mandl, Linda Cappellato, and Nicola Ferro,
10456:138–44. Lecture Notes in Computer Science. Springer, Cham. <a href="https://doi.org/10.1007/978-3-319-65813-1_14" class="external-link">https://doi.org/10.1007/978-3-319-65813-1_14</a>.
</div>
<div id="ref-potha2020" class="csl-entry">
———. 2020. <span>“Improved Algorithms for Extrinsic Author
Verification.”</span> <em>Knowledge and Information Systems</em> 62 (5):
1903–21. <a href="https://doi.org/10.1007/s10115-019-01408-4" class="external-link">https://doi.org/10.1007/s10115-019-01408-4</a>.
</div>
<div id="ref-ramos2013" class="csl-entry">
Ramos, Daniel, Joaquin Gonzalez-Rodriguez, Grzegorz Zadora, and Colin
Aitken. 2013. <span>“Information-Theoretical Assessment of the
Performance of Likelihood Ratio Computation Methods.”</span> <em>Journal
of Forensic Sciences</em> 58 (6): 1503–18. <a href="https://doi.org/10.1111/1556-4029.12233" class="external-link">https://doi.org/10.1111/1556-4029.12233</a>.
</div>
<div id="ref-stamatatos2017" class="csl-entry">
Stamatatos, Efstathios. 2017. <span>“Authorship <span>Attribution Using
Text Distortion</span>.”</span> In <em>Proceedings of the 15th
<span>Conference</span> of the <span>European Chapter</span> of the
<span>Association</span> for <span>Computational Linguistics</span>:
<span>Volume</span> 1, <span>Long Papers</span></em>, 1138–49. Valencia,
Spain: Association for Computational Linguistics.
</div>
<div id="ref-ypma2023" class="csl-entry">
Ypma, Rolf J. F., Daniel Ramos, and Didier Meuwly. 2023. <span>“AI-Based
Forensic Evaluation in Court: The Desirability of Explanation and the
Necessity of Validation.”</span> In, edited by Zeno Geradts and Katrin
Franke, 3–17. Forensic Science in Focus. Hoboken, NJ: Wiley.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Andrea Nini.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
