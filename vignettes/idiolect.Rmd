---
title: "idiolect"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{idiolect}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(idiolect)
```

| `idiolect` is a package that depends on `quanteda` for all the main Natural Language Processing functions. Although the basic object types and functions are described in detail in the documentation of this package, familiarity with `quanteda` is highly recommended. More information about `quanteda` can be found on its [website](http://quanteda.io).

# Introduction

Authorship Analysis is defined as the task of determining the likelihood that a certain candidate is the author of a certain set of questioned or disputed texts. We call *Forensic* Authorship Analysis a task of this kind applied in a real forensic case. In such settings, the disputed texts could be anonymous malicious documents, such as a threatening letter, but could also be text messages, emails, text messages, or any other document that, for various reasons, becomes evidence in a forensic case. In Forensic Linguistics, typically a set of disputed text is indicated as $Q$, while a set of texts of known origin, for example the texts written by the candidate author and collected as comparison material, is labelled using $K$. In addition to these two data sets, the analysis also necessitates of a comparison reference corpus, that we call $R$. In a classic case involving a closed set of suspects, the texts written by the suspects minus the candidate form $R$. In *Authorship Verification* cases that only involve one candidate author, then the reference data set might have to be compiled by the analyst for the specific case.

A crucial difference between Authorship Analysis and Forensic Authorship Analysis is that whereas the former can be treated as a classification task where the final answer is binary ('candidate is the author' vs. 'candidate is NOT the author'), the latter needs an expression of likelihood for the two competing propositions or hypotheses, the Prosecution Hypothesis $H_p$ vs. the Defence Hypothesis ($H_d$):

$H_p$: The author of $K$ is the author of $Q$\
$H_d$: The author of $K$ is NOT the author of $Q$

The job of the forensic linguist in a forensic context is to analyse the linguistic evidence and determine which hypothesis it supports and with what degree of strength. The role of the forensic linguist is therefore not to provide a YES/NO answer but to express a degree of support for one or the other hypothesis.

Given $K$, $Q$ and $R$, the workflow for this analysis involves four steps:

1.  **Preparation**: This step involves any pre-processing step that is necessary for the analysis with the chosen method;
2.  **Validation**: Carry out an analysis on the case data or on a separate data set that has been designed to be similar to the case material in order to validate the method for this particular case;
3.  **Analysis**: Carry out the analysis on the real $K$, $Q$, and $R$;
4.  **Calibration**: Calibrate the output of (2) into a Likelihood Ratio that expresses the degree of support for the competing hypotheses using the output of (1).

# Preparation

This vignette demonstrates the process using a small data set of the Enron corpus (see [enron.sample]) that is included in this package.

```{r}
corpus <- enron.sample
corpus
```

This corpus is a `quanteda` corpus object that contains three authors and three texts per author, as can be seen using `quanteda`'s `docvars()` function

```{r}
docvars(corpus)
```

```{r}
Q <- corpus_subset(corpus, author == "Kimberly.watson" & texttype == "unknown")
K <- corpus_subset(corpus, author == "Kimberly.watson" & texttype == "known")
R <- corpus_subset(corpus, author != "Kimberly.watson")
```

```{r}
validation <- K + R
validation.Q <- corpus_subset(validation, texttype == "unknown")
validation.K <- corpus_subset(validation, texttype == "known")
```

# Validation

```{r}
impostors(validation.Q, validation.K, validation.K, k = 50) -> res
```

```{r}
performance(res)
```

```{r}
a <- density(res[which(res$target == FALSE),"score"])
b <- density(res[which(res$target == TRUE),"score"])

plot(a) 
plot(b)

library(ggplot2)

res |> ggplot() +
  aes(x = score, color = target) +
  geom_density()
```

```{r}
impostors(Q, K, R, k = 100) -> q.res
```

```{r}
calibrate_LLR(res, q.res)
```
