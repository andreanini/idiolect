[{"path":"https://andreanini.github.io/idiolect/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 2, June 1991Copyright © 1989, 1991 Free Software Foundation, Inc.,51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://andreanini.github.io/idiolect/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"licenses software designed take away freedom share change . contrast, GNU General Public License intended guarantee freedom share change free software–make sure software free users. General Public License applies Free Software Foundation’s software program whose authors commit using . (Free Software Foundation software covered GNU Lesser General Public License instead.) can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge service wish), receive source code can get want , can change software use pieces new free programs; know can things. protect rights, need make restrictions forbid anyone deny rights ask surrender rights. restrictions translate certain responsibilities distribute copies software, modify . example, distribute copies program, whether gratis fee, must give recipients rights . must make sure , , receive can get source code. must show terms know rights. protect rights two steps: (1) copyright software, (2) offer license gives legal permission copy, distribute /modify software. Also, author’s protection , want make certain everyone understands warranty free software. software modified someone else passed , want recipients know original, problems introduced others reflect original authors’ reputations. Finally, free program threatened constantly software patents. wish avoid danger redistributors free program individually obtain patent licenses, effect making program proprietary. prevent , made clear patent must licensed everyone’s free use licensed . precise terms conditions copying, distribution modification follow.","code":""},{"path":"https://andreanini.github.io/idiolect/LICENSE.html","id":"terms-and-conditions-for-copying-distribution-and-modification","dir":"","previous_headings":"","what":"TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION","title":"GNU General Public License","text":"0. License applies program work contains notice placed copyright holder saying may distributed terms General Public License. “Program”, , refers program work, “work based Program” means either Program derivative work copyright law: say, work containing Program portion , either verbatim modifications /translated another language. (Hereinafter, translation included without limitation term “modification”.) licensee addressed “”. Activities copying, distribution modification covered License; outside scope. act running Program restricted, output Program covered contents constitute work based Program (independent made running Program). Whether true depends Program . 1. may copy distribute verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice disclaimer warranty; keep intact notices refer License absence warranty; give recipients Program copy License along Program. may charge fee physical act transferring copy, may option offer warranty protection exchange fee. 2. may modify copy copies Program portion , thus forming work based Program, copy distribute modifications work terms Section 1 , provided also meet conditions: ) must cause modified files carry prominent notices stating changed files date change. b) must cause work distribute publish, whole part contains derived Program part thereof, licensed whole charge third parties terms License. c) modified program normally reads commands interactively run, must cause , started running interactive use ordinary way, print display announcement including appropriate copyright notice notice warranty (else, saying provide warranty) users may redistribute program conditions, telling user view copy License. (Exception: Program interactive normally print announcement, work based Program required print announcement.) requirements apply modified work whole. identifiable sections work derived Program, can reasonably considered independent separate works , License, terms, apply sections distribute separate works. distribute sections part whole work based Program, distribution whole must terms License, whose permissions licensees extend entire whole, thus every part regardless wrote . Thus, intent section claim rights contest rights work written entirely ; rather, intent exercise right control distribution derivative collective works based Program. addition, mere aggregation another work based Program Program (work based Program) volume storage distribution medium bring work scope License. 3. may copy distribute Program (work based , Section 2) object code executable form terms Sections 1 2 provided also one following: ) Accompany complete corresponding machine-readable source code, must distributed terms Sections 1 2 medium customarily used software interchange; , b) Accompany written offer, valid least three years, give third party, charge cost physically performing source distribution, complete machine-readable copy corresponding source code, distributed terms Sections 1 2 medium customarily used software interchange; , c) Accompany information received offer distribute corresponding source code. (alternative allowed noncommercial distribution received program object code executable form offer, accord Subsection b .) source code work means preferred form work making modifications . executable work, complete source code means source code modules contains, plus associated interface definition files, plus scripts used control compilation installation executable. However, special exception, source code distributed need include anything normally distributed (either source binary form) major components (compiler, kernel, ) operating system executable runs, unless component accompanies executable. distribution executable object code made offering access copy designated place, offering equivalent access copy source code place counts distribution source code, even though third parties compelled copy source along object code. 4. may copy, modify, sublicense, distribute Program except expressly provided License. attempt otherwise copy, modify, sublicense distribute Program void, automatically terminate rights License. However, parties received copies, rights, License licenses terminated long parties remain full compliance. 5. required accept License, since signed . However, nothing else grants permission modify distribute Program derivative works. actions prohibited law accept License. Therefore, modifying distributing Program (work based Program), indicate acceptance License , terms conditions copying, distributing modifying Program works based . 6. time redistribute Program (work based Program), recipient automatically receives license original licensor copy, distribute modify Program subject terms conditions. may impose restrictions recipients’ exercise rights granted herein. responsible enforcing compliance third parties License. 7. , consequence court judgment allegation patent infringement reason (limited patent issues), conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. distribute satisfy simultaneously obligations License pertinent obligations, consequence may distribute Program . example, patent license permit royalty-free redistribution Program receive copies directly indirectly , way satisfy License refrain entirely distribution Program. portion section held invalid unenforceable particular circumstance, balance section intended apply section whole intended apply circumstances. purpose section induce infringe patents property right claims contest validity claims; section sole purpose protecting integrity free software distribution system, implemented public license practices. Many people made generous contributions wide range software distributed system reliance consistent application system; author/donor decide willing distribute software system licensee impose choice. section intended make thoroughly clear believed consequence rest License. 8. distribution /use Program restricted certain countries either patents copyrighted interfaces, original copyright holder places Program License may add explicit geographical distribution limitation excluding countries, distribution permitted among countries thus excluded. case, License incorporates limitation written body License. 9. Free Software Foundation may publish revised /new versions General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies version number License applies “later version”, option following terms conditions either version later version published Free Software Foundation. Program specify version number License, may choose version ever published Free Software Foundation. 10. wish incorporate parts Program free programs whose distribution conditions different, write author ask permission. software copyrighted Free Software Foundation, write Free Software Foundation; sometimes make exceptions . decision guided two goals preserving free status derivatives free software promoting sharing reuse software generally.","code":""},{"path":"https://andreanini.github.io/idiolect/LICENSE.html","id":"no-warranty","dir":"","previous_headings":"","what":"NO WARRANTY","title":"GNU General Public License","text":"11. PROGRAM LICENSED FREE CHARGE, WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION. 12. EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MAY MODIFY /REDISTRIBUTE PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES. END TERMS CONDITIONS","code":""},{"path":"https://andreanini.github.io/idiolect/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively convey exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program interactive, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, commands use may called something show w show c; even mouse-clicks menu items–whatever suits program. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. sample; alter names: General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA. Gnomovision version 69, Copyright (C) year name of author Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details. Yoyodyne, Inc., hereby disclaims all copyright interest in the program `Gnomovision' (which makes passes at compilers) written by James Hacker.  <signature of Ty Coon>, 1 April 1989 Ty Coon, President of Vice"},{"path":"https://andreanini.github.io/idiolect/articles/idiolect.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"idiolect","text":"Authorship Analysis defined task determining likelihood certain candidate author certain set questioned disputed texts. call Forensic Authorship Analysis task kind applied real forensic case. settings, disputed texts anonymous malicious documents, threatening letter, also text messages, emails, document , various reasons, becomes evidence forensic case. Forensic Linguistics, typically set disputed questioned text indicated QQ, set texts known origin, example texts written candidate author collected comparison material, labelled using KK. addition two datasets, analysis also necessitates comparison reference corpus call RR. classic case involving closed set suspects, texts written suspects minus candidate form RR. Authorship Verification cases involve one candidate author, reference dataset might compiled analyst specific case (Ishihara et al. 2024). crucial difference Authorship Analysis Forensic Authorship Analysis whereas former can treated classification task final answer binary (‘candidate author’ vs. ‘candidate author’), latter needs expression likelihood two competing propositions hypotheses, Prosecution Hypothesis HpH_p vs. Defence Hypothesis HdH_d, example: HpH_p: author KK author QQ individual.HdH_d: author KK author QQ two different individuals. job forensic linguist forensic context analyse linguistic evidence determine hypothesis supports degree strength, thus aiding trier--fact reaching conclusion. role forensic linguist therefore provide YES/answer rather express strength evidence favour two hypotheses. Given KK, QQ RR, workflow analysis involves four steps: Preparation: step involves pre-processing step necessary analysis chosen method; Validation: Carry analysis case data separate dataset designed similar case material order validate method particular case; Analysis: Carry analysis real KK, QQ, RR; Calibration: Turn output (3) Likelihood Ratio expresses strength evidence given two competing hypotheses.","code":""},{"path":"https://andreanini.github.io/idiolect/articles/idiolect.html","id":"preparation","dir":"Articles","previous_headings":"","what":"Preparation","title":"idiolect","text":"idiolect function import texts R called create_corpus(). function simply calling readtext (therefore package must installed) scanning name files metadata text, specifically name author name file. syntax follow name files authorname_textname.txt (e.g. smith_text1.txt). Assuming folder plain text files names according syntax ready user’s computer, following command (executed ) loads folder quanteda corpus object metadata docvars. vignette, instead, workflow demonstrated using small dataset Enron corpus included package (see ?enron.sample). corpus quanteda corpus object contains ten authors approximately amount data.","code":"corpus <- create_corpus(\"path/to/folder\") corpus <- enron.sample"},{"path":"https://andreanini.github.io/idiolect/articles/idiolect.html","id":"content-masking","dir":"Articles","previous_headings":"Preparation","what":"Content masking","title":"idiolect","text":"highly recommended sometimes necessary pre-processing step content masking. step consists masking removing words tokens text likely create noise authorship analysis. Hiding content avoids incorrectly attributing text based correlation topics authors (Bischoff et al. 2020) also tends improve performance authorship analysis methods cross-topic cross-genre situations (Stamatatos 2017). Three content masking methods implemented idiolect: (1) POSnoise algorithm developed Halvani Graner (2021); (2) frame n-grams approach introduced Nini (2023); (3) implementation TextDistortion approach originally introduced Stamatatos (2017). options available contentmask() function. function depends spacyr requires downloading parsing model language automatic tagging Parts Speech (e.g. nouns, adjectives, adverbs), function run vignette. Instead, Enron sample already content-masked using POSnoise, can seen preview corpus POSnoise algorithm essentially replaces words tend contain meaning (nouns, verbs, adjectives, adverbs) Part Speech tag (N, V, J, B) words tokens left unchanged. addition operation, POSnoise contains white list content words mostly tend functional English, verbs like , , make adverbs consequently, therefore. following code used run contentmask() function. require installing initiating spacy parsing model language chosen. process happen automatically","code":"corpus #> Corpus consisting of 49 documents and 1 docvar. #> known [Kh Mail_1].txt : #> \"N N N N wants to be N when he V up likes N P , N for doing t...\" #>  #> known [Kh Mail_3].txt : #> \"i 've V a J one , but the only N N N i have is on a N N from...\" #>  #> known [Kh Mail_4].txt : #> \"this was J towards the N of a J N N N . in N , P P helped th...\" #>  #> known [Kh Mail_5].txt : #> \"V the N for more than D N may get you V . a N N with a N and...\" #>  #> unknown [Kh Mail_2].txt : #> \"P , here 's the J N on our P P N V to V the V needs of the P...\" #>  #> unknown [Kw Mail_3].txt : #> \"they also have J N at the J N of P D per N and only a D J ea...\" #>  #> [ reached max_ndoc ... 43 more documents ] posnoised.corpus <- contentmask(corpus, model = \"en_core_web_sm\", algorithm = \"POSnoise\")"},{"path":"https://andreanini.github.io/idiolect/articles/idiolect.html","id":"data-labelling","dir":"Articles","previous_headings":"Preparation","what":"Data labelling","title":"idiolect","text":"example simulated first text written author Kw real QQ text (one labelled ‘unknown’) known texts written Kw (labelled ‘known’) therefore set known texts KK. remaining texts authors reference samples RR.","code":"Q <- corpus_subset(corpus, author == \"Kw\")[1] K <- corpus_subset(corpus, author == \"Kw\")[2:5] R <- corpus_subset(corpus, author != \"Kw\")"},{"path":"https://andreanini.github.io/idiolect/articles/idiolect.html","id":"vectorisation","dir":"Articles","previous_headings":"Preparation","what":"Vectorisation","title":"idiolect","text":"applying certain authorship analysis methods, text sample must turned numerical representation called feature vector, process typically referred vectorisation. idiolect function vectorise corpus called vectorize(). features normally used many authorship analysis methods nn-grams words punctuation marks characters. example, QQ text can vectorised relative frequencies words using code. , frequent 1,000 character 4-grams relative frequencies, example, using output function quanteda document-feature matrix (dfm) can efficiently store even large matrices. vectorize() function mostly designed expert users different choices parameters vectorisation can made using single authorship analysis method function. addition, since authorship analysis methods already default setting parameters, already default authorship analysis functions. step therefore necessary unless specific requirements vectorisation handled functions apply authorship analysis methods.","code":"vectorize(Q, tokens = \"word\", remove_punct = F, remove_symbols = T, remove_numbers = T,           lowercase = T, n = 1, weighting = \"rel\", trim = F) |>    print(max_nfeat = 3) #> Document-feature matrix of: 1 document, 136 features (0.00% sparse) and 1 docvar. #>                          features #> docs                            they        also      have #>   unknown [Kw Mail_3].txt 0.00289296 0.009643202 0.0192864 #> [ reached max_nfeat ... 133 more features ] vectorize(Q, tokens = \"character\", remove_punct = F, remove_symbols = T, remove_numbers = T,           lowercase = T, n = 4, weighting = \"rel\", trim = T, threshold = 1000) |>    print(max_nfeat = 3) #> Document-feature matrix of: 1 document, 1,094 features (0.00% sparse) and 1 docvar. #>                          features #> docs                              they         hey          ey a #>   unknown [Kw Mail_3].txt 0.0009771987 0.0009771987 0.0003257329 #> [ reached max_nfeat ... 1,091 more features ]"},{"path":"https://andreanini.github.io/idiolect/articles/idiolect.html","id":"validation","dir":"Articles","previous_headings":"","what":"Validation","title":"idiolect","text":"first step validation remove real QQ text. actual forensic sample analyse must therefore removed validating analysis. validation set therefore made KK RR datasets dataset can now re-divided ‘fake’ QQ texts ‘fake’ KK texts. text corpus labelled ‘unknown’ ‘known’ two new disjoint datasets, validation.Q validation.K can created selecting texts based label. way validation analysis can conducted. example, one adopt leave-one-approach taking single text treat QQ run authorship analysis method one . Alternatively, completely different dataset similar case data used. simpler approach suitable small example.","code":"validation <- K + R validation.Q <- corpus_subset(validation, grepl(\"^unknown\", docnames(validation))) validation.K <- corpus_subset(validation, grepl(\"^known\", docnames(validation)))"},{"path":"https://andreanini.github.io/idiolect/articles/idiolect.html","id":"authorship-analysis","dir":"Articles","previous_headings":"Validation","what":"Authorship analysis","title":"idiolect","text":"analysis validated analysis applied QQ text. Therefore, choice method made depending right choice analyse QQ. example, scenario simulated verification: unknown QQ text written KK author, Kw? reason, method chosen one successful authorship verification methods available today, Impostors Method (Koppel Winter 2014), particular one latest variants called Rank-Based Impostors Method (Potha Stamatatos 2017, 2020). analysis can run idiolect using function impostors() selecting default parameter algorithm argument, “RBI”. main argument function q.data, set QQ texts test, k.data, set KK texts one authors going tested, finally set impostors data, cand.imps. example, impostors data RR set generally recommendation use another dataset possible. impostors() function accepts one author k.data also accepts dataset input k.data cand.imps. dataset used, impostors() test author k.data use texts written authors impostors. contrast authorship analysis functions like delta() ngram_tracing(), impostors() offer additional parameters modify vectorisation process Impostors Method algorithms already well-specified default setting. user wants change vectorise corpus separately using vectorize() use dfm input impostors(). RBI variant method also requires setting parameter called kk, number similar impostors texts sample wider set impostors. recommended setting k=100k=100 k=300k=300 simplicity set k=50k=50 example. analysis using Impostors Method can long run times, function can also parallelised using one core. output impostors() data frame showing results comparing KK author QQ text. variable target TRUE comparison -author one FALSE different-author one. variable score contains Impostors score, value ranges 0 1. authorship analysis functions return data frame type columns. variable score therefore represents different quantities depending analysis function used (e.g. delta(), Δ\\Delta coefficient, ). order assess results validation analysis, function performance() can used return series performance metrics. function can take one two result data frames input. two provided, one used training one test. one data frame provided, performance metrics calculated using leave-one-approach. procedure followed function held one text (leave-one-, otherwise test dataset entirety) use rest data (training dataset) calibration dataset calculate Log-Likelihood Ratio (LLRLLR). analysis done using calibrate_LLR() function, fits logistic regression model calibrate score LLRLLRIshihara (2021) using ROC library (Leeuwen 2015). output function following CllrC_{llr} CllrminC_{llr}^{min} coefficients used evaluate performance LLRLLR(Ramos et al. 2013). coefficients estimate cost LLRLLR, value 1 indicates information LLRLLR lower coefficient Cllr<1C_{llr}<1 suggests information LLRLLR, lower values CllrC_{llr} suggesting better performance. binary classification metrics returned, Precision, Recall, F1, calculated using LLR>0LLR > 0 threshold TRUE (-author case) classification. present example, Cllr=C_{llr}= 0.804 suggests enough information LLRLLR able proceed actual forensic analysis. CllrminC_{llr}^{min}, component CllrC_{llr} measuring amount discrimination, even lower, means substantial difference two distributions. confirmed Area Curve value 0.83. large disparity TRUE FALSE test cases, values Precision F1 misleading. Balanced Accuracy value 0.846, however, suggests substantial amount discrimination LLR=0LLR=0. results analysis can also plotted using density plot two distributions, TRUE FALSE. can done using density_plot() function  plot shows values score horizontal axis density TRUE (red) vs. FALSE (blue) vertical axis. findings evidence method validated dataset now possible analyse QQ text use results calibrate LLRLLR QQ.","code":"res <- impostors(validation.Q, validation.K, validation.K, algorithm = \"RBI\", k = 50) res[1:10,] #>     K                       Q target score #> 1  Kw unknown [Kh Mail_2].txt  FALSE 0.404 #> 2  Kw unknown [Lc Mail_1].txt  FALSE 0.243 #> 3  Kw unknown [Ld Mail_4].txt  FALSE 0.752 #> 4  Kw unknown [Lt Mail_2].txt  FALSE 0.215 #> 5  Kw unknown [Lk Mail_4].txt  FALSE 0.306 #> 6  Kw unknown [Lb Mail_3].txt  FALSE 0.975 #> 7  Kw unknown [La Mail_3].txt  FALSE 0.262 #> 8  Kw unknown [Mf Mail_1].txt  FALSE 0.933 #> 9  Kw unknown [Ml Mail_3].txt  FALSE 0.846 #> 10 Kh unknown [Kh Mail_2].txt   TRUE 0.555 p <- performance(res) #>   |                                                                              |                                                                      |   0%  |                                                                              |=                                                                     |   1%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  11%  |                                                                              |=========                                                             |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  15%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |=============                                                         |  19%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |=================                                                     |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  27%  |                                                                              |====================                                                  |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  31%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |========================                                              |  35%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |============================                                          |  40%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  44%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  48%  |                                                                              |===================================                                   |  49%  |                                                                              |===================================                                   |  51%  |                                                                              |====================================                                  |  52%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |=======================================                               |  56%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  65%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  69%  |                                                                              |=================================================                     |  70%  |                                                                              |==================================================                    |  71%  |                                                                              |==================================================                    |  72%  |                                                                              |===================================================                   |  73%  |                                                                              |====================================================                  |  74%  |                                                                              |=====================================================                 |  75%  |                                                                              |=====================================================                 |  76%  |                                                                              |======================================================                |  78%  |                                                                              |=======================================================               |  79%  |                                                                              |========================================================              |  80%  |                                                                              |=========================================================             |  81%  |                                                                              |=========================================================             |  82%  |                                                                              |==========================================================            |  83%  |                                                                              |===========================================================           |  84%  |                                                                              |============================================================          |  85%  |                                                                              |=============================================================         |  87%  |                                                                              |=============================================================         |  88%  |                                                                              |==============================================================        |  89%  |                                                                              |===============================================================       |  90%  |                                                                              |================================================================      |  91%  |                                                                              |================================================================      |  92%  |                                                                              |=================================================================     |  93%  |                                                                              |==================================================================    |  94%  |                                                                              |===================================================================   |  96%  |                                                                              |====================================================================  |  97%  |                                                                              |====================================================================  |  98%  |                                                                              |===================================================================== |  99%  |                                                                              |======================================================================| 100% p$evaluation #>        Cllr  Cllr_min      EER Mean TRUE LLR Mean FALSE LLR TRUE trials #> 1 0.8043863 0.6155326 19.04762      0.413398     -0.3864078          11 #>   FALSE trials      AUC Balanced Accuracy Precision    Recall        F1 TP FN #> 1           83 0.829904          0.845679 0.3333333 0.8888889 0.4848485  8  1 #>   FP TN #> 1 16 65 density_plot(res)"},{"path":"https://andreanini.github.io/idiolect/articles/idiolect.html","id":"analysis-of-q","dir":"Articles","previous_headings":"","what":"Analysis of QQ","title":"idiolect","text":"point thing left analyse forensic data feeding real QQ, KK, RR impostors() function using settings used validation. one QQ text, final table results contains one row","code":"q.res <- impostors(Q, K, R, algorithm = \"RBI\", k = 50) q.res #>    K                       Q target score #> 1 Kw unknown [Kw Mail_3].txt   TRUE 0.975"},{"path":"https://andreanini.github.io/idiolect/articles/idiolect.html","id":"qualitative-examination-of-evidence","dir":"Articles","previous_headings":"Analysis of QQ","what":"Qualitative examination of evidence","title":"idiolect","text":"reaching conclusions, often important inspect features algorithm considered analysis. forensic analysis, good knowledge data important best practice require analyst familiar dataset running computational analysis. Reading data familiar can lead addition pre-processing steps remove noise can help analyst spot problem mistakenly introduced algorithm. addition familiarise data, idiolect allows analyst explore important feature considered authorship analysis method used. example, using RBI algorithm impostors() parameter features can switched TRUE obtain list important features. Running parameter switched produces following results RBI method uses features character 4-grams list features clearly hard interpret human analyst. Despite complexity, impossible task. idiolect offers function aid exploration called concordance(), uses quanteda’s kwic() engine. concordance() takes input string representing one words (punctuation marks). example, important character 4-gram seems <, > search target. search reveals character sequence strong characteristic candidate author’s writing. However, real underlying pattern use comma followed possessive determiner token sequence [, ], used candidate author one author reference corpus. Another important feature represented two character 4-grams, <lso ,> <, >, likely refer token sequence [also ,]. correct referring use also beginning sentence immediately followed comma. Although searching features returned clearly significant amount work, inspecting list features carefully using concordance() explore features data analyst can spot patterns mistakes analysis (Ypma, Ramos, Meuwly 2023).","code":"q.res2 <- impostors(Q, K, R, algorithm = \"RBI\", k = 50, features = T) strwrap(q.res2$features, width = 70) #>  [1] \", her|lso ,|so , |P P f|, i w|N N .| too |our P|ur P |also |re is|i\"   #>  [2] \"jus|ll me|l me |ere i|st wa|P , h|u hav| yet | , he|ou ha|ust w|you\"   #>  [3] \"h| also|rom P|om P |ve no|u and|he J |N N N|the J|V our| here|ve a\"    #>  [4] \"|ith y|re al|few N|ew N | so i|ou an|e a N|P P i|P on |th yo| V my|V\"  #>  [5] \"my |e N w| , so|me to| few |eithe|ither| him |V it | P on|out w|at i\"  #>  [6] \"|t kno| may |P P V| N fr|u to |N fro|e is |V wit| V it|P for|, i h|\"   #>  [7] \"is o|u nee|ou to| V wi|s for| P fo|a N a|do no|h you| P in|P in | B ,\" #>  [8] \"| J N | me t| 'll |you n|ou ne|o not|, N a|t to |n N .|N in |o be |ke\" #>  [9] \"a |n P ,| N or|N or | look|t wan|ave a| it .|ave n|o V i|r J N|e an\"   #> [10] \"|as i |P D .|e tha|is J |o V w|r N w| , i |N who|t V a|e N s|se le| N\" #> [11] \"on|nd th|s you|ng th| is a|N N w|ase l| one | can |e hav| 's N|'s N |\" #> [12] \"i wo|i was|in V |e let| at t|are a| our |t the|N on |i hav|is V | as\"  #> [13] \"i|e J N|you t| to m|P 's |P P o|ake a|e thi|s N N| a N |in a |n you|\"  #> [14] \"you |hat P|o see| week|week |r N o|nt to| N if|N if |or V |N for|ut i\" #> [15] \"| N J |the N|he N | into|into |o hav| P 's|hat y|n our|if yo| N fo|to\" #> [16] \"do|o do | is J|P P .| my N| N in|me N |t you| i ha| do n| get |an V\"   #> [17] \"|e N N|V tha| i am|e P P|you N|ant t| on P| is V| i V |ome N|ore\"      #> [18] \"N|ave b|other|with | a J |at yo| N i | we w| the |did n|thing|hing |e\" #> [19] \"wil| J in|J in |on P |to me|o me |s the|ed to|or P |ve V |my N |to\"    #> [20] \"se| in a| but | , th| P , |t is | see |i wil|this |ou N |V thi|or N |\" #> [21] \"have|is a |N has|J to |have |i V t|o V o| V a | V fo|ther |e V i|i am\" #> [22] \"| J fo|J for|is N |a J N|V and|V on |as V |V for|N , h|can V|J N\"      #> [23] \"N|his N| J , |J N .| N th|f you|some | N wi| V an| with| be J|be J |d\" #> [24] \"the|V to |V in |but i| star|start|ng on|g on |a N t| N wh| just|r\"     #> [25] \"you| V to|re N | this| in P|P to |ow if|w if |e you| for |to be| V N\"  #> [26] \"|P P P|and t|in P | work|our J|ur J | to b| P to| and |e als|r N i|ou\" #> [27] \"wo|i 'll|ch of|o V .|ach o|r D N|ase V|se V |h of |o mak|e and|to\"     #> [28] \"ma|nning|e V y|o the|ill V|and g|y N N|o V u|P tha|look | out\"         #> [29] \"|ether|ve an| it t|t i w|ere w|o V y|ning |o J N| by N|e nee|t V i|,\"  #> [30] \"as |e to | if y|by N | we n|P , a| give| N P | , as|N we |just |e\"     #> [31] \"any|V you|we wo| N V | each|our N|ur N |each |ill h|pleas|lease|ease\"  #> [32] \"|you a|e wou|e in |for D|here | she |needs|eeds | P N |tart |d you|\"   #> [33] \"othe|for N| plea| both|your |ll V |eek .|make | , we|, we | V yo|or D\" #> [34] \"|r N a|both |V any|a N f|worki|orkin|rking|N V N|re J | P V |n the|we\" #> [35] \"ne|o V N|N N a|more | more|s to |ld li| afte| V on|s tha| P D | J\"     #> [36] \"to|in th|any N|ny N |P , w| me a|fter |V the|e V a|be V | be V| who |\" #> [37] \"in t|give | , yo|, you| need|is th| call| did |at we| what|d to |V a\"  #> [38] \"N| any |d lik|after|at P |r N .| has | N we|her N| N , |s a N|ing a|N\" #> [39] \"is | of N|of N |ou ar|u are| V in|would|P P a|ing o|of yo| two |you\"   #> [40] \"w|N N i| your|r N N|t thi|we ar| N is|N N ,| P is| is t|y N o| we V|\"  #> [41] \"a fe|a few|ke to|i wou|r our|uld l|th N |ike t|V P P|e J t|like |\"     #> [42] \"like|e if |ow wh| woul| N by|N by |e V f|ee if|for h|y hav|, ple| ,\"   #> [43] \"pl| a D |V N o| find|find |now w|hat h|ted t|you m|i nee|N , s|th P |\" #> [44] \", bu|, but|e V t|lso V|so V |ith P|eed t| to V|o V t|to V |to th|ill\"  #> [45] \"n| on t|ould | make| let |P and| per |f the|d P P| N yo|you s|V up |\"  #> [46] \"P ha|and h|s N w| P an|N you|or th|e N o|not V|V J N|nd N |ot V | to\"  #> [47] \"s|for t|er N |e bee|ve be|see i|and N| will|are V|n to |ne of|them |\"  #> [48] \"them|N tha|one o|me of|id no|d not|f N .|t wit|N so |o V a|to ta| N\"   #> [49] \"so|e J .|e of |ome o|king |m to |l hav| i wa| of y|ll ha|we ha|a N\"    #> [50] \"o|u V t|need | we '|, P ,| N ar|N are| was | to g|hat N| from|r the|\"  #> [51] \"N B |from | N N |it is|take |to P |at th| to P|e N .|e V .|he P |the\"  #> [52] \"P| V th\" concordance(Q, K, R, search = \", her\", token.type = \"character\") |>    dplyr::select(pre, node, post, authorship) #>      pre  node  post authorship #> 1   . P  , her e is           Q #> 2  we V  , her e is           Q #> 3   . P  , her e is           Q #> 4  ur N  , her e is           K #> 5   . P  , her e is           K #> 6   . P  , her e is           K #> 7  nd P  , her e is           K #> 8   . P  , her e is           K #> 9   N N  , her e is           K #> 10 st P  , her e is           K #> 11  N N  , her e is           K #> 12    P  , her e 's   Reference #> 13  P P  , her e out  Reference #> 14 nd P  , her e is   Reference #> 15  . P  , her e are  Reference #> 16  . P  , her e are  Reference concordance(Q, K, R, search = \", here is\", token.type = \"word\") |>    dplyr::select(pre, node, post, authorship) #>                 pre      node                   post authorship #> 1   each of you . P , here is           the P P P on          Q #> 2    on this N we V , here is          a N N of both          Q #> 3       N for P . P , here is            the P P P P          Q #> 4  all . per your N , here is          some J N on P          K #> 5       B and V . P , here is           the J N N on          K #> 6         N V N . P , here is            the P P P P          K #> 7       N . P and P , here is        the J for the P          K #> 8       N N yet . P , here is            the P P P P          K #> 9    out of the N N , here is the beginning of the N          K #> 10 N on this past P , here is  the N we talked about          K #> 11   as per our N N , here is            the V P P P          K #> 12   know . P and P , here is            a N of N in  Reference concordance(Q, K, R, search = \"lso ,\", token.type = \"character\") |>    dplyr::select(pre, node, post, authorship) #>      pre  node  post authorship #> 1  N ? a lso ,  ther          Q #> 2  V . a lso ,  i ha          Q #> 3  P . a lso ,  plea          Q #> 4  k . a lso ,  P ha          Q #> 5  N . a lso ,  P P           K #> 6  N . a lso ,  at t          K #> 7  N ? a lso ,  i 'v          K #> 8  s . a lso ,  let           K #> 9      a lso ,  V th          K #> 10 N . a lso ,  i V           K #> 11 N . a lso ,  woul  Reference #> 12 . P a lso ,  V to  Reference #> 13 N . a lso ,  any   Reference #> 14 N . a lso ,  have  Reference #> 15 P . a lso ,  P P   Reference #> 16 N . a lso ,  you   Reference #> 17 D . a lso ,  to V  Reference #> 18 N . a lso ,  tell  Reference #> 19 J . a lso ,  coul  Reference #> 20 N . a lso ,  we n  Reference #> 21 J . a lso ,  ther  Reference"},{"path":"https://andreanini.github.io/idiolect/articles/idiolect.html","id":"conclusions","dir":"Articles","previous_headings":"Analysis of QQ","what":"Conclusions","title":"idiolect","text":"Although score assigned QQ high, depending calibration data, can correspond various magnitudes LLRLLR. LLRLLR value QQ can also plotted onto TRUE vs. FALSE distributions using second argument density_plot(). q argument can used draw black vertical line crosses two distributions horizontal axis corresponding score QQ.  perform calibration calibrate_LLR() function used using validation results calibration data function returns LLRLLR value also verbal labels interpretation (Marquis et al. 2016). final conclusion analysis therefore following: similarity score QQ given KK 0.975, corresponds LLR=LLR= 1.432. similarity 27.04 times likely observed case HpH_p case HdH_d. Therefore, linguistic analysis offers Moderate support HpH_p. conclusion can complemented explanation implication results trier facts showing table posterior probabilities assuming range prior probabilities. can done posterior() function inserting input value LLRLLR table reveals , assuming prior probability HpH_p 0.00001 (roughly, one population Manchester), LLRLLR transform probability posterior probability HpH_p 0.000027. words, make much substantial difference trial. However, prior probability HpH_p 0.5, results turn 0.96, substantial change. table shows present evidence change probability HpH_p true equal higher 0.9 prior greater 0.2.","code":"density_plot(res, q = q.res$score) q.llr <- calibrate_LLR(res, q.res, latex = T) q.llr$`Verbal label` #> [1] \"Moderate support for $H_p$\" strwrap(q.llr$Interpretation) #> [1] \"The similarity is 27.04 times more likely to be observed in the case of\" #> [2] \"$H_p$ than in the case of $H_d$\" posterior(q.llr$LLR) |>    dplyr::select(prosecution_prior_probs, prosecution_post_probs) #> # A tibble: 11 × 2 #>    prosecution_prior_probs prosecution_post_probs #>                      <dbl>                  <dbl> #>  1                0.000001              0.0000270 #>  2                0.01                  0.215     #>  3                0.1                   0.750     #>  4                0.2                   0.871     #>  5                0.3                   0.921     #>  6                0.4                   0.947     #>  7                0.5                   0.964     #>  8                0.6                   0.976     #>  9                0.7                   0.984     #> 10                0.8                   0.991     #> 11                0.9                   0.996"},{"path":"https://andreanini.github.io/idiolect/articles/idiolect.html","id":"acknowledgements","dir":"Articles","previous_headings":"","what":"Acknowledgements","title":"idiolect","text":"like thank Shunichi Ishihara Marie Bojsen-Møller helpful comments first draft vignette.","code":""},{"path":[]},{"path":"https://andreanini.github.io/idiolect/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Andrea Nini. Author, maintainer, copyright holder. David van Leeuwen. Copyright holder.           Author bundled functions package ROC","code":""},{"path":"https://andreanini.github.io/idiolect/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Andrea Nini (2024). Idiolect: R package forensic authorship analysis. https://andreanini.github.io/idiolect/.","code":"@Manual{,   title = {Idiolect: An R package for forensic authorship analysis},   author = {{Andrea Nini}},   year = {2024},   url = {https://andreanini.github.io/idiolect/}, }"},{"path":"https://andreanini.github.io/idiolect/index.html","id":"idiolect-","dir":"","previous_headings":"","what":"Forensic Authorship Analysis","title":"Forensic Authorship Analysis","text":"idiolect R package designed provide comprehensive suite tools performing comparative authorship analysis within forensic context using Likelihood Ratio Framework (e.g. Ishihara 2021; Nini 2023). package contains set authorship analysis functions take set texts input output scores can calibrated likelihood ratios. package dependent quanteda (Benoit et al. 2018) Natural Language Processing functions.","code":""},{"path":"https://andreanini.github.io/idiolect/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Forensic Authorship Analysis","text":"can install idiolect CRAN:","code":"install.packages(\"idiolect\")"},{"path":"https://andreanini.github.io/idiolect/index.html","id":"workflow","dir":"","previous_headings":"","what":"Workflow","title":"Forensic Authorship Analysis","text":"main functions contained package reflect typical workflow authorship analysis forensic problems: Input data using create_corpus(); Optionally mask content/topic texts using contentmask(); Launch analysis (e.g. delta(), ngram_tracing(), impostors()); Test performance method ground truth data using performance(); Finally, apply method questioned text generate likelihood ratio calibrate_LLR(). Check website vignette examples.","code":""},{"path":[]},{"path":"https://andreanini.github.io/idiolect/reference/calibrate_LLR.html","id":null,"dir":"Reference","previous_headings":"","what":"Calibrate scores into Log-Likelihood Ratios — calibrate_LLR","title":"Calibrate scores into Log-Likelihood Ratios — calibrate_LLR","text":"function used transform scores returned authorship analysis functions Log-Likelihood Ratio (LLR).","code":""},{"path":"https://andreanini.github.io/idiolect/reference/calibrate_LLR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calibrate scores into Log-Likelihood Ratios — calibrate_LLR","text":"","code":"calibrate_LLR(calibration.dataset, dataset, latex = FALSE)"},{"path":"https://andreanini.github.io/idiolect/reference/calibrate_LLR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calibrate scores into Log-Likelihood Ratios — calibrate_LLR","text":"calibration.dataset data frame containing calibration dataset, typically output authorship analysis function like impostors(). dataset data frame containing scores calibrated LLRs using calibration dataset. typically result applying function like impostors() Q texts. latex logical value. FALSE (default), hypothesis labels printed plain text (Hp/Hd). TRUE labels written read LaTeX ($H_p$/$H_d$).","code":""},{"path":"https://andreanini.github.io/idiolect/reference/calibrate_LLR.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calibrate scores into Log-Likelihood Ratios — calibrate_LLR","text":"function returns data frame LLRs (base 10), well verbal label according Marquis et al (2016) verbal interpretation results.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/calibrate_LLR.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calibrate scores into Log-Likelihood Ratios — calibrate_LLR","text":"Marquis, Raymond, Alex Biedermann, Liv Cadola, Christophe Champod, Line Gueissaz, Geneviève Massonnet, Williams David Mazzella, Franco Taroni & Tacha Hicks. 2016. Discussion implement verbal scale forensic laboratory: Benefits, pitfalls suggestions avoid misunderstandings. Science & Justice 56(5). 364–370. https://doi.org/10.1016/j.scijus.2016.05.009.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/calibrate_LLR.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calibrate scores into Log-Likelihood Ratios — calibrate_LLR","text":"","code":"calib <- data.frame(score = c(0.5, 0.2, 0.8, 0.01, 0.6), target = c(TRUE, FALSE, TRUE, FALSE, TRUE)) q <- data.frame(score = c(0.6, 0.002)) calibrate_LLR(calib, q) #>   score     LLR                    Verbal label #> 1 0.600  16.135 Extremely strong support for Hp #> 2 0.002 -22.723 Extremely strong support for Hd #>                                                                                                     Interpretation #> 1    The similarity is 13645831365889294 times more likely to be observed in the case of Hp than in the case of Hd #> 2 The similarity is 5.28445251775179e+22 times more likely to be observed in the case of Hd than in the case of Hp"},{"path":"https://andreanini.github.io/idiolect/reference/chunk_texts.html","id":null,"dir":"Reference","previous_headings":"","what":"Chunk a corpus — chunk_texts","title":"Chunk a corpus — chunk_texts","text":"function can used chunk corpus order control sample sizes.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/chunk_texts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chunk a corpus — chunk_texts","text":"","code":"chunk_texts(corpus, size)"},{"path":"https://andreanini.github.io/idiolect/reference/chunk_texts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chunk a corpus — chunk_texts","text":"corpus quanteda corpus. size size chunks number tokens.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/chunk_texts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chunk a corpus — chunk_texts","text":"quanteda corpus object text chunk size requested.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/chunk_texts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chunk a corpus — chunk_texts","text":"","code":"corpus <- quanteda::corpus(c(\"The cat sat on the mat\", \"The dog sat on the chair\")) quanteda::docvars(corpus, \"author\") <- c(\"A\", \"B\") chunk_texts(corpus, size = 2) #> Corpus consisting of 6 documents and 1 docvar. #> text1.1 : #> \"The cat\" #>  #> text1.2 : #> \"sat on\" #>  #> text1.3 : #> \"the mat\" #>  #> text2.1 : #> \"The dog\" #>  #> text2.2 : #> \"sat on\" #>  #> text2.3 : #> \"the chair\" #>"},{"path":"https://andreanini.github.io/idiolect/reference/concordance.html","id":null,"dir":"Reference","previous_headings":"","what":"Qualitative examination of evidence — concordance","title":"Qualitative examination of evidence — concordance","text":"function uses quanteda::kwic() return concordance search pattern. function takes input three datasets pattern returns data frame hits labelled authorship.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/concordance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Qualitative examination of evidence — concordance","text":"","code":"concordance(   q.data,   k.data,   reference.data,   search,   token.type = \"word\",   window = 5,   case_insensitive = TRUE )"},{"path":"https://andreanini.github.io/idiolect/reference/concordance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Qualitative examination of evidence — concordance","text":"q.data quanteda corpus object, output create_corpus(). k.data quanteda corpus object, output create_corpus(). reference.data quanteda corpus object, output create_corpus(). optional. search string. can sequence characters also accepts use * wildcard. token.type Choice \"word\" (default), searches word punctuation mark tokens, \"character\", instead uses single character search. window number context items displayed around keyword (quanteda::kwic() parameter). case_insensitive Logical; TRUE, ignore case (quanteda::kwic() parameter).","code":""},{"path":"https://andreanini.github.io/idiolect/reference/concordance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Qualitative examination of evidence — concordance","text":"function returns data frame containing concordances search pattern.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/concordance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Qualitative examination of evidence — concordance","text":"","code":"concordance(enron.sample[1], enron.sample[2], enron.sample[3:49], \"wants to\", token.type = \"word\") #>                 docname from  to                 pre     node           post #> 1 known [Kh Mail_1].txt    5   6             N N N N wants to be N when he V #> 2 known [Ld Mail_5].txt  160 161          D S D . he wants to   V to V the N #> 3 known [Lb Mail_1].txt  573 574 our N . anyone that wants to    V us is J . #>   authorship #> 1          Q #> 2  Reference #> 3  Reference  #using wildcards concordance(enron.sample[1], enron.sample[2], enron.sample[3:49], \"want * to\", token.type = \"word\") #>                 docname from  to                pre       node #> 1 known [Kw Mail_2].txt  672 674 let me know if you want me to #> 2 known [Lc Mail_5].txt  175 177       s N . if you want me to #> 3 known [Ml Mail_5].txt  242 244  need . you do n't want me to #>                    post authorship #> 1       V on other N in  Reference #> 2        , i can put on  Reference #> 3 come work for you too  Reference  #searching character sequences with wildcards concordance(enron.sample[1], enron.sample[2], enron.sample[3:49], \"help*\", token.type = \"character\") #>                    docname from   to   pre  node  post authorship #> 1    known [Kh Mail_1].txt  703  707 need  help  V it           Q #> 2    known [Kh Mail_1].txt 2014 2018 want  help  V it           Q #> 3    known [Kh Mail_3].txt 1797 1801  N ,  helpe d the          K #> 4    known [Kh Mail_4].txt   52   56  P P  helpe d the  Reference #> 5  unknown [Kw Mail_3].txt 2756 2760 ding  help  in th  Reference #> 6    known [Kw Mail_5].txt   31   35 your  help  and N  Reference #> 7    known [Kw Mail_5].txt 1463 1467 need  help  in do  Reference #> 8    known [Lc Mail_2].txt 1600 1604 some  help  . why  Reference #> 9    known [Lc Mail_5].txt 1163 1167 d of  help  and B  Reference #> 10   known [Ld Mail_2].txt  285  289 ally  help  us ou  Reference #> 11   known [Lt Mail_1].txt  884  888 r be  helpi ng to  Reference #> 12   known [Lt Mail_1].txt  919  923 , or  help  V a N  Reference #> 13   known [Lt Mail_3].txt  910  914 your  help  as a   Reference #> 14   known [Lt Mail_4].txt 1611 1615 ttle  help  from   Reference #> 15 unknown [Lk Mail_4].txt 1243 1247 N to  help  V N f  Reference #> 16 unknown [Lk Mail_4].txt 1272 1276 N to  help  V the  Reference #> 17   known [Lk Mail_1].txt 1512 1516 ease  help  him w  Reference #> 18   known [Lk Mail_2].txt  387  391 ight  help  . ple  Reference #> 19   known [Lk Mail_3].txt  994  998 ease  help  him a  Reference #> 20 unknown [Lb Mail_3].txt 2279 2283 N to  help  our N  Reference #> 21 unknown [Lb Mail_3].txt 2405 2409  and  help  the N  Reference #> 22 unknown [Lb Mail_3].txt 2479 2483 g to  help  out w  Reference #> 23 unknown [Lb Mail_3].txt 2617 2621  and  help  them   Reference #> 24   known [Lb Mail_1].txt 1363 1367 d to  help  you i  Reference #> 25   known [Lb Mail_2].txt 1652 1656  and  help  in V   Reference #> 26   known [Lb Mail_2].txt 1676 1680  and  helpi ng ea  Reference #> 27   known [Lb Mail_4].txt 1038 1042 e to  help  P N a  Reference #> 28   known [Lb Mail_5].txt 1066 1070 this  helps  V ou  Reference #> 29   known [La Mail_2].txt 2086 2090 ould  help  V the  Reference #> 30   known [La Mail_2].txt 2494 2498 ould  help  get t  Reference #> 31   known [La Mail_4].txt 1908 1912 also  help  V N .  Reference #> 32   known [La Mail_5].txt 2424 2428 will  help  the N  Reference #> 33   known [Mf Mail_2].txt  805  809 your  help  . thi  Reference #> 34   known [Mf Mail_2].txt 2097 2101  any  help  you c  Reference #> 35   known [Mf Mail_2].txt 2458 2462  can  help  with   Reference #> 36   known [Ml Mail_1].txt  596  600  you  help  and V  Reference #> 37   known [Ml Mail_1].txt 1223 1227 your  help  and l  Reference #> 38   known [Ml Mail_1].txt 2492 2496  and  help  save   Reference #> 39   known [Ml Mail_1].txt 2598 2602 your  help  and V  Reference #> 40   known [Ml Mail_2].txt   12   16 your  help  and V  Reference #> 41   known [Ml Mail_4].txt  622  626 your  help  in V   Reference #> 42   known [Ml Mail_4].txt 1296 1300 ou N  helpe d us   Reference #> 43   known [Ml Mail_4].txt 1589 1593 your  help  so th  Reference #> 44   known [Ml Mail_4].txt 1962 1966 your  help  and V  Reference #> 45   known [Ml Mail_5].txt  475  479 an B  help  you V  Reference"},{"path":"https://andreanini.github.io/idiolect/reference/contentmask.html","id":null,"dir":"Reference","previous_headings":"","what":"Content masking — contentmask","title":"Content masking — contentmask","text":"function offers three algorithms topic/content masking. order run masking algorithms, spacy tokenizer POS-tagger run first (via spacyr). information masking algorithms see Details .","code":""},{"path":"https://andreanini.github.io/idiolect/reference/contentmask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Content masking — contentmask","text":"","code":"contentmask(   corpus,   model = \"en_core_web_sm\",   algorithm = \"POSnoise\",   fw_list = \"eng_halvani\",   replace_non_ascii = TRUE )"},{"path":"https://andreanini.github.io/idiolect/reference/contentmask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Content masking — contentmask","text":"corpus quanteda corpus object, typically output create_corpus() function. model spacy model use. default \"en_core_web_sm\". algorithm string, either \"POSnoise\" (default), \"frames\", \"textdistortion\". fw_list list function words use textdistortion algorithm. either default (\"eng_halvani\") list function words used POSnoise can vector strings string function word keep. replace_non_ascii logical value indicating whether remove non-ASCII characters (including emojis). default.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/contentmask.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Content masking — contentmask","text":"quanteda corpus object containing functional tokens, depending algorithm chosen. corpus contains docvars input. Email addresses URLs treated like nouns.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/contentmask.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Content masking — contentmask","text":"default algorithm content masking function applies POSnoise (Halvani Graner 2021). algorithm works English transforms text masking tokens using POS tag tokens : nouns, verbs, adjectives, adverbs, digits, symbols leaving rest unchanged. POSnoise uses list function words English also includes frequent words belonging masked Part Speech tags tend mostly functional (e.g. make, recently, well). Another algorithm implemented Nini's (2023) frames frame n-grams. algorithm involve special list tokens therefore can potentially work language provided correct spacy model loaded. algorithm consists masking tokens using POS tag nouns, verbs, personal pronouns. Finally, last algorithm implemented version textdistortion, originally proposed Stamatatos (2017). version algorithm essentially POSnoise without POS tag information. default implementation uses list function words used POSnoise. addition function words provided, function treats punctuation marks new line breaks function words keep. basic tokenization done using spacyr right model language analysed selected. never used spacyr please follow instructions set install model using function. removal non-ASCII characters done using textclean package.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/contentmask.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Content masking — contentmask","text":"Halvani, Oren & Lukas Graner. 2021. POSNoise: Effective Countermeasure Topic Biases Authorship Analysis. Proceedings 16th International Conference Availability, Reliability Security, 1–12. Vienna, Austria: Association Computing Machinery. https://doi.org/10.1145/3465481.3470050. Nini, Andrea. 2023. Theory Linguistic Individuality Authorship Analysis (Elements Forensic Linguistics). Cambridge, UK: Cambridge University Press. Stamatatos, Efstathios. 2017. Masking topic-related information enhance authorship attribution. Journal Association Information Science Technology. https://doi.org/10.1002/asi.23968.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/contentmask.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Content masking — contentmask","text":"","code":"if (FALSE) { # \\dontrun{ text <- \"The cat was on the chair. He didn't move\\ncat@pets.com;\\nhttp://quanteda.io/. i.e. a test \" toy.corpus <- quanteda::corpus(text) contentmask(toy.corpus, algorithm = \"POSnoise\") contentmask(toy.corpus, algorithm = \"textdistortion\") } # }"},{"path":"https://andreanini.github.io/idiolect/reference/create_corpus.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a corpus — create_corpus","title":"Create a corpus — create_corpus","text":"Function read text data turn quanteda corpus object.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/create_corpus.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a corpus — create_corpus","text":"","code":"create_corpus(path)"},{"path":"https://andreanini.github.io/idiolect/reference/create_corpus.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a corpus — create_corpus","text":"path string containing path folder plain text files (ending .txt) name structured following: authorname_textname.txt (e.g. smith_text1.txt).","code":""},{"path":"https://andreanini.github.io/idiolect/reference/create_corpus.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a corpus — create_corpus","text":"quanteda corpus object authors' names docvar.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/create_corpus.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a corpus — create_corpus","text":"","code":"if (FALSE) { # \\dontrun{ path <- \"path/to/data\" create_corpus(path) } # }"},{"path":"https://andreanini.github.io/idiolect/reference/delta.html","id":null,"dir":"Reference","previous_headings":"","what":"Delta — delta","title":"Delta — delta","text":"function runs Cosine Delta analysis (Smith Aldridge 2011; Evert et al. 2017).","code":""},{"path":"https://andreanini.github.io/idiolect/reference/delta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delta — delta","text":"","code":"delta(   q.data,   k.data,   tokens = \"word\",   remove_punct = FALSE,   remove_symbols = TRUE,   remove_numbers = TRUE,   lowercase = TRUE,   n = 1,   trim = TRUE,   threshold = 150,   features = FALSE,   cores = NULL )"},{"path":"https://andreanini.github.io/idiolect/reference/delta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delta — delta","text":"q.data questioned disputed data, either corpus (output create_corpus()) quanteda dfm (output vectorize()). k.data known undisputed data, either corpus (output create_corpus()) quanteda dfm (output vectorize()). tokens type tokens extract, either \"word\" (default) \"character\". remove_punct logical value. FALSE (default) keeps punctuation marks. remove_symbols logical value. TRUE (default) removes symbols. remove_numbers logical value. TRUE (default) removes numbers lowercase logical value. TRUE (default) transforms tokens lower case. n order size n-grams extracted. Default 1. trim logical value. TRUE (default) frequent tokens kept. threshold numeric value indicating many frequent tokens keep trim = TRUE. default 150. features Logical default FALSE. TRUE, output contain features used. cores number cores use parallel processing (default one).","code":""},{"path":"https://andreanini.github.io/idiolect/reference/delta.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Delta — delta","text":"features set FALSE output data frame containing results comparisons Q texts K texts. features set TRUE output list containing results data frame vector features used analysis.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/delta.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Delta — delta","text":"Evert, Stefan, Thomas Proisl, Fotis Jannidis, Isabella Reger, Steffen Pielström, Christof Schöch & Thorsten Vitt. 2017. Understanding explaining Delta measures authorship attribution. Digital Scholarship Humanities 32. ii4–ii16. https://doi.org/10.1093/llc/fqx023. Smith, Peter W H & W Aldridge. 2011. Improving Authorship Attribution: Optimizing Burrows’ Delta Method*. Journal Quantitative Linguistics 18(1). 63–88. https://doi.org/10.1080/09296174.2011.533591.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/delta.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Delta — delta","text":"","code":"Q <- enron.sample[c(5:6)] K <- enron.sample[-c(5:6)] delta(Q, K) #>                          Q                       K target  score #> 1  unknown [Kh Mail_2].txt   known [Kh Mail_1].txt   TRUE -0.039 #> 2  unknown [Kw Mail_3].txt   known [Kh Mail_1].txt  FALSE -0.061 #> 3  unknown [Kh Mail_2].txt   known [Kh Mail_3].txt   TRUE  0.177 #> 4  unknown [Kw Mail_3].txt   known [Kh Mail_3].txt  FALSE  0.075 #> 5  unknown [Kh Mail_2].txt   known [Kh Mail_4].txt   TRUE -0.068 #> 6  unknown [Kw Mail_3].txt   known [Kh Mail_4].txt  FALSE -0.078 #> 7  unknown [Kh Mail_2].txt   known [Kh Mail_5].txt   TRUE -0.018 #> 8  unknown [Kw Mail_3].txt   known [Kh Mail_5].txt  FALSE -0.161 #> 9  unknown [Kh Mail_2].txt   known [Kw Mail_1].txt  FALSE -0.144 #> 10 unknown [Kw Mail_3].txt   known [Kw Mail_1].txt   TRUE  0.110 #> 11 unknown [Kh Mail_2].txt   known [Kw Mail_2].txt  FALSE -0.091 #> 12 unknown [Kw Mail_3].txt   known [Kw Mail_2].txt   TRUE  0.195 #> 13 unknown [Kh Mail_2].txt   known [Kw Mail_4].txt  FALSE -0.072 #> 14 unknown [Kw Mail_3].txt   known [Kw Mail_4].txt   TRUE  0.315 #> 15 unknown [Kh Mail_2].txt   known [Kw Mail_5].txt  FALSE -0.113 #> 16 unknown [Kw Mail_3].txt   known [Kw Mail_5].txt   TRUE  0.243 #> 17 unknown [Kh Mail_2].txt unknown [Lc Mail_1].txt  FALSE  0.177 #> 18 unknown [Kw Mail_3].txt unknown [Lc Mail_1].txt  FALSE -0.073 #> 19 unknown [Kh Mail_2].txt   known [Lc Mail_2].txt  FALSE -0.084 #> 20 unknown [Kw Mail_3].txt   known [Lc Mail_2].txt  FALSE -0.192 #> 21 unknown [Kh Mail_2].txt   known [Lc Mail_3].txt  FALSE -0.059 #> 22 unknown [Kw Mail_3].txt   known [Lc Mail_3].txt  FALSE -0.009 #> 23 unknown [Kh Mail_2].txt   known [Lc Mail_4].txt  FALSE  0.073 #> 24 unknown [Kw Mail_3].txt   known [Lc Mail_4].txt  FALSE  0.028 #> 25 unknown [Kh Mail_2].txt   known [Lc Mail_5].txt  FALSE  0.087 #> 26 unknown [Kw Mail_3].txt   known [Lc Mail_5].txt  FALSE -0.172 #> 27 unknown [Kh Mail_2].txt unknown [Ld Mail_4].txt  FALSE  0.017 #> 28 unknown [Kw Mail_3].txt unknown [Ld Mail_4].txt  FALSE -0.021 #> 29 unknown [Kh Mail_2].txt   known [Ld Mail_1].txt  FALSE -0.155 #> 30 unknown [Kw Mail_3].txt   known [Ld Mail_1].txt  FALSE  0.082 #> 31 unknown [Kh Mail_2].txt   known [Ld Mail_2].txt  FALSE -0.046 #> 32 unknown [Kw Mail_3].txt   known [Ld Mail_2].txt  FALSE -0.086 #> 33 unknown [Kh Mail_2].txt   known [Ld Mail_3].txt  FALSE -0.168 #> 34 unknown [Kw Mail_3].txt   known [Ld Mail_3].txt  FALSE -0.084 #> 35 unknown [Kh Mail_2].txt   known [Ld Mail_5].txt  FALSE -0.018 #> 36 unknown [Kw Mail_3].txt   known [Ld Mail_5].txt  FALSE  0.032 #> 37 unknown [Kh Mail_2].txt unknown [Lt Mail_2].txt  FALSE  0.032 #> 38 unknown [Kw Mail_3].txt unknown [Lt Mail_2].txt  FALSE -0.143 #> 39 unknown [Kh Mail_2].txt   known [Lt Mail_1].txt  FALSE  0.113 #> 40 unknown [Kw Mail_3].txt   known [Lt Mail_1].txt  FALSE  0.116 #> 41 unknown [Kh Mail_2].txt   known [Lt Mail_3].txt  FALSE  0.139 #> 42 unknown [Kw Mail_3].txt   known [Lt Mail_3].txt  FALSE  0.080 #> 43 unknown [Kh Mail_2].txt   known [Lt Mail_4].txt  FALSE  0.145 #> 44 unknown [Kw Mail_3].txt   known [Lt Mail_4].txt  FALSE -0.032 #> 45 unknown [Kh Mail_2].txt unknown [Lk Mail_4].txt  FALSE  0.030 #> 46 unknown [Kw Mail_3].txt unknown [Lk Mail_4].txt  FALSE -0.154 #> 47 unknown [Kh Mail_2].txt   known [Lk Mail_1].txt  FALSE -0.038 #> 48 unknown [Kw Mail_3].txt   known [Lk Mail_1].txt  FALSE -0.012 #> 49 unknown [Kh Mail_2].txt   known [Lk Mail_2].txt  FALSE -0.065 #> 50 unknown [Kw Mail_3].txt   known [Lk Mail_2].txt  FALSE -0.104 #> 51 unknown [Kh Mail_2].txt   known [Lk Mail_3].txt  FALSE  0.152 #> 52 unknown [Kw Mail_3].txt   known [Lk Mail_3].txt  FALSE -0.002 #> 53 unknown [Kh Mail_2].txt   known [Lk Mail_5].txt  FALSE -0.008 #> 54 unknown [Kw Mail_3].txt   known [Lk Mail_5].txt  FALSE -0.060 #> 55 unknown [Kh Mail_2].txt unknown [Lb Mail_3].txt  FALSE -0.203 #> 56 unknown [Kw Mail_3].txt unknown [Lb Mail_3].txt  FALSE  0.072 #> 57 unknown [Kh Mail_2].txt   known [Lb Mail_1].txt  FALSE -0.111 #> 58 unknown [Kw Mail_3].txt   known [Lb Mail_1].txt  FALSE  0.114 #> 59 unknown [Kh Mail_2].txt   known [Lb Mail_2].txt  FALSE -0.094 #> 60 unknown [Kw Mail_3].txt   known [Lb Mail_2].txt  FALSE  0.052 #> 61 unknown [Kh Mail_2].txt   known [Lb Mail_4].txt  FALSE  0.042 #> 62 unknown [Kw Mail_3].txt   known [Lb Mail_4].txt  FALSE  0.132 #> 63 unknown [Kh Mail_2].txt   known [Lb Mail_5].txt  FALSE -0.106 #> 64 unknown [Kw Mail_3].txt   known [Lb Mail_5].txt  FALSE -0.070 #> 65 unknown [Kh Mail_2].txt unknown [La Mail_3].txt  FALSE -0.006 #> 66 unknown [Kw Mail_3].txt unknown [La Mail_3].txt  FALSE -0.278 #> 67 unknown [Kh Mail_2].txt   known [La Mail_1].txt  FALSE -0.004 #> 68 unknown [Kw Mail_3].txt   known [La Mail_1].txt  FALSE -0.193 #> 69 unknown [Kh Mail_2].txt   known [La Mail_2].txt  FALSE -0.044 #> 70 unknown [Kw Mail_3].txt   known [La Mail_2].txt  FALSE -0.134 #> 71 unknown [Kh Mail_2].txt   known [La Mail_4].txt  FALSE  0.137 #> 72 unknown [Kw Mail_3].txt   known [La Mail_4].txt  FALSE -0.270 #> 73 unknown [Kh Mail_2].txt   known [La Mail_5].txt  FALSE  0.081 #> 74 unknown [Kw Mail_3].txt   known [La Mail_5].txt  FALSE -0.220 #> 75 unknown [Kh Mail_2].txt unknown [Mf Mail_1].txt  FALSE -0.242 #> 76 unknown [Kw Mail_3].txt unknown [Mf Mail_1].txt  FALSE  0.024 #> 77 unknown [Kh Mail_2].txt   known [Mf Mail_2].txt  FALSE -0.096 #> 78 unknown [Kw Mail_3].txt   known [Mf Mail_2].txt  FALSE -0.023 #> 79 unknown [Kh Mail_2].txt   known [Mf Mail_3].txt  FALSE  0.123 #> 80 unknown [Kw Mail_3].txt   known [Mf Mail_3].txt  FALSE -0.017 #> 81 unknown [Kh Mail_2].txt   known [Mf Mail_4].txt  FALSE -0.148 #> 82 unknown [Kw Mail_3].txt   known [Mf Mail_4].txt  FALSE  0.057 #> 83 unknown [Kh Mail_2].txt   known [Mf Mail_5].txt  FALSE  0.058 #> 84 unknown [Kw Mail_3].txt   known [Mf Mail_5].txt  FALSE -0.135 #> 85 unknown [Kh Mail_2].txt unknown [Ml Mail_3].txt  FALSE -0.214 #> 86 unknown [Kw Mail_3].txt unknown [Ml Mail_3].txt  FALSE  0.053 #> 87 unknown [Kh Mail_2].txt   known [Ml Mail_1].txt  FALSE  0.009 #> 88 unknown [Kw Mail_3].txt   known [Ml Mail_1].txt  FALSE  0.051 #> 89 unknown [Kh Mail_2].txt   known [Ml Mail_2].txt  FALSE -0.143 #> 90 unknown [Kw Mail_3].txt   known [Ml Mail_2].txt  FALSE -0.046 #> 91 unknown [Kh Mail_2].txt   known [Ml Mail_4].txt  FALSE  0.024 #> 92 unknown [Kw Mail_3].txt   known [Ml Mail_4].txt  FALSE -0.032 #> 93 unknown [Kh Mail_2].txt   known [Ml Mail_5].txt  FALSE  0.036 #> 94 unknown [Kw Mail_3].txt   known [Ml Mail_5].txt  FALSE  0.064"},{"path":"https://andreanini.github.io/idiolect/reference/density_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot density of TRUE/FALSE distributions — density_plot","title":"Plot density of TRUE/FALSE distributions — density_plot","text":"Plot density TRUE/FALSE distributions","code":""},{"path":"https://andreanini.github.io/idiolect/reference/density_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot density of TRUE/FALSE distributions — density_plot","text":"","code":"density_plot(dataset, q = NULL)"},{"path":"https://andreanini.github.io/idiolect/reference/density_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot density of TRUE/FALSE distributions — density_plot","text":"dataset data frame containing calibration dataset, typically output authorship analysis function like impostors(). q optional argument one value vector values contain score disputed text(s). plotted lines crossing density distributions.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/density_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot density of TRUE/FALSE distributions — density_plot","text":"ggplot2 plot density distributions scores TRUE (typically, '-author') vs. FALSE (typically, 'different-author').","code":""},{"path":"https://andreanini.github.io/idiolect/reference/density_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot density of TRUE/FALSE distributions — density_plot","text":"","code":"res <- data.frame(score = c(0.5, 0.2, 0.8, 0.01, 0.6), target = c(TRUE, FALSE, TRUE, FALSE, TRUE)) q <- c(0.11, 0.7) density_plot(res, q)"},{"path":"https://andreanini.github.io/idiolect/reference/enron.sample.html","id":null,"dir":"Reference","previous_headings":"","what":"Enron sample — enron.sample","title":"Enron sample — enron.sample","text":"small sample Enron corpus comprising ten authors approximately amount data. author one text labelled 'unknown' texts labelled 'known'. data pre-processed using POSnoise algorithm mask content (see contentmask()).","code":""},{"path":"https://andreanini.github.io/idiolect/reference/enron.sample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enron sample — enron.sample","text":"","code":"enron.sample"},{"path":"https://andreanini.github.io/idiolect/reference/enron.sample.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Enron sample — enron.sample","text":"quanteda corpus object.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/enron.sample.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Enron sample — enron.sample","text":"Halvani, Oren. 2021. Practice-Oriented Authorship Verification. Technical University Darmstadt PhD Thesis. https://tuprints.ulb.tu-darmstadt.de/19861/","code":""},{"path":"https://andreanini.github.io/idiolect/reference/impostors.html","id":null,"dir":"Reference","previous_headings":"","what":"Impostors Method — impostors","title":"Impostors Method — impostors","text":"function runs Impostors Method authorship verification. Impostors Method based calculating similarity score , using corpus impostor texts, perform bootstrapping analysis sampling random subsets features impostors order test robustness similarity.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/impostors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Impostors Method — impostors","text":"","code":"impostors(   q.data,   k.data,   cand.imps,   algorithm = \"RBI\",   coefficient = \"minmax\",   k = 300,   m = 100,   n = 25,   features = FALSE,   cores = NULL )"},{"path":"https://andreanini.github.io/idiolect/reference/impostors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Impostors Method — impostors","text":"q.data questioned disputed data, either corpus (output create_corpus()) quanteda dfm (output vectorize()). k.data known undisputed data, either corpus (output create_corpus()) quanteda dfm (output vectorize()). one sample candidate author accepted algorithms except IM. cand.imps impostors data candidate authors, either corpus (output create_corpus()) quanteda dfm (output vectorize()). can object k.data (e.g. recycle impostors). algorithm string specifying impostors algorithm use, either \"RBI\" (deafult), \"KGI\", \"IM\". coefficient string indicating coefficient use, either \"minmax\" (default) \"cosine\". apply algorithm KGI, distance \"minmax\". k k parameters RBI algorithm. used algorithms. default 300. m m parameter IM algorithm. used algorithms. default 100. n n parameter IM algorithm. used algorithms. default 25. features logical value indicating whether important features retrieved . default FALSE. applies RBI algorithm. cores number cores use parallel processing (default one).","code":""},{"path":"https://andreanini.github.io/idiolect/reference/impostors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Impostors Method — impostors","text":"function test possible combinations Q texts candidate authors return data frame containing score ranging 0 1, higher score indicating higher likelihood author produced two sets texts. data frame contains column called \"target\" logical value TRUE author Q text candidate FALSE otherwise. RBI algorithm selected features parameter TRUE data frame also contain column features likely impact score. features consistently found shared candidate author's data questioned data also tend rare dataset impostors.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/impostors.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Impostors Method — impostors","text":"several variants Impostors Method function can run three : IM: original Impostors Method proposed Koppel Winter (2014). KGI: Kestemont's et al. (2016) version, popular implementation Impostors Method stylometry. inspired IM generalized version, General Impostors Method proposed Seidman (2013). RBI: Rank-Based Impostors Method (Potha Stamatatos 2017, 2020), default option recent tends outperform original. two data sets q.data, k.data, must disjunct terms texts contain otherwise error returned. However, cand.imps k.data can object, example, use candidates' texts impostors. function always exclude impostor texts author Q K texts considered.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/impostors.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Impostors Method — impostors","text":"Kestemont, Mike, Justin Stover, Moshe Koppel, Folgert Karsdorp & Walter Daelemans. 2016. Authenticating writings Julius Caesar. Expert Systems Applications 63. 86–96. https://doi.org/10.1016/j.eswa.2016.06.029. Koppel, Moshe & Yaron Winter. 2014. Determining two documents written author. Journal Association Information Science Technology 65(1). 178–187. Potha, Nektaria & Efstathios Stamatatos. 2017. Improved Impostors Method Authorship Verification. Gareth J.FALSE. Jones, Séamus Lawless, Julio Gonzalo, Liadh Kelly, Lorraine Goeuriot, Thomas Mandl, Linda Cappellato & Nicola Ferro (eds.), Experimental IR Meets Multilinguality, Multimodality, Interaction (Lecture Notes Computer Science), vol. 10456, 138–144. Springer, Cham. https://doi.org/10.1007/978-3-319-65813-1_14. (5 September, 2017). Potha, Nektaria & Efstathios Stamatatos. 2020. Improved algorithms extrinsic author verification. Knowledge Information Systems 62(5). 1903–1921. https://doi.org/10.1007/s10115-019-01408-4. Seidman, Shachar. 2013. Authorship Verification Using Impostors Method. Pamela Forner, Roberto Navigli, Dan Tufis & Nicola Ferro (eds.), Proceedings CLEF 2013 Evaluation Labs Workshop – Working Notes Papers, 23–26. Valencia, Spain. https://ceur-ws.org/Vol-1179/.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/impostors.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Impostors Method — impostors","text":"","code":"Q <- enron.sample[1] K <- enron.sample[2:3] imps <- enron.sample[4:9] impostors(Q, K, imps, algorithm = \"KGI\") #>    K                     Q target score #> 1 Kh known [Kh Mail_1].txt   TRUE  0.41"},{"path":"https://andreanini.github.io/idiolect/reference/lambdaG.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply the LambdaG algorithm — lambdaG","title":"Apply the LambdaG algorithm — lambdaG","text":"function calculates likelihood ratio grammar models, \\(\\lambda_G\\), Nini et al. (review). order run analysis paper, data must preprocessed using contentmask() \"algorithm\" parameter set \"POSnoise\".","code":""},{"path":"https://andreanini.github.io/idiolect/reference/lambdaG.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply the LambdaG algorithm — lambdaG","text":"","code":"lambdaG(q.data, k.data, ref.data, N = 10, r = 30, cores = NULL)"},{"path":"https://andreanini.github.io/idiolect/reference/lambdaG.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply the LambdaG algorithm — lambdaG","text":"q.data questioned disputed data quanteda tokens object tokens sentences (e.g. output tokenize_sents()). k.data known undisputed data quanteda tokens object tokens sentences (e.g. output tokenize_sents()). ref.data reference dataset quanteda tokens object tokens sentences (e.g. output tokenize_sents()). can object k.data. N order model. Default 10. r number iterations. Default 30. cores number cores use parallel processing (default one).","code":""},{"path":"https://andreanini.github.io/idiolect/reference/lambdaG.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply the LambdaG algorithm — lambdaG","text":"function test possible combinations Q texts candidate authors return data frame containing \\(\\lambda_G\\), uncalibrated log-likelihood ratio (base 10). \\(\\lambda_G\\) can calibrated likelihood ratio expresses strength evidence using calibrate_LLR(). data frame contains column called \"target\" logical value TRUE author Q text candidate FALSE otherwise.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/lambdaG.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Apply the LambdaG algorithm — lambdaG","text":"Nini, ., Halvani, O., Graner, L., Gherardi, V., Ishihara, S. Authorship Verification based Likelihood Ratio Grammar Models. https://arxiv.org/abs/2403.08462v1","code":""},{"path":"https://andreanini.github.io/idiolect/reference/lambdaG.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply the LambdaG algorithm — lambdaG","text":"","code":"q.data <- enron.sample[1] |> quanteda::tokens(\"sentence\") k.data <- enron.sample[2:10] |> quanteda::tokens(\"sentence\") ref.data <- enron.sample[11:ndoc(enron.sample)] |> quanteda::tokens(\"sentence\") lambdaG(q.data, k.data, ref.data) #>    K                     Q target  score #> 1 Kh known [Kh Mail_1].txt   TRUE 39.186 #> 2 Kw known [Kh Mail_1].txt  FALSE -6.720"},{"path":"https://andreanini.github.io/idiolect/reference/lambdaG_visualize.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize the output of the LambdaG algorithm — lambdaG_visualize","title":"Visualize the output of the LambdaG algorithm — lambdaG_visualize","text":"function outputs colour-coded list sentences belonging input Q text ordered highest lowest \\(\\lambda_G\\), shown Nini et al. (review).","code":""},{"path":"https://andreanini.github.io/idiolect/reference/lambdaG_visualize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize the output of the LambdaG algorithm — lambdaG_visualize","text":"","code":"lambdaG_visualize(   q.data,   k.data,   ref.data,   N = 10,   r = 30,   output = \"html\",   print = \"\",   scale = \"absolute\",   cores = NULL )"},{"path":"https://andreanini.github.io/idiolect/reference/lambdaG_visualize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize the output of the LambdaG algorithm — lambdaG_visualize","text":"q.data single questioned disputed text quanteda tokens object tokens sentences (e.g. output tokenize_sents()). k.data known undisputed corpus containing exclusively single candidate author's texts quanteda tokens object tokens sentences (e.g. output tokenize_sents()). ref.data reference dataset quanteda tokens object tokens sentences (e.g. output tokenize_sents()). N order model. Default 10. r number iterations. Default 30. output string detailing file type colour-coded text output. Either \"html\" (default) \"latex\". print string indicating path folder print colour-coded text file. left empty (default), nothing printed. scale string indicating scale use colour-code text file. \"absolute\" (default) raw \\(\\lambda_G\\) used; \"relative\", z-score \\(\\lambda_G\\) Q data used instead, thus showing relative importance. cores number cores use parallel processing (default one).","code":""},{"path":"https://andreanini.github.io/idiolect/reference/lambdaG_visualize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize the output of the LambdaG algorithm — lambdaG_visualize","text":"function outputs list two objects: data frame row token Q text values \\(\\lambda_G\\) token sentences, decreasing order sentence \\(\\lambda_G\\) relative contribution token sentence final \\(\\lambda_G\\) percentage; raw code html LaTeX generates colour-coded file. path provided print argument function also save colour-coded text html plain text file.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/lambdaG_visualize.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Visualize the output of the LambdaG algorithm — lambdaG_visualize","text":"Nini, ., Halvani, O., Graner, L., Gherardi, V., Ishihara, S. Authorship Verification based Likelihood Ratio Grammar Models. https://arxiv.org/abs/2403.08462v1","code":""},{"path":"https://andreanini.github.io/idiolect/reference/lambdaG_visualize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize the output of the LambdaG algorithm — lambdaG_visualize","text":"","code":"q.data <- corpus_trim(enron.sample[1], \"sentences\", max_ntoken = 10) |> quanteda::tokens(\"sentence\") k.data <- enron.sample[2:5]|> quanteda::tokens(\"sentence\") ref.data <- enron.sample[6:ndoc(enron.sample)] |> quanteda::tokens(\"sentence\") outputs <- lambdaG_visualize(q.data, k.data, ref.data, r = 2) outputs$table #> # A tibble: 13 × 8 #>    sentence_id token_id t         lambdaG sentence_lambdaG zlambdaG #>          <int>    <int> <chr>       <dbl>            <dbl>    <dbl> #>  1           1        1 J          0.771             0.726    1.40  #>  2           1        2 N          0.220             0.726    0.323 #>  3           1        3 ,         -1.16              0.726   -2.38  #>  4           1        4 but        0.326             0.726    0.531 #>  5           1        5 that      -0.0955            0.726   -0.297 #>  6           1        6 's         0.991             0.726    1.84  #>  7           1        7 just      -0.0210            0.726   -0.151 #>  8           1        8 the        0.0194            0.726   -0.072 #>  9           1        9 N          0.0259            0.726   -0.059 #> 10           1       10 it        -0.252             0.726   -0.605 #> 11           1       11 works     -0.102             0.726   -0.31  #> 12           1       12 .          0.0365            0.726   -0.038 #> 13           1       13 ___EOS___ -0.0385            0.726   -0.185 #> # ℹ 2 more variables: token_contribution <dbl>, sent_contribution <dbl>"},{"path":"https://andreanini.github.io/idiolect/reference/most_similar.html","id":null,"dir":"Reference","previous_headings":"","what":"Select the most similar texts to a specific text — most_similar","title":"Select the most similar texts to a specific text — most_similar","text":"Select similar texts specific text","code":""},{"path":"https://andreanini.github.io/idiolect/reference/most_similar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select the most similar texts to a specific text — most_similar","text":"","code":"most_similar(sample, pool, coefficient, n)"},{"path":"https://andreanini.github.io/idiolect/reference/most_similar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select the most similar texts to a specific text — most_similar","text":"sample single row quanteda dfm representing sample match. pool dfm containing possible samples select top n. coefficient coefficient use similarity. Either \"minmax\", \"cosine\", \"Phi\". n number rows extract pool potential samples.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/most_similar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select the most similar texts to a specific text — most_similar","text":"function returns dfm containing top n similar rows input sample using minmax distance.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/most_similar.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select the most similar texts to a specific text — most_similar","text":"","code":"text1 <- \"The cat sat on the mat\" text2 <- \"The dog sat on the chair\" text3 <- \"Violence is the last refuge of the incompetent\" c <- quanteda::corpus(c(text1, text2, text3)) d <- quanteda::tokens(c) |> quanteda::dfm() |> quanteda::dfm_weight(scheme = \"prop\") most_similar(d[1,], d[-1,], coefficient = \"minmax\", n = 1) #> Document-feature matrix of: 1 document, 13 features (61.54% sparse) and 0 docvars. #>        features #> docs          the cat       sat        on mat       dog     chair violence is #>   text2 0.3333333   0 0.1666667 0.1666667   0 0.1666667 0.1666667        0  0 #>        features #> docs    last #>   text2    0 #> [ reached max_nfeat ... 3 more features ]"},{"path":"https://andreanini.github.io/idiolect/reference/ngram_tracing.html","id":null,"dir":"Reference","previous_headings":"","what":"N-gram tracing — ngram_tracing","title":"N-gram tracing — ngram_tracing","text":"function runs authorship analysis method called n-gram tracing, can used attribution verification.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/ngram_tracing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"N-gram tracing — ngram_tracing","text":"","code":"ngram_tracing(   q.data,   k.data,   tokens = \"character\",   remove_punct = FALSE,   remove_symbols = TRUE,   remove_numbers = TRUE,   lowercase = TRUE,   n = 9,   coefficient = \"simpson\",   features = FALSE,   cores = NULL )"},{"path":"https://andreanini.github.io/idiolect/reference/ngram_tracing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"N-gram tracing — ngram_tracing","text":"q.data questioned disputed data, either corpus (output create_corpus()) quanteda dfm (output vectorize()). k.data known undisputed data, either corpus (output create_corpus()) quanteda dfm (output vectorize()). one sample candidate author accepted function combine make profile. tokens type tokens extract, either \"word\" \"character\" (default). remove_punct logical value. FALSE (default) keeps punctuation marks. remove_symbols logical value. TRUE (default) removes symbols. remove_numbers logical value. TRUE (default) removes numbers. lowercase logical value. TRUE (default) transforms tokens lower case. n order size n-grams extracted. Default 9. coefficient coefficient use compare texts, one : \"simpson\" (default), \"phi\", \"jaccard\", \"kulczynski\", \"cole\". features Logical default FALSE. TRUE result table contain features overlap unique overlap corpus. two texts present return n-grams common. cores number cores use parallel processing (default one).","code":""},{"path":"https://andreanini.github.io/idiolect/reference/ngram_tracing.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"N-gram tracing — ngram_tracing","text":"function test possible combinations Q texts candidate authors return data frame containing value similarity coefficient selected called 'score' optional column overlapping features occur Q candidate considered Qs (ordered length n-gram variable length). data frame contains column called 'target' logical value TRUE author Q text candidate FALSE otherwise.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/ngram_tracing.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"N-gram tracing — ngram_tracing","text":"N-gram tracing originally proposed Grieve et al (2019). Nini (2023) proposed mathematical reinterpretation compatible Cognitive Linguistic theories language processing. tested several variants method found original version, uses Simpson's coefficient, tends outperformed versions using Phi coefficient, Kulczynski's coefficient, Cole coefficient. function can run n-gram tracing method using coefficients plus Jaccard coefficient reference, coefficient applied several forensic linguistic studies.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/ngram_tracing.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"N-gram tracing — ngram_tracing","text":"Grieve, Jack, Emily Chiang, Isobelle Clarke, Hannah Gideon, Aninna Heini, Andrea Nini & Emily Waibel. 2019. Attributing Bixby Letter using n-gram tracing. Digital Scholarship Humanities 34(3). 493–512. Nini, Andrea. 2023. Theory Linguistic Individuality Authorship Analysis (Elements Forensic Linguistics). Cambridge, UK: Cambridge University Press.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/ngram_tracing.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"N-gram tracing — ngram_tracing","text":"","code":"Q <- enron.sample[c(5:6)] K <- enron.sample[-c(5:6)] ngram_tracing(Q, K, coefficient = 'phi') #>                          Q  K target  score #> 1  unknown [Kh Mail_2].txt Kh   TRUE  0.032 #> 2  unknown [Kw Mail_3].txt Kh  FALSE  0.031 #> 3  unknown [Kh Mail_2].txt Kw  FALSE  0.028 #> 4  unknown [Kw Mail_3].txt Kw   TRUE  0.086 #> 5  unknown [Kh Mail_2].txt Lc  FALSE  0.028 #> 6  unknown [Kw Mail_3].txt Lc  FALSE  0.027 #> 7  unknown [Kh Mail_2].txt Ld  FALSE  0.026 #> 8  unknown [Kw Mail_3].txt Ld  FALSE  0.046 #> 9  unknown [Kh Mail_2].txt Lt  FALSE  0.033 #> 10 unknown [Kw Mail_3].txt Lt  FALSE  0.048 #> 11 unknown [Kh Mail_2].txt Lk  FALSE  0.014 #> 12 unknown [Kw Mail_3].txt Lk  FALSE  0.027 #> 13 unknown [Kh Mail_2].txt Lb  FALSE  0.017 #> 14 unknown [Kw Mail_3].txt Lb  FALSE  0.040 #> 15 unknown [Kh Mail_2].txt La  FALSE  0.006 #> 16 unknown [Kw Mail_3].txt La  FALSE -0.018 #> 17 unknown [Kh Mail_2].txt Mf  FALSE  0.021 #> 18 unknown [Kw Mail_3].txt Mf  FALSE  0.038 #> 19 unknown [Kh Mail_2].txt Ml  FALSE  0.016 #> 20 unknown [Kw Mail_3].txt Ml  FALSE  0.047"},{"path":"https://andreanini.github.io/idiolect/reference/performance.html","id":null,"dir":"Reference","previous_headings":"","what":"Performance evaluation — performance","title":"Performance evaluation — performance","text":"function used test performance authorship analysis method.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/performance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Performance evaluation — performance","text":"","code":"performance(training, test = NULL)"},{"path":"https://andreanini.github.io/idiolect/reference/performance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Performance evaluation — performance","text":"training data frame results evaluate, typically output authorship analysis function, impostors(). training present function perform leave-one-cross-validation. test Optional data frame results. present calibration model extracted training performance evaluated data set.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/performance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Performance evaluation — performance","text":"function returns list containing data frame performance statistics, including object can used make tippet plot using tippet.plot() function ROC package (https://github.com/davidavdav/ROC).","code":""},{"path":"https://andreanini.github.io/idiolect/reference/performance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Performance evaluation — performance","text":"applying method real authorship case, good practice test known ground truth data. function performs test taking input either single table results two tables, one training one test, returning output list following performance statistics: log-likelihood ratio cost (\\(C_{llr}\\) \\(C_{llr}^{min}\\)), Equal Error Rate (ERR), mean values log-likelihood ratio -author (TRUE) different-author (FALSE) cases, Area Curve (AUC), Balanced Accuracy, Precision, Recall, F1, full confusion matrix. binary classification statistics calculated considering Log-Likelihood Ratio score 0 threshold.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/performance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Performance evaluation — performance","text":"","code":"results <- data.frame(score = c(0.5, 0.2, 0.8, 0.01), target = c(TRUE, FALSE, TRUE, FALSE)) perf <- performance(results) #>    |                                                                               |                                                                      |   0%   |                                                                               |=======================                                               |  33%   |                                                                               |===============================================                       |  67%   |                                                                               |======================================================================| 100% #> Setting levels: control = FALSE, case = TRUE #> Setting direction: controls < cases perf$evaluation #>        Cllr  Cllr_min EER Mean TRUE LLR Mean FALSE LLR TRUE trials FALSE trials #> 1 0.2422848 0.4150375  25      14.91206      -12.77601           4            4 #>   AUC Balanced Accuracy Precision Recall F1 TP FN FP TN #> 1   1                 1         1      1  1  2  0  0  2"},{"path":"https://andreanini.github.io/idiolect/reference/posterior.html","id":null,"dir":"Reference","previous_headings":"","what":"Posterior prosecution probabilities and odds — posterior","title":"Posterior prosecution probabilities and odds — posterior","text":"function takes input value Log-Likelihood Ratio returns table shows impact simulated prior probabilities prosecution hypothesis.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/posterior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Posterior prosecution probabilities and odds — posterior","text":"","code":"posterior(LLR)"},{"path":"https://andreanini.github.io/idiolect/reference/posterior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Posterior prosecution probabilities and odds — posterior","text":"LLR One single numeric value corresponding Log-Likelihood Ratio (base 10).","code":""},{"path":"https://andreanini.github.io/idiolect/reference/posterior.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Posterior prosecution probabilities and odds — posterior","text":"data frame containing simulated prior probabilities/odds prosecution resulting posterior probabilities/odds LLR.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/posterior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Posterior prosecution probabilities and odds — posterior","text":"","code":"posterior(LLR = 0) #> # A tibble: 11 × 6 #>    prosecution_prior_probs prior_odds   LLR    LR  post_odds #>                      <dbl>      <dbl> <dbl> <dbl>      <dbl> #>  1                0.000001 0.00000100     0     1 0.00000100 #>  2                0.01     0.0101         0     1 0.0101     #>  3                0.1      0.111          0     1 0.111      #>  4                0.2      0.25           0     1 0.25       #>  5                0.3      0.429          0     1 0.429      #>  6                0.4      0.667          0     1 0.667      #>  7                0.5      1              0     1 1          #>  8                0.6      1.5            0     1 1.5        #>  9                0.7      2.33           0     1 2.33       #> 10                0.8      4              0     1 4          #> 11                0.9      9              0     1 9          #> # ℹ 1 more variable: prosecution_post_probs <dbl> posterior(LLR = 1.8) #> # A tibble: 11 × 6 #>    prosecution_prior_probs prior_odds   LLR    LR   post_odds #>                      <dbl>      <dbl> <dbl> <dbl>       <dbl> #>  1                0.000001 0.00000100   1.8  63.1   0.0000631 #>  2                0.01     0.0101       1.8  63.1   0.637     #>  3                0.1      0.111        1.8  63.1   7.01      #>  4                0.2      0.25         1.8  63.1  15.8       #>  5                0.3      0.429        1.8  63.1  27.0       #>  6                0.4      0.667        1.8  63.1  42.1       #>  7                0.5      1            1.8  63.1  63.1       #>  8                0.6      1.5          1.8  63.1  94.6       #>  9                0.7      2.33         1.8  63.1 147.        #> 10                0.8      4            1.8  63.1 252.        #> 11                0.9      9            1.8  63.1 568.        #> # ℹ 1 more variable: prosecution_post_probs <dbl> posterior(LLR = -0.5) #> # A tibble: 11 × 6 #>    prosecution_prior_probs prior_odds   LLR    LR   post_odds #>                      <dbl>      <dbl> <dbl> <dbl>       <dbl> #>  1                0.000001 0.00000100  -0.5 0.316 0.000000316 #>  2                0.01     0.0101      -0.5 0.316 0.00319     #>  3                0.1      0.111       -0.5 0.316 0.0351      #>  4                0.2      0.25        -0.5 0.316 0.0791      #>  5                0.3      0.429       -0.5 0.316 0.136       #>  6                0.4      0.667       -0.5 0.316 0.211       #>  7                0.5      1           -0.5 0.316 0.316       #>  8                0.6      1.5         -0.5 0.316 0.474       #>  9                0.7      2.33        -0.5 0.316 0.738       #> 10                0.8      4           -0.5 0.316 1.26        #> 11                0.9      9           -0.5 0.316 2.85        #> # ℹ 1 more variable: prosecution_post_probs <dbl> posterior(LLR = 4) #> # A tibble: 11 × 6 #>    prosecution_prior_probs prior_odds   LLR    LR  post_odds #>                      <dbl>      <dbl> <dbl> <dbl>      <dbl> #>  1                0.000001 0.00000100     4 10000     0.0100 #>  2                0.01     0.0101         4 10000   101.     #>  3                0.1      0.111          4 10000  1111.     #>  4                0.2      0.25           4 10000  2500      #>  5                0.3      0.429          4 10000  4286.     #>  6                0.4      0.667          4 10000  6667.     #>  7                0.5      1              4 10000 10000      #>  8                0.6      1.5            4 10000 15000      #>  9                0.7      2.33           4 10000 23333.     #> 10                0.8      4              4 10000 40000      #> 11                0.9      9              4 10000 90000      #> # ℹ 1 more variable: prosecution_post_probs <dbl>"},{"path":"https://andreanini.github.io/idiolect/reference/tokenize_sents.html","id":null,"dir":"Reference","previous_headings":"","what":"Tokenize to sentences — tokenize_sents","title":"Tokenize to sentences — tokenize_sents","text":"function turns corpus texts quanteda tokens object sentences.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/tokenize_sents.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tokenize to sentences — tokenize_sents","text":"","code":"tokenize_sents(corpus, model = \"en_core_web_sm\")"},{"path":"https://andreanini.github.io/idiolect/reference/tokenize_sents.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tokenize to sentences — tokenize_sents","text":"corpus quanteda corpus object, typically output create_corpus() function output contentmask(). model spacy model use. default \"en_core_web_sm\".","code":""},{"path":"https://andreanini.github.io/idiolect/reference/tokenize_sents.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tokenize to sentences — tokenize_sents","text":"quanteda tokens object token sentence.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/tokenize_sents.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tokenize to sentences — tokenize_sents","text":"function first split text paragraphs splitting new line markers uses spacy tokenize paragraph sentences. function accepts plain text corpus input output contentmask(). function necessary prepare data lambdaG().","code":""},{"path":"https://andreanini.github.io/idiolect/reference/tokenize_sents.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tokenize to sentences — tokenize_sents","text":"","code":"if (FALSE) { # \\dontrun{ toy.pos <- corpus(\"the N was on the N . he did n't move \\n N ; \\n N N\") tokenize_sents(toy.pos) } # }"},{"path":"https://andreanini.github.io/idiolect/reference/vectorize.html","id":null,"dir":"Reference","previous_headings":"","what":"Vectorize data — vectorize","title":"Vectorize data — vectorize","text":"function turns texts feature vectors.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/vectorize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Vectorize data — vectorize","text":"","code":"vectorize(   input,   tokens,   remove_punct,   remove_symbols,   remove_numbers,   lowercase,   n,   weighting,   trim,   threshold )"},{"path":"https://andreanini.github.io/idiolect/reference/vectorize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Vectorize data — vectorize","text":"input quanteda corpus object author names docvar called \"author\". Typically, output create_corpus() function. tokens type tokens extract, either \"character\" \"word\". remove_punct logical value. FALSE keep punctuation marks TRUE remove . remove_symbols logical value. TRUE removes symbols FALSE keeps . remove_numbers logical value. TRUE removes numbers FALSE keeps . lowercase logical value. TRUE transforms tokens lower case. n order size n-grams extracted. weighting type weighting use, \"rel\" relative frequencies, \"tf-idf\", \"boolean\". trim logical value. TRUE frequent tokens kept. threshold numeric value indicating many frequent tokens keep.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/vectorize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Vectorize data — vectorize","text":"dfm (document-feature matrix) containing text feature vector. N-gram tokenisation cross sentence boundaries.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/vectorize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Vectorize data — vectorize","text":"authorship analysis functions call vectorize() standard parameters algorithm selected. function therefore left users want modify parameters convenience dfm reused algorithms avoid vectorizing data many times. users need run standard analysis need use function.","code":""},{"path":"https://andreanini.github.io/idiolect/reference/vectorize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Vectorize data — vectorize","text":"","code":"mycorpus <- quanteda::corpus(\"The cat sat on the mat.\") quanteda::docvars(mycorpus, \"author\") <- \"author1\" matrix <- vectorize(mycorpus, tokens = \"character\", remove_punct = FALSE, remove_symbols = TRUE, remove_numbers = TRUE, lowercase = TRUE, n = 5, weighting = \"rel\", trim = TRUE, threshold = 1500)"},{"path":"https://andreanini.github.io/idiolect/news/index.html","id":"idiolect-101","dir":"Changelog","previous_headings":"","what":"idiolect 1.0.1","title":"idiolect 1.0.1","text":"Fixed issues CRAN review.","code":""},{"path":"https://andreanini.github.io/idiolect/news/index.html","id":"idiolect-100","dir":"Changelog","previous_headings":"","what":"idiolect 1.0.0","title":"idiolect 1.0.0","text":"Initial CRAN submission.","code":""}]
